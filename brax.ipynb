{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 11:37:44.985159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-03-12 11:37:45.462139: W external/xla/xla/service/gpu/nvptx_compiler.cc:742] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import brax\n",
    "from brax import envs\n",
    "from typing import NamedTuple\n",
    "from brax.envs.wrappers.training import EpisodeWrapper, AutoResetWrapper\n",
    "from gymnax.environments import environment, spaces\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jaxrl_m.tmp import create_learner\n",
    "from typing import Optional, Tuple, Union, Any\n",
    "\n",
    "from functools import partial\n",
    "import chex\n",
    "from flax import struct\n",
    "import jax\n",
    "\n",
    "jax.config.update('jax_log_compiles', False)\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "\n",
    "\n",
    "\n",
    "class GymnaxWrapper(object):\n",
    "    \"\"\"Base class for Gymnax wrappers.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "\n",
    "    # provide proxy access to regular attributes of wrapped object\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "\n",
    "class VecEnv(GymnaxWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.reset = jax.vmap(self._env.reset, in_axes=(0, None))\n",
    "        self.step = jax.vmap(self._env.step, in_axes=(0, 0, 0, None))\n",
    "        \n",
    "    \n",
    "    \n",
    "class BraxGymnaxWrapper:\n",
    "    def __init__(self, env_name, backend=\"spring\"):\n",
    "        env = envs.get_environment(env_name=env_name, backend=backend)\n",
    "        #env = EpisodeWrapper(env, episode_length=1000, action_repeat=1)\n",
    "        #env = AutoResetWrapper(env)\n",
    "        self._env = env\n",
    "        self.action_size = env.action_size\n",
    "        self.observation_size = (env.observation_size,)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        state = self._env.reset(key)\n",
    "        return state.obs, state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        next_state = self._env.step(state, action)\n",
    "        return next_state.obs, next_state, next_state.reward, next_state.done > 0.5, {}\n",
    "\n",
    "    def observation_space(self):\n",
    "        return spaces.Box(\n",
    "            low=-jnp.inf,\n",
    "            high=jnp.inf,\n",
    "            shape=(self._env.observation_size,),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return spaces.Box(\n",
    "            low=-1.0,\n",
    "            high=1.0,\n",
    "            shape=(self._env.action_size,),\n",
    "        )\n",
    "     \n",
    "     \n",
    "@struct.dataclass\n",
    "class LogEnvState:\n",
    "    env_state: environment.EnvState\n",
    "    valid_mask : bool\n",
    "    disc_valid_mask : float\n",
    "    episode_returns: float\n",
    "    disc_episode_returns : float\n",
    "    \n",
    "    episode_lengths: int\n",
    "    timestep: int   \n",
    "\n",
    "class LogWrapper(GymnaxWrapper):\n",
    "    \"\"\"Log the episode returns and lengths.\"\"\"\n",
    "\n",
    "    def __init__(self, env: environment.Environment):\n",
    "        super().__init__(env)\n",
    "        \n",
    "\n",
    "    #@partial(jax.jit, static_argnums=(0,))\n",
    "    def reset(\n",
    "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
    "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
    "        obs, env_state = self._env.reset(key, params)\n",
    "        state = LogEnvState(env_state, 1,1,0,0,0,0)\n",
    "        return obs, state\n",
    "\n",
    "    #@partial(jax.jit, static_argnums=(0,))\n",
    "    def step(\n",
    "        self,\n",
    "        key: chex.PRNGKey,\n",
    "        state: environment.EnvState,\n",
    "        action: Union[int, float],\n",
    "        params: Optional[environment.EnvParams] = None,\n",
    "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state.env_state, action, params\n",
    "        )\n",
    "        \n",
    "        discount = 0.99\n",
    "        valid_mask = state.valid_mask\n",
    "        new_episode_returns = state.episode_returns + reward * valid_mask\n",
    "        new_disc_episode_returns = state.disc_episode_returns + reward * state.disc_valid_mask\n",
    "        new_episode_length = state.episode_lengths + valid_mask\n",
    "        new_valid_mask = state.valid_mask * (1 - done)\n",
    "        new_disc_valid_mask = state.disc_valid_mask * discount * (1-done)\n",
    "        \n",
    "\n",
    "        state = LogEnvState(\n",
    "            env_state=env_state,\n",
    "            valid_mask = new_valid_mask,\n",
    "            disc_valid_mask = new_disc_valid_mask,\n",
    "            episode_returns= new_episode_returns,\n",
    "            disc_episode_returns = new_disc_episode_returns,\n",
    "            episode_lengths=new_episode_length,\n",
    "            timestep=state.timestep + valid_mask,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        info[\"timestep\"] = state.timestep\n",
    "        info[\"valid_mask\"] = state.valid_mask\n",
    "        info[\"episode_returns\"] = state.episode_returns\n",
    "        info[\"episode_lengths\"] = state.episode_lengths\n",
    "        return obs, state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra kwargs: {}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = {\n",
    "    \"NUM_ENVS\": 10,\n",
    "    \"NUM_STEPS\": 1000,\n",
    "    \"BATCH_SIZE\": 1,\n",
    "    \"discount\":0.99,\n",
    "    \"ENV_NAME\": \"halfcheetah\",\n",
    "}\n",
    "\n",
    "\n",
    "rng = jax.random.PRNGKey(0)  # use a dummy rng here\n",
    "env, env_params = BraxGymnaxWrapper(config[\"ENV_NAME\"]), None\n",
    "env = LogWrapper(env)\n",
    "env = VecEnv(env)\n",
    "\n",
    "# INIT ENV\n",
    "rng, _rng = jax.random.split(rng)\n",
    "reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "obsv, env_state = env.reset(reset_rng, env_params)\n",
    "action = env.action_space().sample(rng)\n",
    "\n",
    "\n",
    "learner_args = {\n",
    "    \"seed\":42,\n",
    "    \"observations\":obsv[0].reshape(1,-1),\n",
    "    \"actions\":action.reshape(1,-1),\n",
    "    \"discount\":0.99,\"discount_actor\":True,\"discount_entropy\":True,\"num_critics\":5}\n",
    "\n",
    "\n",
    "agent = create_learner(**learner_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "runner_state = (agent, env_state, obsv, rng)\n",
    "\n",
    "def _env_step(runner_state, unused):\n",
    "                agent, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(rng, config[\"NUM_ENVS\"])\n",
    "                pi = agent.actor(last_obs, temperature=1.)\n",
    "                action = pi.sample(seed=rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "                \n",
    "                # action = jax.vmap(env.action_space().sample)(rng_step)\n",
    "                # log_prob = jnp.ones(config[\"NUM_ENVS\"])\n",
    "\n",
    "                # STEP ENV\n",
    "                obsv, env_state, reward, done, info = env.step(\n",
    "                    rng_step, env_state, action, env_params\n",
    "                )\n",
    "                \n",
    "                #obsv,env_state,reward,done,info = last_obs, env_state, jnp.ones(config[\"NUM_ENVS\"]), jnp.zeros(config[\"NUM_ENVS\"]), {}\n",
    "                transition = Transition(\n",
    "                    done, action, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (agent, env_state, obsv, rng)\n",
    "                \n",
    "                return runner_state, transition\n",
    "                \n",
    "\n",
    "@jax.jit\n",
    "def run_parallel_envs(runner_state):\n",
    "\n",
    "    runner_state, traj_batch = jax.lax.scan(\n",
    "        _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "    )\n",
    "    \n",
    "    return traj_batch\n",
    "\n",
    "tmp =run_parallel_envs(runner_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# tmp = run_parallel_envs(runner_state)\n",
    "\n",
    "rslt = run_parallel_envs(runner_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #with jax.log_compiles(True):\n",
    "\n",
    "# # from tensorflow.python.profiler import trace\n",
    "\n",
    "# # with jax.profiler.trace(\"/tmp/jax-trace\", create_perfetto_link=True):\n",
    "# #     traj = run_parallel_envs(runner_state)\n",
    "\n",
    "# import socketserver\n",
    "# socketserver.TCPServer.allow_reuse_address = True\n",
    "# import jax\n",
    "# with jax.profiler.trace(\"/tmp/jax-trace\", create_perfetto_link=True):\n",
    "#   # Run the operations to be profiled\n",
    "#   key = jax.random.key(0)\n",
    "#   x = jax.random.normal(key, (5000, 5000))\n",
    "#   y = x @ x\n",
    "#   y.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jax\n",
    "\n",
    "# jax.profiler.start_trace(\"/tmp/tensorboard2\")\n",
    "\n",
    "# # Run the operations to be profiled\n",
    "\n",
    "# run_parallel_envs(runner_state)\n",
    "\n",
    "# jax.profiler.stop_trace()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
