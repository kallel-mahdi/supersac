{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import brax\n",
    "from brax import envs\n",
    "from typing import NamedTuple\n",
    "from brax.envs.wrappers.training import EpisodeWrapper, AutoResetWrapper\n",
    "from gymnax.environments import environment, spaces\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jaxrl_m.run_supersac import create_learner\n",
    "from typing import Optional, Tuple, Union, Any\n",
    "\n",
    "from functools import partial\n",
    "import chex\n",
    "from flax import struct\n",
    "import jax\n",
    "\n",
    "jax.config.update('jax_log_compiles', False)\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "\n",
    "\n",
    "\n",
    "class GymnaxWrapper(object):\n",
    "    \"\"\"Base class for Gymnax wrappers.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "\n",
    "    # provide proxy access to regular attributes of wrapped object\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "\n",
    "class VecEnv(GymnaxWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.reset = jax.vmap(self._env.reset, in_axes=(0, None))\n",
    "        self.step = jax.vmap(self._env.step, in_axes=(0, 0, 0, None))\n",
    "    \n",
    "    \n",
    "class BraxGymnaxWrapper:\n",
    "    def __init__(self, env_name, backend=\"mjx\"):\n",
    "        env = envs.get_environment(env_name=env_name, backend=backend)\n",
    "        #env = EpisodeWrapper(env, episode_length=1000, action_repeat=1)\n",
    "        #env = AutoResetWrapper(env)\n",
    "        self._env = env\n",
    "        self.action_size = env.action_size\n",
    "        self.observation_size = (env.observation_size,)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        state = self._env.reset(key)\n",
    "        return state.obs, state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        next_state = self._env.step(state, action)\n",
    "        return next_state.obs, next_state, next_state.reward, next_state.done > 0.5, {}\n",
    "\n",
    "    def observation_space(self, params=None):\n",
    "        return spaces.Box(\n",
    "            low=-jnp.inf,\n",
    "            high=jnp.inf,\n",
    "            shape=(self._env.observation_size,),\n",
    "        )\n",
    "\n",
    "    def action_space(self, params=None):\n",
    "        return spaces.Box(\n",
    "            low=-1.0,\n",
    "            high=1.0,\n",
    "            shape=(self._env.action_size,),\n",
    "        )\n",
    "     \n",
    "     \n",
    "@struct.dataclass\n",
    "class LogEnvState:\n",
    "    env_state: environment.EnvState\n",
    "    valid_mask : bool\n",
    "    disc_valid_mask : float\n",
    "    episode_returns: float\n",
    "    disc_episode_returns : float\n",
    "    \n",
    "    episode_lengths: int\n",
    "    timestep: int   \n",
    "\n",
    "class LogWrapper(GymnaxWrapper):\n",
    "    \"\"\"Log the episode returns and lengths.\"\"\"\n",
    "\n",
    "    def __init__(self, env: environment.Environment):\n",
    "        super().__init__(env)\n",
    "        \n",
    "\n",
    "    #@partial(jax.jit, static_argnums=(0,))\n",
    "    def reset(\n",
    "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
    "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
    "        obs, env_state = self._env.reset(key, params)\n",
    "        state = LogEnvState(env_state, 1,1,0, 0, 0,0 )\n",
    "        return obs, state\n",
    "\n",
    "    #@partial(jax.jit, static_argnums=(0,))\n",
    "    def step(\n",
    "        self,\n",
    "        key: chex.PRNGKey,\n",
    "        state: environment.EnvState,\n",
    "        action: Union[int, float],\n",
    "        params: Optional[environment.EnvParams] = None,\n",
    "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state.env_state, action, params\n",
    "        )\n",
    "        \n",
    "        discount = 0.99\n",
    "        valid_mask = state.valid_mask\n",
    "        new_episode_returns = state.episode_returns + reward * valid_mask\n",
    "        new_disc_episode_returns = state.disc_episode_returns + reward * state.disc_valid_mask\n",
    "        new_episode_length = state.episode_lengths + valid_mask\n",
    "        new_valid_mask = state.valid_mask * (1 - done)\n",
    "        new_disc_valid_mask = state.disc_valid_mask * discount * (1-done)\n",
    "        \n",
    "\n",
    "        state = LogEnvState(\n",
    "            env_state=env_state,\n",
    "            valid_mask = new_valid_mask,\n",
    "            disc_valid_mask = new_disc_valid_mask,\n",
    "            episode_returns= new_episode_returns,\n",
    "            disc_episode_returns = new_disc_episode_returns,\n",
    "            episode_lengths=new_episode_length,\n",
    "            timestep=state.timestep + valid_mask,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        info[\"timestep\"] = state.timestep\n",
    "        info[\"valid_mask\"] = state.valid_mask\n",
    "        info[\"episode_returns\"] = state.episode_returns\n",
    "        info[\"episode_lengths\"] = state.episode_lengths\n",
    "        return obs, state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    \"NUM_ENVS\": 10,\n",
    "    \"NUM_STEPS\": 1000,\n",
    "    \"BATCH_SIZE\": 1,\n",
    "    \"discount\":0.99,\n",
    "    \"ENV_NAME\": \"halfcheetah\",\n",
    "}\n",
    "\n",
    "\n",
    "rng = jax.random.PRNGKey(0)  # use a dummy rng here\n",
    "env, env_params = BraxGymnaxWrapper(config[\"ENV_NAME\"]), None\n",
    "env = LogWrapper(env)\n",
    "env = VecEnv(env)\n",
    "\n",
    "# INIT ENV\n",
    "rng, _rng = jax.random.split(rng)\n",
    "reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "obsv, env_state = env.reset(reset_rng, env_params)\n",
    "action = env.action_space().sample(rng)\n",
    "\n",
    "\n",
    "learner_args = {\n",
    "    \"seed\":42,\n",
    "    \"observations\":obsv[0].reshape(1,-1),\n",
    "    \"actions\":action.reshape(1,-1),\n",
    "    \"discount\":0.99,\"discount_actor\":True,\"discount_entropy\":True,\"num_critics\":5}\n",
    "\n",
    "\n",
    "agent = create_learner(**learner_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "runner_state = (agent, env_state, obsv, rng)\n",
    "\n",
    "def _env_step(runner_state, unused):\n",
    "                agent, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi = agent.actor(last_obs, temperature=1.)\n",
    "                action = pi.sample(seed=rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = env.step(\n",
    "                    rng_step, env_state, action, env_params\n",
    "                )\n",
    "                transition = Transition(\n",
    "                    done, action, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (agent, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def run_parallel_envs(runner_state):\n",
    "\n",
    "    runner_state, traj_batch = jax.lax.scan(\n",
    "        _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "    )\n",
    "    \n",
    "    return traj_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = run_parallel_envs(runner_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
