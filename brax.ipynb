{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import brax\n",
    "from brax import envs\n",
    "from typing import NamedTuple\n",
    "from brax.envs.wrappers.training import EpisodeWrapper, AutoResetWrapper\n",
    "from gymnax.environments import environment, spaces\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jaxrl_m.tmp import create_learner\n",
    "from typing import Optional, Tuple, Union, Any\n",
    "\n",
    "from functools import partial\n",
    "import chex\n",
    "from flax import struct\n",
    "import jax\n",
    "\n",
    "jax.config.update('jax_log_compiles', False)\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "\n",
    "\n",
    "\n",
    "class GymnaxWrapper(object):\n",
    "    \"\"\"Base class for Gymnax wrappers.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "\n",
    "    # provide proxy access to regular attributes of wrapped object\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "\n",
    "class VecEnv(GymnaxWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.reset = jax.vmap(self._env.reset, in_axes=(0, None))\n",
    "        self.step = jax.vmap(self._env.step, in_axes=(0, 0, 0, None))\n",
    "    \n",
    "    \n",
    "class BraxGymnaxWrapper:\n",
    "    def __init__(self, env_name, backend=\"mjx\"):\n",
    "        env = envs.get_environment(env_name=env_name, backend=backend)\n",
    "        #env = EpisodeWrapper(env, episode_length=1000, action_repeat=1)\n",
    "        #env = AutoResetWrapper(env)\n",
    "        self._env = env\n",
    "        self.action_size = env.action_size\n",
    "        self.observation_size = (env.observation_size,)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        state = self._env.reset(key)\n",
    "        return state.obs, state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        next_state = self._env.step(state, action)\n",
    "        return next_state.obs, next_state, next_state.reward, next_state.done > 0.5, {}\n",
    "\n",
    "    def observation_space(self, params=None):\n",
    "        return spaces.Box(\n",
    "            low=-jnp.inf,\n",
    "            high=jnp.inf,\n",
    "            shape=(self._env.observation_size,),\n",
    "        )\n",
    "\n",
    "    def action_space(self, params=None):\n",
    "        return spaces.Box(\n",
    "            low=-1.0,\n",
    "            high=1.0,\n",
    "            shape=(self._env.action_size,),\n",
    "        )\n",
    "     \n",
    "     \n",
    "@struct.dataclass\n",
    "class LogEnvState:\n",
    "    env_state: environment.EnvState\n",
    "    valid_mask : bool\n",
    "    disc_valid_mask : float\n",
    "    episode_returns: float\n",
    "    disc_episode_returns : float\n",
    "    \n",
    "    episode_lengths: int\n",
    "    timestep: int   \n",
    "\n",
    "class LogWrapper(GymnaxWrapper):\n",
    "    \"\"\"Log the episode returns and lengths.\"\"\"\n",
    "\n",
    "    def __init__(self, env: environment.Environment):\n",
    "        super().__init__(env)\n",
    "        \n",
    "\n",
    "    #@partial(jax.jit, static_argnums=(0,))\n",
    "    def reset(\n",
    "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
    "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
    "        obs, env_state = self._env.reset(key, params)\n",
    "        state = LogEnvState(env_state, 1,1,0, 0, 0,0 )\n",
    "        return obs, state\n",
    "\n",
    "    #@partial(jax.jit, static_argnums=(0,))\n",
    "    def step(\n",
    "        self,\n",
    "        key: chex.PRNGKey,\n",
    "        state: environment.EnvState,\n",
    "        action: Union[int, float],\n",
    "        params: Optional[environment.EnvParams] = None,\n",
    "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state.env_state, action, params\n",
    "        )\n",
    "        \n",
    "        discount = 0.99\n",
    "        valid_mask = state.valid_mask\n",
    "        new_episode_returns = state.episode_returns + reward * valid_mask\n",
    "        new_disc_episode_returns = state.disc_episode_returns + reward * state.disc_valid_mask\n",
    "        new_episode_length = state.episode_lengths + valid_mask\n",
    "        new_valid_mask = state.valid_mask * (1 - done)\n",
    "        new_disc_valid_mask = state.disc_valid_mask * discount * (1-done)\n",
    "        \n",
    "\n",
    "        state = LogEnvState(\n",
    "            env_state=env_state,\n",
    "            valid_mask = new_valid_mask,\n",
    "            disc_valid_mask = new_disc_valid_mask,\n",
    "            episode_returns= new_episode_returns,\n",
    "            disc_episode_returns = new_disc_episode_returns,\n",
    "            episode_lengths=new_episode_length,\n",
    "            timestep=state.timestep + valid_mask,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        info[\"timestep\"] = state.timestep\n",
    "        info[\"valid_mask\"] = state.valid_mask\n",
    "        info[\"episode_returns\"] = state.episode_returns\n",
    "        info[\"episode_lengths\"] = state.episode_lengths\n",
    "        return obs, state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    \"NUM_ENVS\": 10,\n",
    "    \"NUM_STEPS\": 1000,\n",
    "    \"BATCH_SIZE\": 1,\n",
    "    \"discount\":0.99,\n",
    "    \"ENV_NAME\": \"halfcheetah\",\n",
    "}\n",
    "\n",
    "\n",
    "rng = jax.random.PRNGKey(0)  # use a dummy rng here\n",
    "env, env_params = BraxGymnaxWrapper(config[\"ENV_NAME\"]), None\n",
    "env = LogWrapper(env)\n",
    "env = VecEnv(env)\n",
    "\n",
    "# INIT ENV\n",
    "rng, _rng = jax.random.split(rng)\n",
    "reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "obsv, env_state = env.reset(reset_rng, env_params)\n",
    "action = env.action_space().sample(rng)\n",
    "\n",
    "\n",
    "learner_args = {\n",
    "    \"seed\":42,\n",
    "    \"observations\":obsv[0].reshape(1,-1),\n",
    "    \"actions\":action.reshape(1,-1),\n",
    "    \"discount\":0.99,\"discount_actor\":True,\"discount_entropy\":True,\"num_critics\":5}\n",
    "\n",
    "\n",
    "agent = create_learner(**learner_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "runner_state = (agent, env_state, obsv, rng)\n",
    "\n",
    "def _env_step(runner_state, unused):\n",
    "                agent, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi = agent.actor(last_obs, temperature=1.)\n",
    "                action = pi.sample(seed=rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = env.step(\n",
    "                    rng_step, env_state, action, env_params\n",
    "                )\n",
    "                transition = Transition(\n",
    "                    done, action, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (agent, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def run_parallel_envs(runner_state):\n",
    "\n",
    "    runner_state, traj_batch = jax.lax.scan(\n",
    "        _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "    )\n",
    "    \n",
    "    return traj_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = run_parallel_envs(runner_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import Optional\n",
    "import chex\n",
    "\n",
    "\n",
    "class GymnaxFitness(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env_name: str = \"CartPole-v1\",\n",
    "        num_env_steps: Optional[int] = None,\n",
    "        num_rollouts: int = 16,\n",
    "        env_kwargs: dict = {},\n",
    "        env_params: dict = {},\n",
    "        test: bool = False,\n",
    "        n_devices: Optional[int] = None,\n",
    "    ):\n",
    "        self.env_name = env_name\n",
    "        self.num_rollouts = num_rollouts\n",
    "        self.test = test\n",
    "\n",
    "        try:\n",
    "            import gymnax\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"You need to install `gymnax` to use its fitness rollouts.\"\n",
    "            )\n",
    "\n",
    "        # Define the RL environment & replace default parameters if desired\n",
    "        self.env, self.env_params = gymnax.make(env_name, **env_kwargs)\n",
    "        self.env_params.replace(**env_params)\n",
    "\n",
    "        if num_env_steps is None:\n",
    "            self.num_env_steps = self.env_params.max_steps_in_episode\n",
    "        else:\n",
    "            self.num_env_steps = num_env_steps\n",
    "        self.steps_per_member = self.num_env_steps * num_rollouts\n",
    "\n",
    "        self.action_shape = self.env.num_actions\n",
    "        self.input_shape = self.env.observation_space(self.env_params).shape\n",
    "        if n_devices is None:\n",
    "            self.n_devices = jax.local_device_count()\n",
    "        else:\n",
    "            self.n_devices = n_devices\n",
    "\n",
    "        # Keep track of total steps executed in environment\n",
    "        self.total_env_steps = 0\n",
    "\n",
    "    def set_apply_fn(self, network_apply, carry_init=None):\n",
    "        \"\"\"Set the network forward function.\"\"\"\n",
    "        self.network = network_apply\n",
    "        # Set rollout function based on model architecture\n",
    "        if carry_init is not None:\n",
    "            self.single_rollout = self.rollout_rnn\n",
    "            self.carry_init = carry_init\n",
    "        else:\n",
    "            self.single_rollout = self.rollout_ffw\n",
    "        self.rollout_repeats = jax.vmap(self.single_rollout, in_axes=(0, None))\n",
    "        self.rollout_pop = jax.vmap(self.rollout_repeats, in_axes=(None, 0))\n",
    "        # pmap over popmembers if > 1 device is available - otherwise pmap\n",
    "        if self.n_devices > 1:\n",
    "            self.rollout_map = self.rollout_pmap\n",
    "            print(\n",
    "                f\"GymFitness: {self.n_devices} devices detected. Please make\"\n",
    "                \" sure that the ES population size divides evenly across the\"\n",
    "                \" number of devices to pmap/parallelize over.\"\n",
    "            )\n",
    "        else:\n",
    "            self.rollout_map = self.rollout_pop\n",
    "\n",
    "    def rollout_pmap(\n",
    "        self, rng_input: chex.PRNGKey, policy_params: chex.ArrayTree\n",
    "    ):\n",
    "        \"\"\"Parallelize rollout across devices. Split keys/reshape correctly.\"\"\"\n",
    "        keys_pmap = jnp.tile(rng_input, (self.n_devices, 1, 1))\n",
    "        rew_dev, steps_dev = jax.pmap(self.rollout_pop)(\n",
    "            keys_pmap, policy_params\n",
    "        )\n",
    "        rew_re = rew_dev.reshape(-1, self.num_rollouts)\n",
    "        steps_re = steps_dev.reshape(-1, self.num_rollouts)\n",
    "        return rew_re, steps_re\n",
    "\n",
    "    def rollout(self, rng_input: chex.PRNGKey, policy_params: chex.ArrayTree):\n",
    "        \"\"\"Placeholder fn call for rolling out a population for multi-evals.\"\"\"\n",
    "        rng_pop = jax.random.split(rng_input, self.num_rollouts)\n",
    "        scores, masks = jax.jit(self.rollout_map)(rng_pop, policy_params)\n",
    "        # Update total step counter using only transitions before termination\n",
    "        self.total_env_steps += masks.sum()\n",
    "        return scores\n",
    "\n",
    "    def rollout_ffw(\n",
    "        self, rng_input: chex.PRNGKey, policy_params: chex.ArrayTree\n",
    "    ):\n",
    "        \"\"\"Rollout an episode with lax.scan.\"\"\"\n",
    "        # Reset the environment\n",
    "        rng_reset, rng_episode = jax.random.split(rng_input)\n",
    "        obs, state = self.env.reset(rng_reset, self.env_params)\n",
    "\n",
    "        def policy_step(state_input, tmp):\n",
    "            \"\"\"lax.scan compatible step transition in jax env.\"\"\"\n",
    "            obs, state, policy_params, rng, cum_reward, valid_mask = state_input\n",
    "            rng, rng_step, rng_net = jax.random.split(rng, 3)\n",
    "            action = self.network(policy_params, obs, rng=rng_net)\n",
    "            next_o, next_s, reward, done, _ = self.env.step(\n",
    "                rng_step, state, action, self.env_params\n",
    "            )\n",
    "            new_cum_reward = cum_reward + reward * valid_mask\n",
    "            new_valid_mask = valid_mask * (1 - done)\n",
    "            carry = [\n",
    "                next_o.squeeze(),\n",
    "                next_s,\n",
    "                policy_params,\n",
    "                rng,\n",
    "                new_cum_reward,\n",
    "                new_valid_mask,\n",
    "            ]\n",
    "            y = [new_valid_mask]\n",
    "            return carry, y\n",
    "\n",
    "        # Scan over episode step loop\n",
    "        carry_out, scan_out = jax.lax.scan(\n",
    "            policy_step,\n",
    "            [\n",
    "                obs,\n",
    "                state,\n",
    "                policy_params,\n",
    "                rng_episode,\n",
    "                jnp.array([0.0]),\n",
    "                jnp.array([1.0]),\n",
    "            ],\n",
    "            (),\n",
    "            self.num_env_steps,\n",
    "        )\n",
    "        # Return the sum of rewards accumulated by agent in episode rollout\n",
    "        ep_mask = scan_out\n",
    "        cum_return = carry_out[-2].squeeze()\n",
    "        return cum_return, jnp.array(ep_mask)\n",
    "\n",
    "    def rollout_rnn(\n",
    "        self, rng_input: chex.PRNGKey, policy_params: chex.ArrayTree\n",
    "    ):\n",
    "        \"\"\"Rollout a jitted episode with lax.scan.\"\"\"\n",
    "        # Reset the environment\n",
    "        rng, rng_reset = jax.random.split(rng_input)\n",
    "        obs, state = self.env.reset(rng_reset, self.env_params)\n",
    "        hidden = self.carry_init()\n",
    "\n",
    "        def policy_step(state_input, tmp):\n",
    "            \"\"\"lax.scan compatible step transition in jax env.\"\"\"\n",
    "            (\n",
    "                obs,\n",
    "                state,\n",
    "                policy_params,\n",
    "                rng,\n",
    "                hidden,\n",
    "                cum_reward,\n",
    "                valid_mask,\n",
    "            ) = state_input\n",
    "            rng, rng_step, rng_net = jax.random.split(rng, 3)\n",
    "            hidden, action = self.network(policy_params, obs, hidden, rng_net)\n",
    "            next_o, next_s, reward, done, _ = self.env.step(\n",
    "                rng_step, state, action, self.env_params\n",
    "            )\n",
    "            new_cum_reward = cum_reward + reward * valid_mask\n",
    "            new_valid_mask = valid_mask * (1 - done)\n",
    "            carry, y = [\n",
    "                next_o.squeeze(),\n",
    "                next_s,\n",
    "                policy_params,\n",
    "                rng,\n",
    "                hidden,\n",
    "                new_cum_reward,\n",
    "                new_valid_mask,\n",
    "            ], [new_valid_mask]\n",
    "            return carry, y\n",
    "\n",
    "        # Scan over episode step loop\n",
    "        carry_out, scan_out = jax.lax.scan(\n",
    "            policy_step,\n",
    "            [\n",
    "                obs,\n",
    "                state,\n",
    "                policy_params,\n",
    "                rng,\n",
    "                hidden,\n",
    "                jnp.array([0.0]),\n",
    "                jnp.array([1.0]),\n",
    "            ],\n",
    "            (),\n",
    "            self.num_env_steps,\n",
    "        )\n",
    "        # Return masked sum of rewards accumulated by agent in episode\n",
    "        ep_mask = scan_out\n",
    "        cum_return = carry_out[-2].squeeze()\n",
    "        return cum_return, jnp.array(ep_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "halfcheetah is not in registered gymnax environments.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mGymnaxFitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhalfcheetah\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rollouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmjx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_devices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m, in \u001b[0;36mGymnaxFitness.__init__\u001b[0;34m(self, env_name, num_env_steps, num_rollouts, env_kwargs, env_params, test, n_devices)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to install `gymnax` to use its fitness rollouts.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Define the RL environment & replace default parameters if desired\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_params \u001b[38;5;241m=\u001b[39m \u001b[43mgymnax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menv_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_params\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv_params)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_env_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/gymnax/registration.py:35\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(env_id, **env_kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A JAX-version of OpenAI's infamous env.make(env_name)\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m registered_envs:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not in registered gymnax environments.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# 1. Classic OpenAI Control Tasks\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPendulum-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: halfcheetah is not in registered gymnax environments."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = GymnaxFitness(env_name=\"halfcheetah\", num_rollouts=10, env_kwargs={\"backend\":\"mjx\"}, env_params={}, test=False, n_devices=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
