{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementations of algorithms for continuous control.\"\"\"\n",
    "import functools\n",
    "from jaxrl_m.typing import *\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from jaxrl_m.common import TrainState, target_update, nonpytree_field\n",
    "from jaxrl_m.networks import Policy, Critic, ensemblize\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "class Temperature(nn.Module):\n",
    "    initial_temperature: float = 1.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self) -> jnp.ndarray:\n",
    "        log_temp = self.param('log_temp',\n",
    "                              init_fn=lambda key: jnp.full(\n",
    "                                  (), jnp.log(self.initial_temperature)))\n",
    "        return jnp.exp(log_temp)\n",
    "\n",
    "class SACAgent(flax.struct.PyTreeNode):\n",
    "    rng: PRNGKey\n",
    "    critic: TrainState\n",
    "    target_critic: TrainState\n",
    "    actor: TrainState\n",
    "    temp: TrainState\n",
    "    config: dict = nonpytree_field()\n",
    "\n",
    "    \n",
    "    @jax.jit\n",
    "    def update_critic(agent, batch: Batch):\n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "\n",
    "     \n",
    "        \n",
    "        def critic_loss_fn(critic_params):\n",
    "            \n",
    "            next_dist = agent.actor(batch['next_observations'])\n",
    "            next_actions, next_log_probs = next_dist.sample_and_log_prob(seed=next_key)\n",
    "\n",
    "            next_q1, next_q2 = agent.target_critic(batch['next_observations'], next_actions,True,\n",
    "                                                   params=None,rngs={'dropout': next_key})\n",
    "            next_q = jnp.minimum(next_q1, next_q2)\n",
    "            target_q = batch['rewards'] + agent.config['discount'] * batch['masks'] * next_q\n",
    "\n",
    "            if agent.config['backup_entropy']:\n",
    "                target_q = target_q - agent.config['discount'] * batch['masks'] * next_log_probs * agent.temp()\n",
    "            \n",
    "            \n",
    "            q1, q2 = agent.critic(batch['observations'], batch['actions'],True,\n",
    "                                                params=critic_params,rngs={'dropout': curr_key},\n",
    "                                                )\n",
    "            critic_loss = ((q1 - target_q)**2 + (q2 - target_q)**2).mean()\n",
    "            \n",
    "            return critic_loss, {\n",
    "                'critic_loss': critic_loss,\n",
    "                'q1': q1.mean(),\n",
    "            }     \n",
    "    \n",
    "  \n",
    "    \n",
    "\n",
    "        \n",
    "        new_critic, critic_info = agent.critic.apply_loss_fn(loss_fn=critic_loss_fn, has_aux=True)\n",
    "        new_target_critic = target_update(agent.critic, agent.target_critic, agent.config['target_update_rate'])\n",
    "        \n",
    "        return agent.replace(rng=new_rng, critic=new_critic, target_critic=new_target_critic), {**critic_info}\n",
    "        \n",
    "        \n",
    "        \n",
    "    @jax.jit\n",
    "    def update_actor(agent, batch: Batch):\n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "\n",
    "        def actor_loss_fn(actor_params):\n",
    "            # dist = agent.actor(batch['observations'], params=actor_params)\n",
    "            # actions, log_probs = dist.sample_and_log_prob(seed=curr_key)\n",
    "            # q1, q2 = agent.critic(batch['observations'], actions)\n",
    "            \n",
    "\n",
    "            dist = agent.actor(jnp.repeat(batch['observations'],5,axis=0), params=actor_params)\n",
    "            actions, log_probs = dist.sample_and_log_prob(seed=curr_key)\n",
    "            q1, q2 = agent.critic(jnp.repeat(batch['observations'],5,axis=0), actions)\n",
    "            \n",
    "\n",
    "            q = jnp.minimum(q1, q2)\n",
    "\n",
    "            actor_loss = ((log_probs * agent.temp() - q)).mean()\n",
    "            #actor_loss = ((log_probs * agent.temp() - q)*weights).sum()\n",
    "            return actor_loss, {\n",
    "                'actor_loss': actor_loss,\n",
    "                'entropy': -1 * log_probs.mean(),\n",
    "            }\n",
    "        \n",
    "                \n",
    "        def temp_loss_fn(temp_params, entropy, target_entropy):\n",
    "            temperature = agent.temp(params=temp_params)\n",
    "            temp_loss = (temperature * (entropy - target_entropy)).mean()\n",
    "            return temp_loss, {\n",
    "                'temp_loss': temp_loss,\n",
    "                'temperature': temperature,\n",
    "            }\n",
    "\n",
    "        \n",
    "        new_actor, actor_info = agent.actor.apply_loss_fn(loss_fn=actor_loss_fn, has_aux=True)\n",
    "        temp_loss_fn = functools.partial(temp_loss_fn, entropy=actor_info['entropy'], target_entropy=agent.config['target_entropy'])\n",
    "        new_temp, temp_info = agent.temp.apply_loss_fn(loss_fn=temp_loss_fn, has_aux=True)\n",
    "\n",
    "\n",
    "        return agent.replace(rng=new_rng,actor=new_actor, temp=new_temp), {**actor_info, **temp_info}\n",
    "        \n",
    "\n",
    "\n",
    "    @jax.jit\n",
    "    def sample_actions(agent,   \n",
    "                       observations: np.ndarray,\n",
    "                       *,\n",
    "                       seed: PRNGKey,\n",
    "                       temperature: float = 1.0,\n",
    "                       ) -> jnp.ndarray:\n",
    "        actions = agent.actor(observations, temperature=temperature).sample(seed=seed)\n",
    "        #actions = jnp.clip(actions, -1, 1)\n",
    "        return actions\n",
    "\n",
    "\n",
    "\n",
    "def create_learner(\n",
    "                 seed: int,\n",
    "                 observations: jnp.ndarray,\n",
    "                 actions: jnp.ndarray,\n",
    "                 actor_lr: float = 3e-4,\n",
    "                 critic_lr: float = 3e-4,\n",
    "                 temp_lr: float = 3e-4,\n",
    "                 hidden_dims: Sequence[int] = (256, 256),\n",
    "                 discount: float = 0.99,\n",
    "                 tau: float = 0.005,\n",
    "                 target_entropy: float = None,\n",
    "                 backup_entropy: bool = True,\n",
    "            **kwargs):\n",
    "\n",
    "        print('Extra kwargs:', kwargs)\n",
    "\n",
    "        rng = jax.random.PRNGKey(seed)\n",
    "        rng, actor_key, critic_key = jax.random.split(rng, 3)\n",
    "\n",
    "        action_dim = actions.shape[-1]\n",
    "        actor_def = Policy((hidden_dims), action_dim=action_dim, \n",
    "            log_std_min=-10.0, state_dependent_std=True, tanh_squash_distribution=True, final_fc_init_scale=1.0)\n",
    "\n",
    "        actor_params = actor_def.init(actor_key, observations)['params']\n",
    "        actor = TrainState.create(actor_def, actor_params, tx=optax.adam(learning_rate=actor_lr))\n",
    "\n",
    "        critic_def = ensemblize(Critic, num_qs=2,split_rngs={\"dropout\":True})(hidden_dims)\n",
    "        critic_params = critic_def.init(critic_key, observations, actions)['params']\n",
    "        critic = TrainState.create(critic_def, critic_params, tx=optax.adam(learning_rate=critic_lr))\n",
    "        target_critic = TrainState.create(critic_def, critic_params)\n",
    "\n",
    "        temp_def = Temperature()\n",
    "        temp_params = temp_def.init(rng)['params']\n",
    "        temp = TrainState.create(temp_def, temp_params, tx=optax.adam(learning_rate=temp_lr))\n",
    "\n",
    "        if target_entropy is None:\n",
    "            target_entropy = -0.5 * action_dim\n",
    "\n",
    "        config = flax.core.FrozenDict(dict(\n",
    "            discount=discount,\n",
    "            target_update_rate=tau,\n",
    "            target_entropy=target_entropy,\n",
    "            backup_entropy=backup_entropy,            \n",
    "        ))\n",
    "\n",
    "        return SACAgent(rng, critic=critic, target_critic=target_critic, actor=actor, temp=temp, config=config)\n",
    "\n",
    "def get_default_config():\n",
    "    import ml_collections\n",
    "\n",
    "    return ml_collections.ConfigDict({\n",
    "        'actor_lr': 3e-4,\n",
    "        'critic_lr': 3e-4,\n",
    "        'temp_lr': 3e-4,\n",
    "        'hidden_dims': (256, 256),\n",
    "        'discount': 0.99,\n",
    "        'tau': 0.005,\n",
    "        'target_entropy': ml_collections.config_dict.placeholder(float),\n",
    "        'backup_entropy': True,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 13:04:25.972393: W external/xla/xla/service/gpu/nvptx_compiler.cc:679] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.103). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahdikallel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/tmppj3eebop/wandb/run-20231128_130426-u958rgzs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mahdikallel/jaxrl_m/runs/u958rgzs' target=\"_blank\">eager-wildflower-55</a></strong> to <a href='https://wandb.ai/mahdikallel/jaxrl_m' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mahdikallel/jaxrl_m' target=\"_blank\">https://wandb.ai/mahdikallel/jaxrl_m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mahdikallel/jaxrl_m/runs/u958rgzs' target=\"_blank\">https://wandb.ai/mahdikallel/jaxrl_m/runs/u958rgzs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra kwargs: {'max_steps': 1000000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 94972/1000000 [04:39<44:18, 340.38it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mahdi/Desktop/jaxrl_m/droq_safe.ipynb Cell 2\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/droq_safe.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/droq_safe.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m     batch \u001b[39m=\u001b[39m replay_buffer\u001b[39m.\u001b[39msample(batch_size)  \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/droq_safe.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m     agent, critic_update_info \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mupdate_critic(batch)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/droq_safe.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m batch \u001b[39m=\u001b[39m replay_buffer\u001b[39m.\u001b[39msample(batch_size)      \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/droq_safe.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m agent, actor_update_info \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mupdate_actor(batch)    \n",
      "File \u001b[0;32m~/Desktop/jaxrl_m/.venv/lib/python3.10/site-packages/flax/core/frozen_dict.py:168\u001b[0m, in \u001b[0;36mFrozenDict.tree_unflatten\u001b[0;34m(cls, keys, values)\u001b[0m\n\u001b[1;32m    163\u001b[0m   sorted_keys \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dict)\n\u001b[1;32m    164\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(\n\u001b[1;32m    165\u001b[0m       [(jax\u001b[39m.\u001b[39mtree_util\u001b[39m.\u001b[39mDictKey(k), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dict[k]) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m sorted_keys]\n\u001b[1;32m    166\u001b[0m   ), \u001b[39mtuple\u001b[39m(sorted_keys)\n\u001b[0;32m--> 168\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_unflatten\u001b[39m(\u001b[39mcls\u001b[39m, keys, values):\n\u001b[1;32m    170\u001b[0m   \u001b[39m# data is already deep copied due to tree map mechanism\u001b[39;00m\n\u001b[1;32m    171\u001b[0m   \u001b[39m# we can skip the deep copy in the constructor\u001b[39;00m\n\u001b[1;32m    172\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m({k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(keys, values)}, __unsafe_skip_copy__\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import jax\n",
    "import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "#import examples.mujoco.sac as learner\n",
    "\n",
    "from jaxrl_m.wandb import setup_wandb, default_wandb_config, get_flag_dict\n",
    "import wandb\n",
    "from jaxrl_m.evaluation import supply_rng, evaluate, flatten, EpisodeMonitor\n",
    "from jaxrl_m.dataset import ReplayBuffer\n",
    "\n",
    "#from ml_collections import config_flags\n",
    "import pickle\n",
    "#from flax.training import checkpoints\n",
    "\n",
    "\n",
    "#FLAGS = flags.FLAGS\n",
    "env_name='Hopper-v4'\n",
    "seed=np.random.choice(1000000)\n",
    "eval_episodes=10\n",
    "batch_size = 256\n",
    "max_steps = int(1e6)\n",
    "start_steps = int(1e4)                     \n",
    "log_interval = 10000\n",
    "eval_interval = 10000\n",
    "\n",
    "wandb_config = default_wandb_config()\n",
    "wandb_config.update({\n",
    "    'project': 'd4rl_test',\n",
    "    'group': 'sac_test',\n",
    "    'name': 'sac_{env_name}',\n",
    "})\n",
    "\n",
    "\n",
    "env = EpisodeMonitor(gym.make(env_name))\n",
    "eval_env = EpisodeMonitor(gym.make(env_name))\n",
    "setup_wandb({\"bonjour\":1})\n",
    "\n",
    "example_transition = dict(\n",
    "    observations=env.observation_space.sample(),\n",
    "    actions=env.action_space.sample(),\n",
    "    rewards=0.0,\n",
    "    masks=1.0,\n",
    "    next_observations=env.observation_space.sample(),\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer.create(example_transition, size=int(1e6))\n",
    "\n",
    "#agent = learner.create_learner(seed,\n",
    "agent = create_learner(seed,\n",
    "                example_transition['observations'][None],\n",
    "                example_transition['actions'][None],\n",
    "                max_steps=max_steps,\n",
    "                #**FLAGS.config\n",
    "                )\n",
    "\n",
    "exploration_metrics = dict()\n",
    "obs,info = env.reset()    \n",
    "exploration_rng = jax.random.PRNGKey(0)\n",
    "\n",
    "for i in tqdm.tqdm(range(1, max_steps + 1),\n",
    "                    smoothing=0.1,\n",
    "                    dynamic_ncols=True):\n",
    "\n",
    "    if i < start_steps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        exploration_rng, key = jax.random.split(exploration_rng)\n",
    "        action = agent.sample_actions(obs, seed=key)\n",
    "\n",
    "    #next_obs, reward, done, info = env.step(action)\n",
    "    next_obs, reward, done, truncated, info = env.step(action)\n",
    "    reward = reward/300\n",
    "    mask = float(not done or 'TimeLimit.truncated' in info)\n",
    "    \n",
    "    replay_buffer.add_transition(dict(\n",
    "        observations=obs,\n",
    "        actions=action,\n",
    "        rewards=reward,\n",
    "        masks=mask,\n",
    "        next_observations=next_obs,\n",
    "    ))\n",
    "    obs = next_obs\n",
    "\n",
    "    if (done or truncated):\n",
    "        exploration_metrics = {f'exploration/{k}': v for k, v in flatten(info).items()}\n",
    "        obs,info= env.reset()\n",
    "        episode_reward = 0.0\n",
    "\n",
    "    if replay_buffer.size < start_steps:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    #agent, update_info = agent.update(batch)\n",
    "    for j in range(5):\n",
    "        batch = replay_buffer.sample(batch_size)  \n",
    "        agent, critic_update_info = agent.update_critic(batch)\n",
    "    \n",
    "    batch = replay_buffer.sample(batch_size)      \n",
    "    agent, actor_update_info = agent.update_actor(batch)    \n",
    "    \n",
    "    update_info = {**critic_update_info, **actor_update_info}\n",
    "    \n",
    "\n",
    "    if i % log_interval == 0:\n",
    "        train_metrics = {f'training/{k}': v for k, v in update_info.items()}\n",
    "        wandb.log(train_metrics, step=i)\n",
    "        wandb.log(exploration_metrics, step=i)\n",
    "        exploration_metrics = dict()\n",
    "\n",
    "    if i % eval_interval == 0:\n",
    "        \n",
    "        \n",
    "        policy_fn = partial(supply_rng(agent.sample_actions), temperature=0.0)\n",
    "        eval_info = evaluate(policy_fn, eval_env, num_episodes=eval_episodes)\n",
    "        eval_metrics = {f'evaluation/{k}': v for k, v in eval_info.items()}\n",
    "        wandb.log(eval_metrics, step=i,commit=True)\n",
    "\n",
    "    # if i % FLAGS.save_interval == 0 and FLAGS.save_dir is not None:\n",
    "    #     checkpoints.save_checkpoint(FLAGS.save_dir, agent, i)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
