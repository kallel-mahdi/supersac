{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementations of algorithms for continuous control.\"\"\"\n",
    "import functools\n",
    "from jaxrl_m.typing import *\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from jaxrl_m.common import TrainState, target_update, nonpytree_field\n",
    "from jaxrl_m.networks import Policy, Critic, ensemblize\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "class Temperature(nn.Module):\n",
    "    initial_temperature: float = 1.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self) -> jnp.ndarray:\n",
    "        log_temp = self.param('log_temp',\n",
    "                              init_fn=lambda key: jnp.full(\n",
    "                                  (), jnp.log(self.initial_temperature)))\n",
    "        return jnp.exp(log_temp)\n",
    "\n",
    "class SACAgent(flax.struct.PyTreeNode):\n",
    "    rng: PRNGKey\n",
    "    critic: TrainState\n",
    "    target_critic: TrainState\n",
    "    actor: TrainState\n",
    "    temp: TrainState\n",
    "    config: dict = nonpytree_field()\n",
    "\n",
    "    @jax.jit\n",
    "    def update(agent, batch: Batch):\n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "\n",
    "        def critic_loss_fn(critic_params):\n",
    "            next_dist = agent.actor(batch['next_observations'])\n",
    "            next_actions, next_log_probs = next_dist.sample_and_log_prob(seed=next_key)\n",
    "\n",
    "            next_q1, next_q2 = agent.target_critic(batch['next_observations'], next_actions)\n",
    "            next_q = jnp.minimum(next_q1, next_q2)\n",
    "            target_q = batch['rewards'] + agent.config['discount'] * batch['masks'] * next_q\n",
    "\n",
    "            if agent.config['backup_entropy']:\n",
    "                target_q = target_q - agent.config['discount'] * batch['masks'] * next_log_probs * agent.temp()\n",
    "            \n",
    "            q1, q2 = agent.critic(batch['observations'], batch['actions'], params=critic_params)\n",
    "            critic_loss = ((q1 - target_q)**2 + (q2 - target_q)**2).mean()\n",
    "            \n",
    "            return critic_loss, {\n",
    "                'critic_loss': critic_loss,\n",
    "                'q1': q1.mean(),\n",
    "            }        \n",
    "\n",
    "        def actor_loss_fn(actor_params):\n",
    "            dist = agent.actor(batch['observations'], params=actor_params)\n",
    "            actions, log_probs = dist.sample_and_log_prob(seed=curr_key)\n",
    "            \n",
    "            q1, q2 = agent.critic(batch['observations'], actions)\n",
    "            q = jnp.minimum(q1, q2)\n",
    "\n",
    "            actor_loss = (log_probs * agent.temp() - q).mean()\n",
    "            return actor_loss, {\n",
    "                'actor_loss': actor_loss,\n",
    "                'entropy': -1 * log_probs.mean(),\n",
    "            }\n",
    "        \n",
    "        def temp_loss_fn(temp_params, entropy, target_entropy):\n",
    "            temperature = agent.temp(params=temp_params)\n",
    "            temp_loss = (temperature * (entropy - target_entropy)).mean()\n",
    "            return temp_loss, {\n",
    "                'temp_loss': temp_loss,\n",
    "                'temperature': temperature,\n",
    "            }\n",
    "        \n",
    "        new_critic, critic_info = agent.critic.apply_loss_fn(loss_fn=critic_loss_fn, has_aux=True)\n",
    "        new_target_critic = target_update(agent.critic, agent.target_critic, agent.config['target_update_rate'])\n",
    "        new_actor, actor_info = agent.actor.apply_loss_fn(loss_fn=actor_loss_fn, has_aux=True)\n",
    "\n",
    "        temp_loss_fn = functools.partial(temp_loss_fn, entropy=actor_info['entropy'], target_entropy=agent.config['target_entropy'])\n",
    "        new_temp, temp_info = agent.temp.apply_loss_fn(loss_fn=temp_loss_fn, has_aux=True)\n",
    "\n",
    "        return agent.replace(rng=new_rng, critic=new_critic, target_critic=new_target_critic, actor=new_actor, temp=new_temp), {\n",
    "            **critic_info, **actor_info, **temp_info}\n",
    "\n",
    "    @jax.jit\n",
    "    def sample_actions(agent,   \n",
    "                       observations: np.ndarray,\n",
    "                       seed: PRNGKey,\n",
    "                       random = bool,\n",
    "                       temperature: float = 1.0,\n",
    "                       ) -> jnp.ndarray:\n",
    "        actions = agent.actor(observations, temperature=temperature).sample(seed=seed)\n",
    "        \n",
    "        return actions\n",
    "\n",
    "\n",
    "\n",
    "def create_learner(\n",
    "                 seed: int,\n",
    "                 observations: jnp.ndarray,\n",
    "                 actions: jnp.ndarray,\n",
    "                 actor_lr: float = 3e-4,\n",
    "                 critic_lr: float = 3e-4,\n",
    "                 temp_lr: float = 3e-4,\n",
    "                 hidden_dims: Sequence[int] = (256, 256),\n",
    "                 discount: float = 0.99,\n",
    "                 tau: float = 0.005,\n",
    "                 target_entropy: float = None,\n",
    "                 backup_entropy: bool = True,\n",
    "            **kwargs):\n",
    "\n",
    "        print('Extra kwargs:', kwargs)\n",
    "\n",
    "        rng = jax.random.PRNGKey(seed)\n",
    "        rng, actor_key, critic_key = jax.random.split(rng, 3)\n",
    "\n",
    "        action_dim = actions.shape[-1]\n",
    "        actor_def = Policy(hidden_dims, action_dim=action_dim, \n",
    "            log_std_min=-10.0, state_dependent_std=True, tanh_squash_distribution=True, final_fc_init_scale=1.0)\n",
    "\n",
    "        actor_params = actor_def.init(actor_key, observations)['params']\n",
    "        actor = TrainState.create(actor_def, actor_params, tx=optax.adam(learning_rate=actor_lr))\n",
    "\n",
    "        critic_def = ensemblize(Critic, num_qs=2)(hidden_dims)\n",
    "        critic_params = critic_def.init(critic_key, observations, actions)['params']\n",
    "        critic = TrainState.create(critic_def, critic_params, tx=optax.adam(learning_rate=critic_lr))\n",
    "        target_critic = TrainState.create(critic_def, critic_params)\n",
    "\n",
    "        temp_def = Temperature()\n",
    "        temp_params = temp_def.init(rng)['params']\n",
    "        temp = TrainState.create(temp_def, temp_params, tx=optax.adam(learning_rate=temp_lr))\n",
    "\n",
    "        if target_entropy is None:\n",
    "            target_entropy = -0.5 * action_dim\n",
    "\n",
    "        config = flax.core.FrozenDict(dict(\n",
    "            discount=discount,\n",
    "            target_update_rate=tau,\n",
    "            target_entropy=target_entropy,\n",
    "            backup_entropy=backup_entropy,            \n",
    "        ))\n",
    "\n",
    "        return SACAgent(rng, critic=critic, target_critic=target_critic, actor=actor, temp=temp, config=config)\n",
    "\n",
    "def get_default_config():\n",
    "    import ml_collections\n",
    "\n",
    "    return ml_collections.ConfigDict({\n",
    "        'actor_lr': 3e-4,\n",
    "        'critic_lr': 3e-4,\n",
    "        'temp_lr': 3e-4,\n",
    "        'hidden_dims': (256, 256),\n",
    "        'discount': 0.99,\n",
    "        'tau': 0.005,\n",
    "        'target_entropy': ml_collections.config_dict.placeholder(float),\n",
    "        'backup_entropy': True,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 22:53:23.583279: W external/xla/xla/service/gpu/nvptx_compiler.cc:744] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahdikallel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/tmpstcxo7l7/wandb/run-20240226_225324-sac_test_sac_{env_name}_20240226_225323</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mahdikallel/dormant/runs/sac_test_sac_%7Benv_name%7D_20240226_225323' target=\"_blank\">sac_{env_name}</a></strong> to <a href='https://wandb.ai/mahdikallel/dormant' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mahdikallel/dormant' target=\"_blank\">https://wandb.ai/mahdikallel/dormant</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mahdikallel/dormant/runs/sac_test_sac_%7Benv_name%7D_20240226_225323' target=\"_blank\">https://wandb.ai/mahdikallel/dormant/runs/sac_test_sac_%7Benv_name%7D_20240226_225323</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra kwargs: {'max_steps': 1000000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 15207/1000000 [00:13<18:13, 900.78it/s] \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 10000 is less than current step: 10001. Dropping entry: {'evaluation/disc_policy_return': 0.11572885170505745, 'evaluation/undisc_policy_return': -0.21250671668714666, '_timestamp': 1708984411.6786757}).\n",
      "  3%|▎         | 30299/1000000 [00:28<21:15, 760.27it/s] \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 20000 is less than current step: 20001. Dropping entry: {'evaluation/disc_policy_return': 74.23428129870408, 'evaluation/undisc_policy_return': 251.9951176810695, '_timestamp': 1708984425.147226}).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 30000 is less than current step: 30001. Dropping entry: {'evaluation/disc_policy_return': 82.74711044684254, 'evaluation/undisc_policy_return': 239.11496829996037, '_timestamp': 1708984434.6490128}).\n",
      "  3%|▎         | 33495/1000000 [00:31<15:06, 1066.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39msample_actions(obs, seed\u001b[38;5;241m=\u001b[39mkey)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#next_obs, reward, done, info = env.step(action)\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m next_obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimeLimit.truncated\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m info)\n\u001b[1;32m     79\u001b[0m replay_buffer\u001b[38;5;241m.\u001b[39madd_transition(\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     80\u001b[0m     observations\u001b[38;5;241m=\u001b[39mobs,\n\u001b[1;32m     81\u001b[0m     actions\u001b[38;5;241m=\u001b[39maction,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     next_observations\u001b[38;5;241m=\u001b[39mnext_obs,\n\u001b[1;32m     85\u001b[0m ))\n",
      "File \u001b[0;32m~/Desktop/supersac/jaxrl_m/evaluation.py:148\u001b[0m, in \u001b[0;36mEpisodeMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# observation, reward, done, info = self.env.step(action)\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     observation, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_length \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/gymnasium/envs/mujoco/walker2d_v4.py:261\u001b[0m, in \u001b[0;36mWalker2dEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    258\u001b[0m x_position_after \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mqpos[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    259\u001b[0m x_velocity \u001b[38;5;241m=\u001b[39m (x_position_after \u001b[38;5;241m-\u001b[39m x_position_before) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt\n\u001b[0;32m--> 261\u001b[0m ctrl_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m forward_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_reward_weight \u001b[38;5;241m*\u001b[39m x_velocity\n\u001b[1;32m    264\u001b[0m healthy_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhealthy_reward\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/gymnasium/envs/mujoco/walker2d_v4.py:224\u001b[0m, in \u001b[0;36mWalker2dEnv.control_cost\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_cost\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 224\u001b[0m     control_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctrl_cost_weight \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m control_cost\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/array.py:407\u001b[0m, in \u001b[0;36mArrayImpl.__array__\u001b[0;34m(self, dtype, context)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 407\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/profiler.py:335\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 335\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    337\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import jax\n",
    "import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "#import examples.mujoco.sac as learner\n",
    "\n",
    "from jaxrl_m.wandb import setup_wandb, default_wandb_config, get_flag_dict\n",
    "import wandb\n",
    "from jaxrl_m.evaluation import supply_rng, evaluate, flatten, EpisodeMonitor\n",
    "from jaxrl_m.dataset import ReplayBuffer\n",
    "from jaxrl_m.rollout import rollout_policy2,rollout_policy\n",
    "#from ml_collections import config_flags\n",
    "import pickle\n",
    "#from flax.training import checkpoints\n",
    "\n",
    "\n",
    "#FLAGS = flags.FLAGS\n",
    "env_name='Walker2d-v4'\n",
    "seed=np.random.choice(1000000)\n",
    "eval_episodes=10\n",
    "batch_size = 256\n",
    "max_steps = int(1e6)\n",
    "start_steps = int(1e4)                     \n",
    "log_interval = 10000\n",
    "eval_interval = 10000\n",
    "\n",
    "wandb_config = default_wandb_config()\n",
    "wandb_config.update({\n",
    "    'project': 'dormant',\n",
    "    'group': 'sac_test',\n",
    "    'name': 'sac_{env_name}',\n",
    "})\n",
    "\n",
    "\n",
    "env = EpisodeMonitor(gym.make(env_name))\n",
    "eval_env = EpisodeMonitor(gym.make(env_name))\n",
    "setup_wandb(**wandb_config,hyperparam_dict={})\n",
    "\n",
    "example_transition = dict(\n",
    "    observations=env.observation_space.sample(),\n",
    "    actions=env.action_space.sample(),\n",
    "    rewards=0.0,\n",
    "    masks=1.0,\n",
    "    next_observations=env.observation_space.sample(),\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer.create(example_transition, size=int(1e6))\n",
    "placeholder = ReplayBuffer.create(example_transition, size=int(1e6))\n",
    "\n",
    "agent = create_learner(seed,\n",
    "                example_transition['observations'][None],\n",
    "                example_transition['actions'][None],\n",
    "                max_steps=max_steps,\n",
    "                #**FLAGS.config\n",
    "                )\n",
    "\n",
    "exploration_metrics = dict()\n",
    "obs,info = env.reset()    \n",
    "exploration_rng = jax.random.PRNGKey(0)\n",
    "\n",
    "for i in tqdm.tqdm(range(1, max_steps + 1),\n",
    "                    smoothing=0.1,\n",
    "                    dynamic_ncols=True):\n",
    "\n",
    "    if i < start_steps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        exploration_rng, key = jax.random.split(exploration_rng)\n",
    "        action = agent.sample_actions(obs, seed=key)\n",
    "\n",
    "    #next_obs, reward, done, info = env.step(action)\n",
    "    next_obs, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    mask = float(not done or 'TimeLimit.truncated' in info)\n",
    "    \n",
    "    replay_buffer.add_transition(dict(\n",
    "        observations=obs,\n",
    "        actions=action,\n",
    "        rewards=reward,\n",
    "        masks=mask,\n",
    "        next_observations=next_obs,\n",
    "    ))\n",
    "    obs = next_obs\n",
    "\n",
    "    if (done or truncated):\n",
    "        exploration_metrics = {f'exploration/{k}': v for k, v in flatten(info).items()}\n",
    "        obs,info= env.reset()\n",
    "\n",
    "    if replay_buffer.size < start_steps:\n",
    "        continue\n",
    "\n",
    "    batch = replay_buffer.sample(batch_size)  \n",
    "    agent, update_info = agent.update(batch)\n",
    "\n",
    "    if i % log_interval == 0:\n",
    "        train_metrics = {f'training/{k}': v for k, v in update_info.items()}\n",
    "        wandb.log(train_metrics, step=i)\n",
    "        wandb.log(exploration_metrics, step=i)\n",
    "        exploration_metrics = dict()\n",
    "\n",
    "    if i % eval_interval == 0:\n",
    "        \n",
    "        \n",
    "        # policy_fn = partial(supply_rng(agent.sample_actions), temperature=0.0)\n",
    "        # eval_info = evaluate(policy_fn, eval_env, num_episodes=eval_episodes)\n",
    "        # eval_metrics = {f'evaluation/{k}': v for k, v in eval_info.items()}\n",
    "        _,_,_,disc_policy_return,_,undisc_policy_return_e,_ = rollout_policy(agent,eval_env,exploration_rng,\n",
    "                                                                None,None,warmup=False,\n",
    "                                                                num_rollouts=10,random=False,\n",
    "                                                            )\n",
    "        eval_info = {'disc_policy_return': disc_policy_return,'undisc_policy_return': undisc_policy_return_e}\n",
    "        eval_metrics = {f'evaluation/{k}': v for k, v in eval_info.items()}\n",
    "        wandb.log(eval_metrics, step=int(i),commit=True)\n",
    "        #wandb.log(eval_metrics, step=i)\n",
    "\n",
    "    # if i % FLAGS.save_interval == 0 and FLAGS.save_dir is not None:\n",
    "    #     checkpoints.save_checkpoint(FLAGS.save_dir, agent, i)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
