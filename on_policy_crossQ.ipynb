{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementations of algorithms for continuous control.\"\"\"\n",
    "import functools\n",
    "from jaxrl_m.typing import *\n",
    "\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from jaxrl_m.common import TrainState, target_update, nonpytree_field\n",
    "from jaxrl_m.networks import DeterministicPolicy,Policy, Critic, ensemblize,ensemblize2\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from flax.training import train_state\n",
    "\n",
    "\n",
    "\n",
    "class SACAgent(flax.struct.PyTreeNode):\n",
    "    rng: PRNGKey\n",
    "    critic: TrainState\n",
    "    target_critic: TrainState\n",
    "    actor: TrainState\n",
    "    config: dict = nonpytree_field()\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    @jax.jit    \n",
    "    def reset_critic_optimizer(agent):\n",
    "    \n",
    "        new_opt_state = agent.critic.tx.init(agent.critic.params)\n",
    "        new_critic = agent.critic.replace(opt_state=new_opt_state)\n",
    "        \n",
    "        return agent.replace(critic=new_critic)\n",
    "        \n",
    "    @partial(jax.jit,static_argnames=('num_steps',))  \n",
    "    def update_critic(agent, transitions: Batch,idxs:jnp.array,num_steps:int):\n",
    "        \n",
    "        \n",
    "        \n",
    "        def one_update(agent, batch: Batch):\n",
    "                \n",
    "            new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "\n",
    "        \n",
    "            def critic_loss_fn(critic_params):\n",
    "                next_actions = agent.actor(batch['next_observations'])\n",
    "                concat_actions = jnp.concatenate([batch[\"actions\"],next_actions])\n",
    "                concat_observations = jnp.concatenate([batch[\"observations\"],batch[\"next_observations\"]])\n",
    "                \n",
    "                \n",
    "                concat_q,updates = agent.critic(concat_observations, concat_actions,True,\n",
    "                                                params=critic_params,mutable=['batch_stats'])\n",
    "                \n",
    "                # concat_q = agent.critic(concat_observations, concat_actions,\n",
    "                #                         True,params=critic_params)\n",
    "                \n",
    "            \n",
    "                q,next_q = jnp.split(concat_q,2,axis=1)\n",
    "                \n",
    "                next_q = jnp.min(next_q,axis=0)\n",
    "             \n",
    "                target_q = batch['rewards'] + agent.config['discount'] * batch['masks'] * next_q\n",
    "                target_q = jax.lax.stop_gradient(target_q)\n",
    "                \n",
    "                critic_loss = ((target_q-q)**2).mean()\n",
    "                \n",
    "                return critic_loss, updates\n",
    "                #return critic_loss, {}\n",
    "            \n",
    "            #new_critic,_ = agent.critic.apply_loss_fn(loss_fn=critic_loss_fn, has_aux=True)\n",
    "            new_critic, updates = agent.critic.apply_loss_fn(loss_fn=critic_loss_fn, has_aux=True)\n",
    "            new_critic = new_critic.replace(batch_stats=updates[\"batch_stats\"])\n",
    "            \n",
    "            return agent.replace(rng=new_rng, critic=new_critic)\n",
    "        \n",
    "        \n",
    "        get_batch = lambda transitions,idx : jax.tree_map(lambda x : x[idx],transitions)\n",
    "        \n",
    "        agent = jax.lax.fori_loop(0, num_steps, \n",
    "                        lambda i, agent: one_update(agent,get_batch(transitions,idxs[i])),\n",
    "                        agent)\n",
    "        \n",
    "        return agent,{}\n",
    "       \n",
    "    \n",
    "\n",
    "        \n",
    "    @jax.jit\n",
    "    def update_actor(agent, batch: Batch):\n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "\n",
    "        def actor_loss_fn(actor_params):\n",
    "            \n",
    "            actions,updates = agent.actor(batch['observations'],True,params=actor_params,mutable=['batch_stats'])\n",
    "            qs = agent.critic(batch['observations'], actions)\n",
    "            q = qs.mean(axis=0)\n",
    "            \n",
    "            actor_loss = (-q).mean()\n",
    "            \n",
    "            return actor_loss, updates,actor_loss\n",
    "\n",
    "        new_actor, updates,actor_info = agent.actor.apply_loss_fn(loss_fn=actor_loss_fn, has_aux=True)\n",
    "        new_actor = new_actor.replace(batch_stats=updates[\"batch_stats\"])\n",
    "        return agent.replace(rng=new_rng,actor=new_actor,), {**actor_info}\n",
    "        \n",
    "\n",
    "\n",
    "    @jax.jit\n",
    "    def sample_actions(agent,observations: np.ndarray) -> jnp.ndarray:\n",
    "        actions = agent.actor(observations)\n",
    "       \n",
    "        return actions\n",
    "    \n",
    " \n",
    "\n",
    "def create_learner(\n",
    "                 seed: int,\n",
    "                 observations: jnp.ndarray,\n",
    "                 actions: jnp.ndarray,\n",
    "                 actor_lr: float = 3e-4,\n",
    "                 critic_lr: float = 3e-4,\n",
    "                 hidden_dims: Sequence[int] = (256, 256),\n",
    "                 discount: float = 0.99,\n",
    "                 tau: float = 0.005,\n",
    "            **kwargs):\n",
    "\n",
    "        print('Extra kwargs:', kwargs)\n",
    "\n",
    "        rng = jax.random.PRNGKey(seed)\n",
    "        rng, actor_key, critic_key = jax.random.split(rng, 3)\n",
    "\n",
    "        action_dim = actions.shape[-1]\n",
    "        actor_def = DeterministicPolicy((64,64), action_dim=action_dim,final_fc_init_scale=1.0)\n",
    "\n",
    "      \n",
    "        critic_def = ensemblize2(Critic, num_qs=2)(hidden_dims)\n",
    "        critic_variables = critic_def.init(critic_key, observations, actions,False)\n",
    "        critic_params = critic_variables[\"params\"]\n",
    "        critic_stats = critic_variables[\"batch_stats\"]\n",
    "        critic = TrainState.create(critic_def, critic_params,critic_stats,tx=optax.adam(learning_rate=critic_lr))\n",
    "        \n",
    "        actor_variables = actor_def.init(actor_key, observations,False)\n",
    "        actor_params = actor_variables[\"params\"]\n",
    "        actor_stats = actor_params[\"batch_stats\"]\n",
    "        actor = TrainState.create(actor_def, actor_params,actor_stats,tx=optax.adam(learning_rate=actor_lr))\n",
    "        \n",
    "\n",
    "        config = flax.core.FrozenDict(dict(\n",
    "            discount=discount,\n",
    "            target_update_rate=tau,    \n",
    "        ))\n",
    "\n",
    "        return SACAgent(rng, critic=critic, target_critic=critic, actor=actor, config=config)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from jaxrl_m.rollout import PolicyRollout,rollout_policy\n",
    "\n",
    "\n",
    "def f(anc_agent,obs,actor):\n",
    "\n",
    "    actions = anc_agent.actor(obs, params=actor)\n",
    "    qs = anc_agent.critic(obs, actions,False,params=anc_agent.critic.params)\n",
    "    q = qs.mean(axis=0)\n",
    "   \n",
    "    return q\n",
    "    \n",
    "\n",
    "@jax.jit\n",
    "def estimate_return(anc_agent,anc_return,acq_rollout:PolicyRollout,):\n",
    "    \n",
    "    acq_obs = acq_rollout.observations\n",
    "    acq_masks = acq_rollout.disc_masks\n",
    "  \n",
    "    acq_actor = acq_rollout.policy_params\n",
    "    acq_return = acq_rollout.policy_return\n",
    "    \n",
    "    anc_actor = anc_agent.actor.params\n",
    "    \n",
    "    acq_q = f(anc_agent,acq_obs,acq_actor)\n",
    "    anc_q = f(anc_agent,acq_obs,anc_actor)\n",
    "    \n",
    "    adv = ((acq_q - anc_q)*acq_masks).sum()/5\n",
    "    acq_return_pred = anc_return + adv\n",
    "  \n",
    "    \n",
    "    return acq_return_pred,acq_return\n",
    "\n",
    "\n",
    "def evaluate_critic(anc_agent,anc_return,policy_rollouts):\n",
    "\n",
    "    y_pred,y= [],[]\n",
    "    for policy_rollout in policy_rollouts:\n",
    "        \n",
    "        acq_return_pred,acq_return = estimate_return(anc_agent,anc_return,policy_rollout)\n",
    "        y_pred.append(acq_return_pred),y.append(acq_return)\n",
    "        \n",
    "    y_pred,y = np.array(y_pred),np.array(y)\n",
    "    a2 = jnp.clip(((y-y_pred)**2),a_min=1e-4).sum()\n",
    "    b2=((y-y.mean())**2).sum()\n",
    "    R2 = 1-(a2/b2)  \n",
    "    bias = (y_pred-y).mean()\n",
    "    \n",
    "    return R2,bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 15:38:53.869581: W external/xla/xla/service/gpu/nvptx_compiler.cc:679] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.103). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahdikallel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/tmpabkiw_2d/wandb/run-20231219_153854-61d4vm8c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mahdikallel/jaxrl_m/runs/61d4vm8c' target=\"_blank\">lilac-river-203</a></strong> to <a href='https://wandb.ai/mahdikallel/jaxrl_m' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mahdikallel/jaxrl_m' target=\"_blank\">https://wandb.ai/mahdikallel/jaxrl_m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mahdikallel/jaxrl_m/runs/61d4vm8c' target=\"_blank\">https://wandb.ai/mahdikallel/jaxrl_m/runs/61d4vm8c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra kwargs: {'max_steps': 1000000}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tuple arity mismatch: 3 != 2; tuple: (array([[ 0.57683103,  1.62721348, -0.03782179,  0.32784108, -0.90224621,\n         0.36969894, -1.02813124,  0.07521067, -0.2945554 ,  1.62219445,\n         2.64148087]]), array([[ 0.18315211,  0.00380711, -0.35931978]], dtype=float32), False).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb#W2sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m replay_buffer \u001b[39m=\u001b[39m ReplayBuffer\u001b[39m.\u001b[39mcreate(example_transition, size\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(\u001b[39m1e6\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb#W2sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m actor_buffer \u001b[39m=\u001b[39m ReplayBuffer\u001b[39m.\u001b[39mcreate(example_transition, size\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(\u001b[39m5e3\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb#W2sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m agent \u001b[39m=\u001b[39m create_learner(seed,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb#W2sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m                 example_transition[\u001b[39m'\u001b[39;49m\u001b[39mobservations\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39mNone\u001b[39;49;00m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb#W2sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m                 example_transition[\u001b[39m'\u001b[39;49m\u001b[39mactions\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39mNone\u001b[39;49;00m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb#W2sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m                 max_steps\u001b[39m=\u001b[39;49mmax_steps,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb#W2sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m                 \u001b[39m#**FLAGS.config\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb#W2sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m                 )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb#W2sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m exploration_metrics \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb#W2sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m obs,info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()    \n",
      "\u001b[1;32m/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb#W2sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m actor_def \u001b[39m=\u001b[39m DeterministicPolicy((\u001b[39m64\u001b[39m,\u001b[39m64\u001b[39m), action_dim\u001b[39m=\u001b[39maction_dim,final_fc_init_scale\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb#W2sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m critic_def \u001b[39m=\u001b[39m ensemblize2(Critic, num_qs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)(hidden_dims)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb#W2sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m critic_variables \u001b[39m=\u001b[39m critic_def\u001b[39m.\u001b[39;49minit(critic_key, observations, actions,\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb#W2sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m critic_params \u001b[39m=\u001b[39m critic_variables[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/mahdi/Desktop/supersac/on_policy_crossQ.ipynb#W2sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m critic_stats \u001b[39m=\u001b[39m critic_variables[\u001b[39m\"\u001b[39m\u001b[39mbatch_stats\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/tree_util.py:243\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Maps a multi-input function over pytree args to produce a new pytree.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39m  [[5, 7, 9], [6, 1, 2]]\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    242\u001b[0m leaves, treedef \u001b[39m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[0;32m--> 243\u001b[0m all_leaves \u001b[39m=\u001b[39m [leaves] \u001b[39m+\u001b[39m [treedef\u001b[39m.\u001b[39;49mflatten_up_to(r) \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m rest]\n\u001b[1;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m treedef\u001b[39m.\u001b[39munflatten(f(\u001b[39m*\u001b[39mxs) \u001b[39mfor\u001b[39;00m xs \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mall_leaves))\n",
      "\u001b[0;31mValueError\u001b[0m: Tuple arity mismatch: 3 != 2; tuple: (array([[ 0.57683103,  1.62721348, -0.03782179,  0.32784108, -0.90224621,\n         0.36969894, -1.02813124,  0.07521067, -0.2945554 ,  1.62219445,\n         2.64148087]]), array([[ 0.18315211,  0.00380711, -0.35931978]], dtype=float32), False)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import jax\n",
    "import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "from jaxrl_m.wandb import setup_wandb, default_wandb_config, get_flag_dict\n",
    "import wandb\n",
    "from jaxrl_m.evaluation import supply_rng, evaluate, flatten, EpisodeMonitor\n",
    "from jaxrl_m.dataset import ReplayBuffer\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "env_name='Hopper-v4'\n",
    "seed=np.random.choice(1000000)\n",
    "eval_episodes=10\n",
    "batch_size = 256\n",
    "max_steps = int(1e6)\n",
    "start_steps = 50000                   \n",
    "log_interval = 5000\n",
    "#eval_interval = 10000\n",
    "\n",
    "wandb_config = default_wandb_config()\n",
    "wandb_config.update({\n",
    "    'project': 'd4rl_test',\n",
    "    'group': 'sac_test',\n",
    "    'name': 'sac_{env_name}',\n",
    "})\n",
    "\n",
    "\n",
    "env = EpisodeMonitor(gym.make(env_name))\n",
    "eval_env = EpisodeMonitor(gym.make(env_name))\n",
    "setup_wandb({\"bonjour\":1})\n",
    "\n",
    "example_transition = dict(\n",
    "    observations=env.observation_space.sample(),\n",
    "    actions=env.action_space.sample(),\n",
    "    rewards=0.0,\n",
    "    masks=1.0,\n",
    "    next_observations=env.observation_space.sample(),\n",
    "    discounts=1.0,\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer.create(example_transition, size=int(1e6))\n",
    "actor_buffer = ReplayBuffer.create(example_transition, size=int(5e3))\n",
    "\n",
    "agent = create_learner(seed,\n",
    "                example_transition['observations'][None],\n",
    "                example_transition['actions'][None],\n",
    "                max_steps=max_steps,\n",
    "                #**FLAGS.config\n",
    "                )\n",
    "\n",
    "exploration_metrics = dict()\n",
    "obs,info = env.reset()    \n",
    "exploration_rng = jax.random.PRNGKey(0)\n",
    "i = 0\n",
    "unlogged_steps = 0\n",
    "policy_rollouts = deque([], maxlen=10)\n",
    "with tqdm.tqdm(total=max_steps) as pbar:\n",
    "    \n",
    "    while i < max_steps:\n",
    "    \n",
    "        replay_buffer,actor_buffer,policy_rollout,policy_return,num_steps = rollout_policy(agent,env,exploration_rng,\n",
    "                replay_buffer,actor_buffer,\n",
    "                warmup=(i < start_steps))\n",
    "        policy_rollouts.append(policy_rollout)\n",
    "        unlogged_steps += num_steps\n",
    "        i+=num_steps\n",
    "        pbar.update(num_steps)\n",
    "        \n",
    "            \n",
    "        if replay_buffer.size > start_steps:\n",
    "        \n",
    "        \n",
    "            transitions = replay_buffer.get_all()\n",
    "            idxs = jax.random.choice(a=replay_buffer.size, shape=(5000,256), replace=True,key=jax.random.PRNGKey(0))\n",
    "            agent = agent.reset_critic_optimizer()\n",
    "            agent, critic_update_info = agent.update_critic(transitions,idxs,5000)\n",
    "            R2,bias = evaluate_critic(agent,policy_rollouts[-1].policy_return,policy_rollouts)\n",
    "\n",
    "            \n",
    "            actor_batch = actor_buffer.get_all()      \n",
    "            agent, actor_update_info = agent.update_actor(actor_batch)    \n",
    "            update_info = {**critic_update_info, **actor_update_info, 'R2_validation': R2,'bias': bias}\n",
    "            \n",
    "            \n",
    "            if unlogged_steps > log_interval:\n",
    "                exploration_metrics = {f'exploration/disc_return': policy_return}\n",
    "                wandb.log(exploration_metrics, step=int(i),commit=False)\n",
    "                train_metrics = {f'training/{k}': v for k, v in update_info.items()}\n",
    "                wandb.log(train_metrics, step=int(i),commit=False)\n",
    "                #wandb.log(exploration_metrics, step=i)\n",
    "                policy_fn = agent.actor\n",
    "                eval_info = evaluate(policy_fn, eval_env, num_episodes=eval_episodes)\n",
    "                eval_metrics = {f'evaluation/{k}': v for k, v in eval_info.items()}\n",
    "                print('evaluating')\n",
    "                wandb.log(eval_metrics, step=int(i),commit=True)\n",
    "                unlogged_steps = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from jax.random import PRNGKey\n",
    "import jax\n",
    "\n",
    "\n",
    "class Foo(nn.Module):\n",
    "    features: int\n",
    "    axis_name: str\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x,train):\n",
    "        x = nn.Dense(features=self.features)(x)\n",
    "        x = nn.BatchNorm(axis_name=self.axis_name)(x, use_running_average=not train)\n",
    "        return x\n",
    "\n",
    "\n",
    "# prepare data\n",
    "BATCH_SIZE = 12\n",
    "INPUT_SIZE = 3\n",
    "FEATURE_SIZE = 5\n",
    "\n",
    "minibatch = jnp.ones((BATCH_SIZE, INPUT_SIZE))\n",
    "\n",
    "# prepare modules and instances\n",
    "vmap_config = {\n",
    "    \"variable_axes\": {\"params\": 0, \"batch_stats\": 0},\n",
    "    \"split_rngs\": {\"params\": True, \"batch_stats\": True},\n",
    "    \"in_axes\": (None,None),\n",
    "    \"out_axes\": 0,\n",
    "    \"axis_size\":3,\n",
    "    #\"axis_name\": \"batch\",\n",
    "}\n",
    "\n",
    "\n",
    "foo = Foo(features=FEATURE_SIZE, axis_name=None)\n",
    "#vmap_foo = VmapFoo(features=FEATURE_SIZE, axis_name=\"batch\")\n",
    "\n",
    "\n",
    "# get variables from foo\n",
    "rngs = {\"params\": PRNGKey(0), \"batch_stats\": PRNGKey(1)}\n",
    "\n",
    "\n",
    "# apply modules\n",
    "train = True\n",
    "mutable = [\"batch_stats\"]\n",
    "\n",
    "# variables = foo.init(rngs, False, minibatch[0])\n",
    "# foo_output, foo_state = foo.apply(variables, train, minibatch, mutable=mutable)\n",
    "\n",
    "vmap_def = nn.vmap(Foo, **vmap_config)(features=FEATURE_SIZE, axis_name=None)\n",
    "vmap_variables = vmap_def.init(rngs,minibatch,False)\n",
    "# vmap_foo_output, vmap_foo_state = vmap_foo.apply(variables, train, minibatch, mutable=mutable)\n",
    "# assert vmap_foo_output.shape == (BATCH_SIZE_1, FEATURE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params,updates = vmap_def.apply(vmap_variables,minibatch,True,mutable=mutable)\n",
    "updates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
