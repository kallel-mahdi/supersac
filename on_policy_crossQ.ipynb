{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementations of algorithms for continuous control.\"\"\"\n",
    "import functools\n",
    "from jaxrl_m.typing import *\n",
    "\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from jaxrl_m.common import TrainState, target_update, nonpytree_field\n",
    "from jaxrl_m.networks import DeterministicPolicy,Policy, Critic, ensemblize\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "class SACAgent(flax.struct.PyTreeNode):\n",
    "    rng: PRNGKey\n",
    "    critic: (TrainState)\n",
    "    actor: TrainState\n",
    "    config: dict = nonpytree_field()\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "    @partial(jax.jit,static_argnames=('num_steps',))  \n",
    "    \n",
    "    \n",
    "    def update_many_critics(agent,transitions: Batch,idxs:jnp.array,num_steps:int):\n",
    "    \n",
    "        def update_one_critic(critic_params,critic_opt_state,\n",
    "                            agent,transitions,idxs,num_steps):\n",
    "            \n",
    "            \n",
    "            def one_update(agent,\n",
    "                        critic_params,critic_opt_state,\n",
    "                        batch: Batch):\n",
    "                                  \n",
    "                def critic_loss_fn(critic_params):\n",
    "                    next_actions = agent.actor(batch['next_observations'])\n",
    "                    concat_actions = jnp.concatenate([batch[\"actions\"],next_actions])\n",
    "                    concat_observations = jnp.concatenate([batch[\"observations\"],batch[\"next_observations\"]])\n",
    "                    \n",
    "                    concat_q = agent.critic(concat_observations, concat_actions,\n",
    "                                            True,params=critic_params)\n",
    "                    q,next_q = jnp.split(concat_q,2,axis=0) ## axis=1 for ensemble\n",
    "                    \n",
    "                    target_q = batch['rewards'] + agent.config['discount'] * batch['masks'] * next_q\n",
    "                    target_q = jax.lax.stop_gradient(target_q)\n",
    "                    \n",
    "                    critic_loss = ((target_q-q)**2).mean()\n",
    "                    \n",
    "                    return critic_loss, {}\n",
    "        \n",
    "\n",
    "                grads,_ = jax.grad(critic_loss_fn, has_aux=True)(critic_params)\n",
    "                updates, new_opt_state = agent.critic.tx.update(grads, critic_opt_state, critic_params)\n",
    "                new_params = optax.apply_updates(critic_params, updates)\n",
    "\n",
    "                \n",
    "                return agent,new_params,new_opt_state\n",
    "            \n",
    "            \n",
    "            get_batch = lambda transitions,idx : jax.tree_map(lambda x : x[idx],transitions)\n",
    "                \n",
    "            agent,critic_params,critic_opt_state = jax.lax.fori_loop(0, num_steps, \n",
    "                        lambda i, args: one_update(*args,get_batch(transitions,idxs[i])),\n",
    "                        (agent,critic_params,critic_opt_state))\n",
    "            \n",
    "            return critic_params,critic_opt_state\n",
    "        \n",
    "        \n",
    "        \n",
    "        critic = agent.critic\n",
    "        opt_state = jax.vmap(critic.tx.init,in_axes=0)(agent.critic.params)\n",
    "        critic = critic.replace(opt_state=opt_state)\n",
    "        tmp = partial(update_one_critic,agent=agent,transitions=transitions,idxs=idxs,num_steps=num_steps)\n",
    "        critic_params,critic_opt_state = jax.vmap(tmp,in_axes=(0,0))(critic.params,critic.opt_state)\n",
    "        new_critic = critic.replace(params=critic_params,opt_state=critic_opt_state)\n",
    "        agent = agent.replace(critic=new_critic)\n",
    "        \n",
    "    \n",
    "        return agent,{}\n",
    "       \n",
    "    \n",
    "\n",
    "        \n",
    "    @jax.jit\n",
    "    def update_actor(agent, batch: Batch,q_weights):\n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "\n",
    "        def actor_loss_fn(actor_params,q_weights):\n",
    "            \n",
    "            actions = agent.actor(batch['observations'], params=actor_params)\n",
    "            \n",
    "            call_one_critic = lambda observations,actions,params : agent.critic(observations,actions,params=params)\n",
    "            q = jax.vmap(call_one_critic,in_axes=(None,None,0))(batch['observations'], actions,agent.critic.params)\n",
    "            q_weights = jax.nn.softmax(q_weights,axis=0)\n",
    "            q = jnp.sum(q_weights*q,axis=0)\n",
    "            \n",
    "            actor_loss = (-q).mean()\n",
    "            \n",
    "            return actor_loss, {\n",
    "                'actor_loss': actor_loss,\n",
    "              \n",
    "            }\n",
    "\n",
    "        loss_fn = partial(actor_loss_fn,q_weights=q_weights)\n",
    "        new_actor, actor_info = agent.actor.apply_loss_fn(loss_fn=loss_fn, has_aux=True)\n",
    "\n",
    "        return agent.replace(rng=new_rng,actor=new_actor,), {**actor_info}\n",
    "        \n",
    "\n",
    "\n",
    "    @jax.jit\n",
    "    def sample_actions(agent,observations: np.ndarray) -> jnp.ndarray:\n",
    "        actions = agent.actor(observations)\n",
    "       \n",
    "        return actions\n",
    "    \n",
    " \n",
    "\n",
    "def create_learner(\n",
    "                 seed: int,\n",
    "                 observations: jnp.ndarray,\n",
    "                 actions: jnp.ndarray,\n",
    "                 actor_lr: float = 3e-4,\n",
    "                 critic_lr: float = 3e-4,\n",
    "                 hidden_dims: Sequence[int] = (256, 256),\n",
    "                 discount: float = 0.99,\n",
    "                 tau: float = 0.005,\n",
    "            **kwargs):\n",
    "\n",
    "        print('Extra kwargs:', kwargs)\n",
    "\n",
    "        rng = jax.random.PRNGKey(seed)\n",
    "        rng, actor_key, critic_key = jax.random.split(rng, 3)\n",
    "\n",
    "        action_dim = actions.shape[-1]\n",
    "        actor_def = DeterministicPolicy((64,64), action_dim=action_dim,final_fc_init_scale=1.0)\n",
    "\n",
    "        actor_params = actor_def.init(actor_key, observations)['params']\n",
    "        actor = TrainState.create(actor_def, actor_params, tx=optax.adam(learning_rate=actor_lr))\n",
    "        \n",
    "        \n",
    "        critic_def = Critic(hidden_dims)\n",
    "        critic_params = critic_def.init(critic_key, observations, actions,False)['params']\n",
    "        tmp_critic = TrainState.create(critic_def, critic_params,optax.adam(learning_rate=critic_lr))\n",
    "        \n",
    "        critic_keys  = jax.random.split(critic_key, 5)\n",
    "        critic_params = jax.vmap(critic_def.init,in_axes=(0,None,None))(critic_keys, observations, actions)['params']\n",
    "        critic = tmp_critic.replace(params=critic_params) ## opt_state is updated in \"update_many_critics\"\n",
    "\n",
    "        \n",
    "        config = flax.core.FrozenDict(dict(\n",
    "            discount=discount,\n",
    "            target_update_rate=tau,    \n",
    "        ))\n",
    "\n",
    "        return SACAgent(rng, critic=critic, actor=actor, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def compute_q(anc_agent,obs,actor_params,critic_params):\n",
    "\n",
    "    actions = anc_agent.actor(obs, params=actor_params)\n",
    "    q = anc_agent.critic(obs, actions,False,params=critic_params)\n",
    "   \n",
    "    return q\n",
    "    \n",
    "    \n",
    "\n",
    "def estimate_return(acq_rollout,\n",
    "                    anc_agent,anc_critic_params,anc_return):\n",
    "    \n",
    "    acq_obs = acq_rollout.observations\n",
    "    acq_masks = acq_rollout.disc_masks\n",
    "    acq_return = acq_rollout.policy_return\n",
    "  \n",
    "    \n",
    "    anc_actor_params = anc_agent.actor.params\n",
    "    acq_actor_params = acq_rollout.policy_params\n",
    "    \n",
    "    \n",
    "    anc_q = compute_q(anc_agent,acq_obs,anc_actor_params,anc_critic_params)\n",
    "    acq_q = compute_q(anc_agent,acq_obs,acq_actor_params,anc_critic_params)\n",
    "    \n",
    "    adv = ((acq_q - anc_q)*acq_masks).sum()/5\n",
    "    acq_return_pred = anc_return + adv\n",
    "  \n",
    "    \n",
    "    return acq_return_pred,acq_return\n",
    "\n",
    "\n",
    "def evaluate_critic(anc_agent,anc_critic_params,\n",
    "                    anc_return,policy_rollouts):\n",
    "\n",
    "    \n",
    "    tmp =  partial(estimate_return,\n",
    "                   anc_agent=anc_agent,\n",
    "                   anc_critic_params =anc_critic_params,\n",
    "                   anc_return = anc_return)\n",
    "    y_pred,y = jax.vmap(tmp)(policy_rollouts)\n",
    "    a2 = jnp.clip(((y-y_pred)**2),a_min=1e-4).sum()\n",
    "    b2=((y-y.mean())**2).sum()\n",
    "    R2 = 1-(a2/b2)  \n",
    "    bias = (y_pred-y).mean()\n",
    "    \n",
    "    return R2,bias\n",
    "\n",
    "@jax.jit\n",
    "def evaluate_critics(anc_agent,anc_critic_params,\n",
    "                    anc_return,policy_rollouts):\n",
    "    \n",
    "    return jax.vmap(evaluate_critic,in_axes=(None,0,None,None))(anc_agent,anc_critic_params,anc_return,policy_rollouts)\n",
    "\n",
    "\n",
    "\n",
    "def merge(x,y):\n",
    "\n",
    "    return jax.tree_map(lambda x,y : jnp.vstack([x,y]),x,y)\n",
    "\n",
    "def flatten_rollouts(policy_rollouts):\n",
    "    \n",
    "    n_policies = len(policy_rollouts)\n",
    "    merged_rollouts = functools.reduce(merge, policy_rollouts)\n",
    "    merged_rollouts = jax.tree_map(lambda x:jnp.stack(jnp.split(x,n_policies,axis=0)),merged_rollouts)\n",
    "    \n",
    "    def reshape_tree(tree, reference_tree,n_policies):\n",
    "        def reshape_fn(x, reference_x):\n",
    "            return jnp.reshape(x, (n_policies,*reference_x.shape))\n",
    "        \n",
    "        return jax.tree_map(reshape_fn, tree, reference_tree)\n",
    "    \n",
    "    merged_rollouts = reshape_tree(merged_rollouts,policy_rollouts[0],n_policies)\n",
    "    \n",
    "    return merged_rollouts\n",
    "\n",
    "# tmp_list = [policy_rollouts[0],policy_rollouts[9]]\n",
    "# tmp = [rollout.num_rollouts for rollout in tmp_list]\n",
    "# flattened_rollouts = flatten_rollouts(policy_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 00:42:42.918285: W external/xla/xla/service/gpu/nvptx_compiler.cc:679] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.103). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahdikallel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/tmp3dw77fvk/wandb/run-20231221_004243-qcgxx6an</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mahdikallel/jaxrl_m/runs/qcgxx6an' target=\"_blank\">swept-violet-283</a></strong> to <a href='https://wandb.ai/mahdikallel/jaxrl_m' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mahdikallel/jaxrl_m' target=\"_blank\">https://wandb.ai/mahdikallel/jaxrl_m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mahdikallel/jaxrl_m/runs/qcgxx6an' target=\"_blank\">https://wandb.ai/mahdikallel/jaxrl_m/runs/qcgxx6an</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra kwargs: {'max_steps': 1000000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 50889/1000000 [00:13<04:05, 3870.42it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(5,), (5, 5000)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/util.py:284\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/util.py:277\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 277\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:152\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 152\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:168\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(5,), (5, 5000)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 91\u001b[0m\n\u001b[1;32m     87\u001b[0m R2,bias \u001b[38;5;241m=\u001b[39m evaluate_critics(agent,critic_params,anc_return,flattened_rollouts)\n\u001b[1;32m     90\u001b[0m actor_batch \u001b[38;5;241m=\u001b[39m actor_buffer\u001b[38;5;241m.\u001b[39mget_all()      \n\u001b[0;32m---> 91\u001b[0m agent, actor_update_info \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactor_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mR2\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     92\u001b[0m num_grad_updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \n\u001b[1;32m     93\u001b[0m update_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcritic_update_info, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mactor_update_info,\n\u001b[1;32m     94\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2_validation\u001b[39m\u001b[38;5;124m'\u001b[39m: jnp\u001b[38;5;241m.\u001b[39mmax(R2),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m: jnp\u001b[38;5;241m.\u001b[39mmax(bias),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_grad_updates\u001b[39m\u001b[38;5;124m'\u001b[39m: num_grad_updates\n\u001b[1;32m     95\u001b[0m                }\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 113\u001b[0m, in \u001b[0;36mSACAgent.update_actor\u001b[0;34m(agent, batch, q_weights)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m actor_loss, {\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: actor_loss,\n\u001b[1;32m    109\u001b[0m       \n\u001b[1;32m    110\u001b[0m     }\n\u001b[1;32m    112\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m partial(actor_loss_fn,q_weights\u001b[38;5;241m=\u001b[39mq_weights)\n\u001b[0;32m--> 113\u001b[0m new_actor, actor_info \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mreplace(rng\u001b[38;5;241m=\u001b[39mnew_rng,actor\u001b[38;5;241m=\u001b[39mnew_actor,), {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mactor_info}\n",
      "File \u001b[0;32m~/Desktop/supersac/jaxrl_m/common.py:147\u001b[0m, in \u001b[0;36mTrainState.apply_loss_fn\u001b[0;34m(self, loss_fn, pmap_axis, has_aux)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03mTakes a gradient step towards minimizing `loss_fn`. Internally, this calls\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m`jax.grad` followed by `TrainState.apply_gradients`. If pmap_axis is provided,\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03madditionally it averages gradients (and info) across devices before performing update.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[0;32m--> 147\u001b[0m     grads, info \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pmap_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m         grads \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mpmean(grads, axis_name\u001b[38;5;241m=\u001b[39mpmap_axis)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 103\u001b[0m, in \u001b[0;36mSACAgent.update_actor.<locals>.actor_loss_fn\u001b[0;34m(actor_params, q_weights)\u001b[0m\n\u001b[1;32m    101\u001b[0m q \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(call_one_critic,in_axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;241m0\u001b[39m))(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m'\u001b[39m], actions,agent\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    102\u001b[0m q_weights \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(q_weights)\n\u001b[0;32m--> 103\u001b[0m q \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msum(\u001b[43mq_weights\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mq\u001b[49m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    105\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mq)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m actor_loss, {\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: actor_loss,\n\u001b[1;32m    109\u001b[0m   \n\u001b[1;32m    110\u001b[0m }\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:728\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 728\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:256\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    254\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 256\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/numpy/ufuncs.py:96\u001b[0m, in \u001b[0;36m_maybe_bool_binop.<locals>.fn\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(x1, x2, \u001b[38;5;241m/\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m   x1, x2 \u001b[38;5;241m=\u001b[39m \u001b[43mpromote_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m lax_fn(x1, x2) \u001b[38;5;28;01mif\u001b[39;00m x1\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_ \u001b[38;5;28;01melse\u001b[39;00m bool_lax_fn(x1, x2)\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:363\u001b[0m, in \u001b[0;36mpromote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    361\u001b[0m check_arraylike(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    362\u001b[0m _check_no_float0s(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpromote_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpromote_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:248\u001b[0m, in \u001b[0;36mpromote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mnumpy_rank_promotion\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    247\u001b[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[0;32m--> 248\u001b[0m result_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, (\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m (result_rank \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(shp)) \u001b[38;5;241m+\u001b[39m shp)\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arg, shp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, shapes)]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:168\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    166\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(5,), (5, 5000)]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import jax\n",
    "import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "from jaxrl_m.wandb import setup_wandb, default_wandb_config, get_flag_dict\n",
    "import wandb\n",
    "from jaxrl_m.evaluation import supply_rng, evaluate, flatten, EpisodeMonitor\n",
    "from jaxrl_m.dataset import ReplayBuffer\n",
    "from collections import deque\n",
    "from jaxrl_m.rollout import PolicyRollout,rollout_policy\n",
    "\n",
    "env_name='Ant-v4'\n",
    "seed=np.random.choice(1000000)\n",
    "eval_episodes=10\n",
    "batch_size = 256\n",
    "max_steps = int(1e6)\n",
    "start_steps = 50000                   \n",
    "log_interval = 5000\n",
    "#eval_interval = 10000\n",
    "\n",
    "wandb_config = default_wandb_config()\n",
    "wandb_config.update({\n",
    "    'project': 'd4rl_test',\n",
    "    'group': 'sac_test',\n",
    "    'name': 'sac_{env_name}',\n",
    "})\n",
    "\n",
    "\n",
    "env = EpisodeMonitor(gym.make(env_name))\n",
    "eval_env = EpisodeMonitor(gym.make(env_name))\n",
    "setup_wandb({\"bonjour\":1})\n",
    "\n",
    "example_transition = dict(\n",
    "    observations=env.observation_space.sample(),\n",
    "    actions=env.action_space.sample(),\n",
    "    rewards=0.0,\n",
    "    masks=1.0,\n",
    "    next_observations=env.observation_space.sample(),\n",
    "    discounts=1.0,\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer.create(example_transition, size=int(1e6))\n",
    "actor_buffer = ReplayBuffer.create(example_transition, size=int(5e3))\n",
    "\n",
    "agent = create_learner(seed,\n",
    "                example_transition['observations'][None],\n",
    "                example_transition['actions'][None],\n",
    "                max_steps=max_steps,\n",
    "                #**FLAGS.config\n",
    "                )\n",
    "\n",
    "exploration_metrics = dict()\n",
    "obs,info = env.reset()    \n",
    "exploration_rng = jax.random.PRNGKey(0)\n",
    "i = 0\n",
    "num_grad_updates = 0\n",
    "unlogged_steps = 0\n",
    "policy_rollouts = deque([], maxlen=10)\n",
    "with tqdm.tqdm(total=max_steps) as pbar:\n",
    "    \n",
    "    while i < max_steps:\n",
    "    \n",
    "        replay_buffer,actor_buffer,policy_rollout,policy_return,num_steps = rollout_policy(agent,env,exploration_rng,\n",
    "                replay_buffer,actor_buffer,\n",
    "                warmup=(i < start_steps))\n",
    "        policy_rollouts.append(policy_rollout)\n",
    "        unlogged_steps += num_steps\n",
    "        i+=num_steps\n",
    "        pbar.update(num_steps)\n",
    "        \n",
    "            \n",
    "        if replay_buffer.size > start_steps:\n",
    "        \n",
    "        \n",
    "            transitions = replay_buffer.get_all()\n",
    "            idxs = jax.random.choice(a=replay_buffer.size, shape=(5000,256), replace=True,key=jax.random.PRNGKey(0))\n",
    "            agent, critic_update_info = agent.update_many_critics(transitions,idxs,5000)\n",
    "            anc_return = policy_rollouts[-1].policy_return\n",
    "            tmp_list = [policy_rollouts[0],policy_rollouts[9]]\n",
    "            flattened_rollouts = flatten_rollouts(policy_rollouts)\n",
    "            #one_critic_params = jax.tree_map(lambda x : x[0],agent.critic.params)\n",
    "            critic_params = agent.critic.params\n",
    "            R2,bias = evaluate_critics(agent,critic_params,anc_return,flattened_rollouts)\n",
    "\n",
    "            \n",
    "            actor_batch = actor_buffer.get_all()      \n",
    "            agent, actor_update_info = agent.update_actor(actor_batch,R2.reshape(-1,1))   \n",
    "            num_grad_updates += 1 \n",
    "            update_info = {**critic_update_info, **actor_update_info,\n",
    "                           'R2_validation': jnp.max(R2),'bias': jnp.max(bias),'num_grad_updates': num_grad_updates\n",
    "                           }\n",
    "            \n",
    "            if unlogged_steps > log_interval:\n",
    "                exploration_metrics = {f'exploration/disc_return': policy_return}\n",
    "                wandb.log(exploration_metrics, step=int(i),commit=False)\n",
    "                train_metrics = {f'training/{k}': v for k, v in update_info.items()}\n",
    "                wandb.log(train_metrics, step=int(i),commit=False)\n",
    "                #wandb.log(exploration_metrics, step=i)\n",
    "                policy_fn = agent.actor\n",
    "                eval_info = evaluate(policy_fn, eval_env, num_episodes=eval_episodes)\n",
    "                eval_metrics = {f'evaluation/{k}': v for k, v in eval_info.items()}\n",
    "                print('evaluating')\n",
    "                wandb.log(eval_metrics, step=int(i),commit=True)\n",
    "                unlogged_steps = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20007662]\n",
      " [0.19994996]\n",
      " [0.20015115]\n",
      " [0.19991498]\n",
      " [0.19990726]]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "vector = jnp.array([1, 2, 3, 4, 5])\n",
    "softmax_vector = jax.nn.softmax(R2.reshape(-1,1),axis=0)\n",
    "\n",
    "print(softmax_vector)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
