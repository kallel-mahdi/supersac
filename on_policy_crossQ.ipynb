{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementations of algorithms for continuous control.\"\"\"\n",
    "import functools\n",
    "from jaxrl_m.typing import *\n",
    "\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from jaxrl_m.common import TrainState, target_update, nonpytree_field\n",
    "from jaxrl_m.networks import DeterministicPolicy,Policy, Critic, ensemblize\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "class SACAgent(flax.struct.PyTreeNode):\n",
    "    rng: PRNGKey\n",
    "    critic: (TrainState)\n",
    "    actor: TrainState\n",
    "    config: dict = nonpytree_field()\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "    @partial(jax.jit,static_argnames=('num_steps',))  \n",
    "    \n",
    "    \n",
    "    def update_many_critics(agent,transitions: Batch,idxs:jnp.array,num_steps:int,R2):\n",
    "        \n",
    "        \n",
    "        \n",
    "        def update_one_critic(critic_params,critic_opt_state,idxs,\n",
    "                            agent,transitions,num_steps):\n",
    "            \n",
    "            \n",
    "            def one_update(agent,\n",
    "                        critic_params,critic_opt_state,\n",
    "                        batch: Batch):\n",
    "                                  \n",
    "                def critic_loss_fn(critic_params):\n",
    "                    next_actions = agent.actor(batch['next_observations'])\n",
    "                    concat_actions = jnp.concatenate([batch[\"actions\"],next_actions])\n",
    "                    concat_observations = jnp.concatenate([batch[\"observations\"],batch[\"next_observations\"]])\n",
    "                    \n",
    "                    concat_q = agent.critic(concat_observations, concat_actions,\n",
    "                                            True,params=critic_params)\n",
    "                    q,next_q = jnp.split(concat_q,2,axis=0) ## axis=1 for ensemble\n",
    "                    \n",
    "                    target_q = batch['rewards'] + agent.config['discount'] * batch['masks'] * next_q\n",
    "                    target_q = jax.lax.stop_gradient(target_q)\n",
    "                    \n",
    "                    critic_loss = ((target_q-q)**2).mean()\n",
    "                    \n",
    "                    return critic_loss, {}\n",
    "        \n",
    "\n",
    "                grads,_ = jax.grad(critic_loss_fn, has_aux=True)(critic_params)\n",
    "                updates, new_opt_state = agent.critic.tx.update(grads, critic_opt_state, critic_params)\n",
    "                new_params = optax.apply_updates(critic_params, updates)\n",
    "\n",
    "                \n",
    "                return agent,new_params,new_opt_state\n",
    "            \n",
    "            \n",
    "            get_batch = lambda transitions,idx : jax.tree_map(lambda x : x[idx],transitions)\n",
    "                \n",
    "            agent,critic_params,critic_opt_state = jax.lax.fori_loop(0, num_steps, \n",
    "                        lambda i, args: one_update(*args,get_batch(transitions,idxs[i])),\n",
    "                        (agent,critic_params,critic_opt_state))\n",
    "            \n",
    "            return critic_params,critic_opt_state\n",
    "        \n",
    "        \n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "        \n",
    "        critic = agent.critic\n",
    "        ###### Reset optimizer params ######\n",
    "        opt_state = jax.vmap(critic.tx.init,in_axes=0)(agent.critic.params)\n",
    "        ###### Reset critic params ######\n",
    "        \n",
    "        reset = lambda rng,params : critic.init(rng,\n",
    "                                                agent.config[\"observations\"], agent.config[\"actions\"],False)[\"params\"]\n",
    "        no_reset = lambda rng,params: params\n",
    "        f = lambda  mask,rng,params :lax.cond(mask,reset,no_reset,rng,params)\n",
    "        mask = jnp.zeros((10))\n",
    "        mask.at[jnp.argmin(R2)].set(1)\n",
    "        rngs = jax.random.split(agent.rng, 10)\n",
    "        critic_params = jax.vmap(f,in_axes=(0,0,0))(mask,rngs,critic.params)\n",
    "        ###################################\n",
    "        tmp = partial(update_one_critic,agent=agent,transitions=transitions,num_steps=num_steps)\n",
    "        critic_params,critic_opt_state = jax.vmap(tmp,in_axes=(0,0,0))(critic_params,opt_state,idxs)\n",
    "        new_critic = critic.replace(params=critic_params,opt_state=critic_opt_state)\n",
    "        agent = agent.replace(rng=new_rng,critic=new_critic)\n",
    "        \n",
    "    \n",
    "        return agent,{}\n",
    "       \n",
    "    \n",
    "\n",
    "        \n",
    "    @jax.jit\n",
    "    def update_actor(agent, batch: Batch,q_weights,sample_weights):\n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "\n",
    "        def actor_loss_fn(actor_params,q_weights):\n",
    "            \n",
    "            actions = agent.actor(batch['observations'], params=actor_params)\n",
    "            \n",
    "            call_one_critic = lambda observations,actions,params : agent.critic(observations,actions,params=params)\n",
    "            q = jax.vmap(call_one_critic,in_axes=(None,None,0))(batch['observations'], actions,agent.critic.params)\n",
    "            q_weights = jax.nn.softmax(q_weights,axis=0)\n",
    "            q = jnp.sum(q_weights*q,axis=0)\n",
    "            q = q*sample_weights\n",
    "            \n",
    "            actor_loss = (-q).mean()\n",
    "            \n",
    "            return actor_loss, {\n",
    "                'actor_loss': actor_loss,\n",
    "              \n",
    "            }\n",
    "\n",
    "        loss_fn = partial(actor_loss_fn,q_weights=q_weights)\n",
    "        new_actor, actor_info = agent.actor.apply_loss_fn(loss_fn=loss_fn, has_aux=True)\n",
    "\n",
    "        return agent.replace(rng=new_rng,actor=new_actor,), {**actor_info}\n",
    "        \n",
    "\n",
    "\n",
    "    @jax.jit\n",
    "    def sample_actions(agent,observations: np.ndarray) -> jnp.ndarray:\n",
    "        actions = agent.actor(observations)\n",
    "       \n",
    "        return actions\n",
    "    \n",
    " \n",
    "\n",
    "def create_learner(\n",
    "                 seed: int,\n",
    "                 observations: jnp.ndarray,\n",
    "                 actions: jnp.ndarray,\n",
    "                 actor_lr: float = 3e-4,\n",
    "                 critic_lr: float = 3e-4,\n",
    "                 hidden_dims: Sequence[int] = (256, 256),\n",
    "                 discount: float = 0.99,\n",
    "                 tau: float = 0.005,\n",
    "            **kwargs):\n",
    "\n",
    "        print('Extra kwargs:', kwargs)\n",
    "\n",
    "        rng = jax.random.PRNGKey(seed)\n",
    "        rng, actor_key, critic_key = jax.random.split(rng, 3)\n",
    "\n",
    "        action_dim = actions.shape[-1]\n",
    "        actor_def = DeterministicPolicy((64,64), action_dim=action_dim,final_fc_init_scale=1.0)\n",
    "\n",
    "        actor_params = actor_def.init(actor_key, observations)['params']\n",
    "        actor = TrainState.create(actor_def, actor_params, tx=optax.adam(learning_rate=actor_lr))\n",
    "        \n",
    "        critic_def = Critic(hidden_dims)\n",
    "        critic_params = critic_def.init(critic_key, observations, actions,False)['params']\n",
    "        tmp_critic = TrainState.create(critic_def, critic_params,optax.adam(learning_rate=critic_lr))\n",
    "        \n",
    "        critic_keys  = jax.random.split(critic_key, 10)\n",
    "        critic_params = jax.vmap(critic_def.init,in_axes=(0,None,None))(critic_keys, observations, actions)['params']\n",
    "        critic = tmp_critic.replace(params=critic_params) ## opt_state is updated in \"update_many_critics\"\n",
    "\n",
    "        \n",
    "        config = flax.core.FrozenDict(dict(\n",
    "            discount=discount,\n",
    "            target_update_rate=tau, \n",
    "            observations = observations,\n",
    "            actions = actions,\n",
    "        ))\n",
    "\n",
    "        return SACAgent(rng, critic=critic, actor=actor, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def compute_q(anc_agent,obs,actor_params,critic_params):\n",
    "\n",
    "    actions = anc_agent.actor(obs, params=actor_params)\n",
    "    q = anc_agent.critic(obs, actions,False,params=critic_params)\n",
    "   \n",
    "    return q\n",
    "    \n",
    "    \n",
    "\n",
    "def estimate_return(acq_rollout,\n",
    "                    anc_agent,anc_critic_params,anc_return):\n",
    "    \n",
    "    acq_obs = acq_rollout.observations\n",
    "    acq_masks = acq_rollout.disc_masks\n",
    "    acq_return = acq_rollout.policy_return\n",
    "  \n",
    "    \n",
    "    anc_actor_params = anc_agent.actor.params\n",
    "    acq_actor_params = acq_rollout.policy_params\n",
    "    \n",
    "    \n",
    "    anc_q = compute_q(anc_agent,acq_obs,anc_actor_params,anc_critic_params)\n",
    "    acq_q = compute_q(anc_agent,acq_obs,acq_actor_params,anc_critic_params)\n",
    "    \n",
    "    adv = ((acq_q - anc_q)*acq_masks).sum()/5\n",
    "    acq_return_pred = anc_return + adv\n",
    "  \n",
    "    \n",
    "    return acq_return_pred,acq_return\n",
    "\n",
    "\n",
    "def evaluate_critic(anc_agent,anc_critic_params,\n",
    "                    anc_return,policy_rollouts):\n",
    "\n",
    "    \n",
    "    tmp =  partial(estimate_return,\n",
    "                   anc_agent=anc_agent,\n",
    "                   anc_critic_params =anc_critic_params,\n",
    "                   anc_return = anc_return)\n",
    "    y_pred,y = jax.vmap(tmp)(policy_rollouts)\n",
    "    a2 = jnp.clip(((y-y_pred)**2),a_min=1e-4).sum()\n",
    "    b2=((y-y.mean())**2).sum()\n",
    "    R2 = 1-(a2/b2)  \n",
    "    bias = (y_pred-y).mean()\n",
    "    \n",
    "    return R2,bias\n",
    "\n",
    "@jax.jit\n",
    "def evaluate_critics(anc_agent,anc_critic_params,\n",
    "                    anc_return,policy_rollouts):\n",
    "    \n",
    "    return jax.vmap(evaluate_critic,in_axes=(None,0,None,None))(anc_agent,anc_critic_params,anc_return,policy_rollouts)\n",
    "\n",
    "\n",
    "\n",
    "def merge(x,y):\n",
    "\n",
    "    return jax.tree_map(lambda x,y : jnp.vstack([x,y]),x,y)\n",
    "\n",
    "def flatten_rollouts(policy_rollouts):\n",
    "    \n",
    "    n_policies = len(policy_rollouts)\n",
    "    merged_rollouts = functools.reduce(merge, policy_rollouts)\n",
    "    merged_rollouts = jax.tree_map(lambda x:jnp.stack(jnp.split(x,n_policies,axis=0)),merged_rollouts)\n",
    "    \n",
    "    def reshape_tree(tree, reference_tree,n_policies):\n",
    "        def reshape_fn(x, reference_x):\n",
    "            return jnp.reshape(x, (n_policies,*reference_x.shape))\n",
    "        \n",
    "        return jax.tree_map(reshape_fn, tree, reference_tree)\n",
    "    \n",
    "    merged_rollouts = reshape_tree(merged_rollouts,policy_rollouts[0],n_policies)\n",
    "    \n",
    "    return merged_rollouts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-24 02:24:53.617743: W external/xla/xla/service/gpu/nvptx_compiler.cc:679] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.103). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahdikallel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/tmpdri9u867/wandb/run-20231224_022454-89r586a9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mahdikallel/jaxrl_m/runs/89r586a9' target=\"_blank\">lemon-monkey-3</a></strong> to <a href='https://wandb.ai/mahdikallel/jaxrl_m' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mahdikallel/jaxrl_m' target=\"_blank\">https://wandb.ai/mahdikallel/jaxrl_m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mahdikallel/jaxrl_m/runs/89r586a9' target=\"_blank\">https://wandb.ai/mahdikallel/jaxrl_m/runs/89r586a9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra kwargs: {'max_steps': 1000000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 159986/1000000 [07:16<38:12, 366.37it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m             tmp \u001b[38;5;241m=\u001b[39m partial(jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice,a\u001b[38;5;241m=\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39msize, shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10000\u001b[39m,\u001b[38;5;241m256\u001b[39m), replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     88\u001b[0m             idxs \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(tmp)(jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m---> 89\u001b[0m             agent, critic_update_info \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_many_critics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43midxs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mR2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#        with CodeTimer('evaluate_critic'):        \u001b[39;00m\n\u001b[1;32m     92\u001b[0m             critic_params \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mparams\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/flax/core/frozen_dict.py:168\u001b[0m, in \u001b[0;36mFrozenDict.tree_unflatten\u001b[0;34m(cls, keys, values)\u001b[0m\n\u001b[1;32m    163\u001b[0m   sorted_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict)\n\u001b[1;32m    164\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    165\u001b[0m       [(jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mDictKey(k), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict[k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m sorted_keys]\n\u001b[1;32m    166\u001b[0m   ), \u001b[38;5;28mtuple\u001b[39m(sorted_keys)\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_unflatten\u001b[39m(\u001b[38;5;28mcls\u001b[39m, keys, values):\n\u001b[1;32m    170\u001b[0m   \u001b[38;5;66;03m# data is already deep copied due to tree map mechanism\u001b[39;00m\n\u001b[1;32m    171\u001b[0m   \u001b[38;5;66;03m# we can skip the deep copy in the constructor\u001b[39;00m\n\u001b[1;32m    172\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m({k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keys, values)}, __unsafe_skip_copy__\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import jax\n",
    "import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "from jaxrl_m.common import CodeTimer\n",
    "from jaxrl_m.wandb import setup_wandb, default_wandb_config, get_flag_dict\n",
    "import wandb\n",
    "from jaxrl_m.evaluation import supply_rng, evaluate, flatten, EpisodeMonitor\n",
    "from jaxrl_m.dataset import ReplayBuffer\n",
    "from collections import deque\n",
    "from jaxrl_m.rollout import PolicyRollout,rollout_policy\n",
    "\n",
    "env_name='Walker2d-v4'\n",
    "seed=np.random.choice(1000000)\n",
    "eval_episodes=10\n",
    "batch_size = 256\n",
    "max_steps = int(1e6)\n",
    "start_steps = 50000                   \n",
    "log_interval = 5000\n",
    "#eval_interval = 10000\n",
    "\n",
    "wandb_config = default_wandb_config()\n",
    "wandb_config.update({\n",
    "    'project': 'd4rl_test',\n",
    "    'group': 'sac_test',\n",
    "    'name': 'sac_{env_name}',\n",
    "})\n",
    "\n",
    "\n",
    "env = EpisodeMonitor(gym.make(env_name))\n",
    "eval_env = EpisodeMonitor(gym.make(env_name))\n",
    "setup_wandb({\"bonjour\":1})\n",
    "\n",
    "example_transition = dict(\n",
    "    observations=env.observation_space.sample(),\n",
    "    actions=env.action_space.sample(),\n",
    "    rewards=0.0,\n",
    "    masks=1.0,\n",
    "    next_observations=env.observation_space.sample(),\n",
    "    discounts=1.0,\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer.create(example_transition, size=int(1e6))\n",
    "actor_buffer = ReplayBuffer.create(example_transition, size=int(5e3))\n",
    "\n",
    "agent = create_learner(seed,\n",
    "                example_transition['observations'][None],\n",
    "                example_transition['actions'][None],\n",
    "                max_steps=max_steps,\n",
    "                #**FLAGS.config\n",
    "                )\n",
    "\n",
    "exploration_metrics = dict()\n",
    "obs,info = env.reset()    \n",
    "exploration_rng = jax.random.PRNGKey(0)\n",
    "i = 0\n",
    "num_grad_updates = 0\n",
    "unlogged_steps = 0\n",
    "policy_rollouts = deque([], maxlen=20)\n",
    "R2 = jnp.ones(10)\n",
    "with tqdm.tqdm(total=max_steps) as pbar:\n",
    "    \n",
    "    while i < max_steps:\n",
    "\n",
    "\n",
    "        #with CodeTimer('rollout'):\n",
    "            \n",
    "            replay_buffer,actor_buffer,policy_rollout,policy_return,undisc_policy_return,num_steps = rollout_policy(agent,env,exploration_rng,\n",
    "                    replay_buffer,actor_buffer,\n",
    "                    warmup=(i < start_steps))\n",
    "            policy_rollouts.append(policy_rollout)\n",
    "            unlogged_steps += num_steps\n",
    "            i+=num_steps\n",
    "            pbar.update(num_steps)\n",
    "            \n",
    "            \n",
    "        \n",
    "            if replay_buffer.size > start_steps:\n",
    "        \n",
    "        \n",
    "            #with CodeTimer('update_critic'):\n",
    "                key = jax.random.PRNGKey(0)\n",
    "                transitions = replay_buffer.get_all()\n",
    "                tmp = partial(jax.random.choice,a=replay_buffer.size, shape=(10000,256), replace=True)\n",
    "                idxs = jax.vmap(tmp)(jax.random.split(key, 10))\n",
    "                agent, critic_update_info = agent.update_many_critics(transitions,idxs,10000,R2)\n",
    "            \n",
    "    #        with CodeTimer('evaluate_critic'):        \n",
    "                critic_params = agent.critic.params\n",
    "                anc_return = policy_rollouts[-1].policy_return\n",
    "                flattened_rollouts = flatten_rollouts(policy_rollouts)\n",
    "                R2,bias = evaluate_critics(agent,critic_params,anc_return,flattened_rollouts)\n",
    "\n",
    "            #with CodeTimer('update_actor'):\n",
    "                actor_batch = actor_buffer.get_all() \n",
    "                sample_weights = policy_rollouts[-1].disc_masks\n",
    "                agent, actor_update_info = agent.update_actor(actor_batch,R2.reshape(-1,1),sample_weights)   \n",
    "                num_grad_updates += 1 \n",
    "                \n",
    "            \n",
    "                \n",
    "                if unlogged_steps > log_interval:\n",
    "                    \n",
    "                    #with CodeTimer('eval'):\n",
    "                            \n",
    "                        \n",
    "                        update_info = {**critic_update_info, **actor_update_info,\n",
    "                                'R2_validation': jnp.max(R2),'bias': jnp.max(bias),'num_grad_updates': num_grad_updates\n",
    "                                }\n",
    "                        exploration_metrics = {f'exploration/disc_return': policy_return}\n",
    "                        \n",
    "                        train_metrics = {f'training/{k}': v for k, v in update_info.items()}\n",
    "                        \n",
    "                        # eval_info = evaluate(agent.actor, eval_env, num_episodes=eval_episodes)\n",
    "                        # eval_metrics = {f'evaluation/{k}': v for k, v in eval_info.items()}\n",
    "                        \n",
    "                    #with CodeTimer('logging'):\n",
    "                        \n",
    "                        wandb.log(train_metrics, step=int(i),commit=False)\n",
    "                        wandb.log({\"undisc_return\":undisc_policy_return},step=int(i))\n",
    "                        wandb.log(exploration_metrics, step=int(i),commit=True)\n",
    "                        #wandb.log(eval_metrics, step=int(i),commit=True)\n",
    "                        unlogged_steps = 0\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
