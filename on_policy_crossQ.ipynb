{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementations of algorithms for continuous control.\"\"\"\n",
    "import functools\n",
    "from jaxrl_m.typing import *\n",
    "\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from jaxrl_m.common import TrainState, target_update, nonpytree_field\n",
    "from jaxrl_m.networks import DeterministicPolicy,Policy, Critic, ensemblize\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "class SACAgent(flax.struct.PyTreeNode):\n",
    "    rng: PRNGKey\n",
    "    critic: (TrainState)\n",
    "    actor: TrainState\n",
    "    config: dict = nonpytree_field()\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "    @partial(jax.jit,static_argnames=('num_steps',))  \n",
    "    \n",
    "    \n",
    "    def update_many_critics(agent,transitions: Batch,idxs:jnp.array,num_steps:int,R2):\n",
    "        \n",
    "        \n",
    "        \n",
    "        def update_one_critic(critic_params,critic_opt_state,batch_stats,idxs,\n",
    "                            agent,transitions,num_steps):\n",
    "            \n",
    "            \n",
    "            def one_update(agent,\n",
    "                        critic_params,critic_opt_state,batch_stats,\n",
    "                        batch: Batch):\n",
    "                                  \n",
    "                def critic_loss_fn(critic_params):\n",
    "                    next_actions = agent.actor(batch['next_observations'])\n",
    "                    concat_actions = jnp.concatenate([batch[\"actions\"],next_actions])\n",
    "                    concat_observations = jnp.concatenate([batch[\"observations\"],batch[\"next_observations\"]])\n",
    "                    \n",
    "                    concat_q,updates = agent.critic(concat_observations, concat_actions,True,\n",
    "                                                    params=critic_params,extra_variables=batch_stats,mutable=[\"batch_stats\"])\n",
    "                    q,next_q = jnp.split(concat_q,2,axis=0) ## axis=1 for ensemble\n",
    "                    target_q = batch['rewards'] + agent.config['discount'] * batch['masks'] * next_q\n",
    "                    target_q = jax.lax.stop_gradient(target_q)\n",
    "                    \n",
    "                    critic_loss = ((target_q-q)**2).mean()\n",
    "                    \n",
    "                    return critic_loss, {\"batch_stats\":updates[\"batch_stats\"]}\n",
    "        \n",
    "\n",
    "                grads,batch_stats = jax.grad(critic_loss_fn, has_aux=True)(critic_params)\n",
    "                updates, new_opt_state = agent.critic.tx.update(grads, critic_opt_state, critic_params)\n",
    "                new_params = optax.apply_updates(critic_params, updates)\n",
    "\n",
    "                \n",
    "                return agent,new_params,new_opt_state,batch_stats\n",
    "            \n",
    "            \n",
    "            get_batch = lambda transitions,idx : jax.tree_map(lambda x : x[idx],transitions)\n",
    "            \n",
    "            \n",
    "            agent,critic_params,critic_opt_state,batch_stats = jax.lax.fori_loop(0, num_steps, \n",
    "                        lambda i, args: one_update(*args,get_batch(transitions,idxs[i])),\n",
    "                        (agent,critic_params,critic_opt_state,batch_stats))\n",
    "            \n",
    "            \n",
    "            \n",
    "            # args = (agent,critic_params,critic_opt_state,batch_stats)\n",
    "            # agent,critic_params,critic_opt_state,batch_stats =   one_update(*args,get_batch(transitions,idxs[0]))\n",
    "            \n",
    "                        \n",
    "            \n",
    "            return critic_params,critic_opt_state,batch_stats\n",
    "        \n",
    "        \n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "        \n",
    "        critic = agent.critic\n",
    "        ###### Reset optimizer params ######\n",
    "        opt_state = jax.vmap(critic.tx.init,in_axes=0)(agent.critic.params)\n",
    "        ###### Reset critic params ######\n",
    "        \n",
    "        reset = lambda rng,params : critic.init(rng,\n",
    "                                                agent.config[\"observations\"], agent.config[\"actions\"],False)[\"params\"]\n",
    "        no_reset = lambda rng,params: params\n",
    "        f = lambda  mask,rng,params :lax.cond(mask,reset,no_reset,rng,params)\n",
    "        mask = jnp.zeros((10))\n",
    "        mask.at[jnp.argmin(R2)].set(1)\n",
    "        rngs = jax.random.split(agent.rng, 10)\n",
    "        critic_params = jax.vmap(f,in_axes=(0,0,0))(mask,rngs,critic.params)\n",
    "        ###################################\n",
    "        tmp = partial(update_one_critic,agent=agent,transitions=transitions,num_steps=num_steps)\n",
    "        batch_stats = critic.extra_variables\n",
    "        critic_params,critic_opt_state,batch_stats = jax.vmap(tmp,in_axes=(0,0,0,0))(critic_params,opt_state,batch_stats,idxs)\n",
    "        new_critic = critic.replace(params=critic_params,opt_state=critic_opt_state,extra_variables=batch_stats)\n",
    "        agent = agent.replace(rng=new_rng,critic=new_critic)\n",
    "        \n",
    "    \n",
    "        return agent,{}\n",
    "       \n",
    "    \n",
    "\n",
    "        \n",
    "    @jax.jit\n",
    "    def update_actor(agent, batch: Batch,q_weights,sample_weights):\n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "\n",
    "        def actor_loss_fn(actor_params,actor_stats,q_weights):\n",
    "            \n",
    "            actions,updates = agent.actor(batch['observations'],True, params=actor_params,extra_variables=actor_stats,mutable=[\"batch_stats\"])\n",
    "            \n",
    "            call_one_critic = lambda observations,actions,params,batch_stats: agent.critic(observations,actions,\n",
    "                                                                                           params=params,extra_variables=batch_stats)\n",
    "            q = jax.vmap(call_one_critic,in_axes=(None,None,0,0))(batch['observations'], actions,agent.critic.params,agent.critic.extra_variables)\n",
    "            q_weights = jax.nn.softmax(q_weights,axis=0)\n",
    "            q = jnp.sum(q_weights*q,axis=0)\n",
    "            q = q*sample_weights\n",
    "            \n",
    "            actor_loss = (-q).mean()\n",
    "            \n",
    "            return actor_loss, updates[\"batch_stats\"]\n",
    "\n",
    "        loss_fn = partial(actor_loss_fn,actor_stats=agent.actor.extra_variables,q_weights=q_weights)\n",
    "        new_actor, batch_stats = agent.actor.apply_loss_fn(loss_fn=loss_fn, has_aux=True)\n",
    "        new_actor = new_actor.replace(extra_variables={\"batch_stats\":batch_stats})\n",
    "        actor_info = {}\n",
    "        return agent.replace(rng=new_rng,actor=new_actor,), {**actor_info}\n",
    "        \n",
    "\n",
    "\n",
    "    @jax.jit\n",
    "    def sample_actions(agent,observations: np.ndarray) -> jnp.ndarray:\n",
    "        actions = agent.actor(observations)\n",
    "       \n",
    "        return actions\n",
    "    \n",
    " \n",
    "\n",
    "def create_learner(\n",
    "                 seed: int,\n",
    "                 observations: jnp.ndarray,\n",
    "                 actions: jnp.ndarray,\n",
    "                 actor_lr: float = 3e-4,\n",
    "                 critic_lr: float = 3e-4,\n",
    "                 hidden_dims: Sequence[int] = (256, 256),\n",
    "                 discount: float = 0.99,\n",
    "                 tau: float = 0.005,\n",
    "            **kwargs):\n",
    "\n",
    "        print('Extra kwargs:', kwargs)\n",
    "\n",
    "        rng = jax.random.PRNGKey(seed)\n",
    "        rng, actor_key, critic_key = jax.random.split(rng, 3)\n",
    "\n",
    "        action_dim = actions.shape[-1]\n",
    "        actor_def = DeterministicPolicy((64,64), action_dim=action_dim,final_fc_init_scale=1.0)\n",
    "\n",
    "        actor_tmp = actor_def.init(actor_key, observations,training=True)\n",
    "        actor_params = actor_tmp['params']\n",
    "        actor_stats = actor_tmp['batch_stats']\n",
    "        actor = TrainState.create(actor_def, actor_params, tx=optax.adam(learning_rate=actor_lr),\n",
    "                                  extra_variables={\"batch_stats\":actor_stats})\n",
    "        \n",
    "        critic_def = Critic(hidden_dims)\n",
    "        critic_tmp = critic_def.init(critic_key, observations, actions,False)\n",
    "        critic_params = critic_tmp['params']\n",
    "        critic_stats = critic_tmp['batch_stats']\n",
    "        tmp_critic = TrainState.create(critic_def, critic_params,optax.adam(learning_rate=critic_lr),\n",
    "                                       extra_variables={\"batch_stats\":critic_stats})\n",
    "        \n",
    "        critic_keys  = jax.random.split(critic_key, 10)\n",
    "        critic_tmp = jax.vmap(critic_def.init,in_axes=(0,None,None))(critic_keys, observations, actions)\n",
    "        critic_params = critic_tmp['params']\n",
    "        critic_stats = critic_tmp['batch_stats']\n",
    "        critic = tmp_critic.replace(params=critic_params,\n",
    "                                    extra_variables={\"batch_stats\":critic_stats}) ## opt_state is updated in \"update_many_critics\"\n",
    "        \n",
    "        \n",
    "        print(\"Initialisation\",jax.tree_map(lambda x : x.shape,critic_stats))\n",
    "\n",
    "        \n",
    "        config = flax.core.FrozenDict(dict(\n",
    "            discount=discount,\n",
    "            target_update_rate=tau, \n",
    "            observations = observations,\n",
    "            actions = actions,\n",
    "        ))\n",
    "\n",
    "        return SACAgent(rng, critic=critic, actor=actor, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def compute_q(anc_agent,obs,actor_params,actor_stats,\n",
    "              critic_params,critic_stats):\n",
    "\n",
    "    actions = anc_agent.actor(obs, params=actor_params,extra_variables=actor_stats)\n",
    "    q = anc_agent.critic(obs, actions,False,params=critic_params,extra_variables=critic_stats)\n",
    "   \n",
    "    return q\n",
    "    \n",
    "    \n",
    "\n",
    "def estimate_return(acq_rollout,\n",
    "                    anc_agent,anc_critic_params,anc_critic_stats,anc_return):\n",
    "    \n",
    "    acq_obs = acq_rollout.observations\n",
    "    acq_masks = acq_rollout.disc_masks\n",
    "    acq_return = acq_rollout.policy_return\n",
    "    \n",
    "    anc_actor_params = anc_agent.actor.params\n",
    "    anc_actor_stats = anc_agent.actor.extra_variables\n",
    "    acq_actor_params = acq_rollout.policy_params\n",
    "    acq_actor_stats = acq_rollout.policy_stats\n",
    "      \n",
    "    anc_q = compute_q(anc_agent,acq_obs,\n",
    "                      anc_actor_params,anc_actor_stats,\n",
    "                      anc_critic_params,anc_critic_stats)\n",
    "    acq_q = compute_q(anc_agent,acq_obs,\n",
    "                      acq_actor_params,acq_actor_stats,\n",
    "                      anc_critic_params,anc_critic_stats)\n",
    "    \n",
    "    adv = ((acq_q - anc_q)*acq_masks).sum()/5\n",
    "    acq_return_pred = anc_return + adv\n",
    "  \n",
    "    \n",
    "    return acq_return_pred,acq_return\n",
    "\n",
    "\n",
    "def evaluate_critic(anc_agent,anc_critic_params,anc_critic_stats,\n",
    "                    anc_return,policy_rollouts):\n",
    "\n",
    "    \n",
    "    tmp =  partial(estimate_return,\n",
    "                   anc_agent=anc_agent,\n",
    "                   anc_critic_params =anc_critic_params,\n",
    "                   anc_critic_stats =anc_critic_stats,\n",
    "                   anc_return = anc_return)\n",
    "    y_pred,y = jax.vmap(tmp)(policy_rollouts)\n",
    "    a2 = jnp.clip(((y-y_pred)**2),a_min=1e-4).sum()\n",
    "    b2=((y-y.mean())**2).sum()\n",
    "    R2 = 1-(a2/b2)  \n",
    "    bias = (y_pred-y).mean()\n",
    "    \n",
    "    return R2,bias\n",
    "\n",
    "@jax.jit\n",
    "def evaluate_critics(anc_agent,anc_critic_params,anc_critic_stats,\n",
    "                    anc_return,policy_rollouts):\n",
    "    \n",
    "    return jax.vmap(evaluate_critic,in_axes=(None,0,0,None,None))(anc_agent,anc_critic_params,anc_critic_stats,\n",
    "                                                                  anc_return,policy_rollouts)\n",
    "\n",
    "\n",
    "\n",
    "def merge(x,y):\n",
    "\n",
    "    return jax.tree_map(lambda x,y : jnp.vstack([x,y]),x,y)\n",
    "\n",
    "def flatten_rollouts(policy_rollouts):\n",
    "    \n",
    "    n_policies = len(policy_rollouts)\n",
    "    merged_rollouts = functools.reduce(merge, policy_rollouts)\n",
    "    merged_rollouts = jax.tree_map(lambda x:jnp.stack(jnp.split(x,n_policies,axis=0)),merged_rollouts)\n",
    "    \n",
    "    def reshape_tree(tree, reference_tree,n_policies):\n",
    "        def reshape_fn(x, reference_x):\n",
    "            return jnp.reshape(x, (n_policies,*reference_x.shape))\n",
    "        \n",
    "        return jax.tree_map(reshape_fn, tree, reference_tree)\n",
    "    \n",
    "    merged_rollouts = reshape_tree(merged_rollouts,policy_rollouts[0],n_policies)\n",
    "    \n",
    "    return merged_rollouts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-25 14:29:11.201557: W external/xla/xla/service/gpu/nvptx_compiler.cc:679] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.103). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahdikallel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/tmpy4nicpzj/wandb/run-20231225_142912-9809mgd8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mahdikallel/jaxrl_m/runs/9809mgd8' target=\"_blank\">swift-meadow-37</a></strong> to <a href='https://wandb.ai/mahdikallel/jaxrl_m' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mahdikallel/jaxrl_m' target=\"_blank\">https://wandb.ai/mahdikallel/jaxrl_m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mahdikallel/jaxrl_m/runs/9809mgd8' target=\"_blank\">https://wandb.ai/mahdikallel/jaxrl_m/runs/9809mgd8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra kwargs: {'max_steps': 1000000}\n",
      "Initialisation {'MLP_0': {'BatchNorm_0': {'mean': (10, 256), 'var': (10, 256)}, 'BatchNorm_1': {'mean': (10, 256), 'var': (10, 256)}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 294999/1000000 [10:09<24:16, 483.91it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m             tmp \u001b[38;5;241m=\u001b[39m partial(jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice,a\u001b[38;5;241m=\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39msize, shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10000\u001b[39m,\u001b[38;5;241m256\u001b[39m), replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     88\u001b[0m             idxs \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(tmp)(jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m---> 89\u001b[0m             agent, critic_update_info \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_many_critics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43midxs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mR2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#        with CodeTimer('evaluate_critic'):        \u001b[39;00m\n\u001b[1;32m     92\u001b[0m             critic_params \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mparams\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import jax\n",
    "import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "from jaxrl_m.common import CodeTimer\n",
    "from jaxrl_m.wandb import setup_wandb, default_wandb_config, get_flag_dict\n",
    "import wandb\n",
    "from jaxrl_m.evaluation import supply_rng, evaluate, flatten, EpisodeMonitor\n",
    "from jaxrl_m.dataset import ReplayBuffer\n",
    "from collections import deque\n",
    "from jaxrl_m.rollout import PolicyRollout,rollout_policy\n",
    "\n",
    "env_name='InvertedPendulum-v4'\n",
    "seed=np.random.choice(1000000)\n",
    "eval_episodes=10\n",
    "batch_size = 256\n",
    "max_steps = int(1e6)\n",
    "start_steps = 5000                   \n",
    "log_interval = 5000\n",
    "#eval_interval = 10000\n",
    "\n",
    "wandb_config = default_wandb_config()\n",
    "wandb_config.update({\n",
    "    'project': 'd4rl_test',\n",
    "    'group': 'sac_test',\n",
    "    'name': 'sac_{env_name}',\n",
    "})\n",
    "\n",
    "\n",
    "env = EpisodeMonitor(gym.make(env_name))\n",
    "eval_env = EpisodeMonitor(gym.make(env_name))\n",
    "setup_wandb({\"bonjour\":1})\n",
    "\n",
    "example_transition = dict(\n",
    "    observations=env.observation_space.sample(),\n",
    "    actions=env.action_space.sample(),\n",
    "    rewards=0.0,\n",
    "    masks=1.0,\n",
    "    next_observations=env.observation_space.sample(),\n",
    "    discounts=1.0,\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer.create(example_transition, size=int(1e6))\n",
    "actor_buffer = ReplayBuffer.create(example_transition, size=int(5e3))\n",
    "\n",
    "agent = create_learner(seed,\n",
    "                example_transition['observations'][None],\n",
    "                example_transition['actions'][None],\n",
    "                max_steps=max_steps,\n",
    "                #**FLAGS.config\n",
    "                )\n",
    "\n",
    "exploration_metrics = dict()\n",
    "obs,info = env.reset()    \n",
    "exploration_rng = jax.random.PRNGKey(0)\n",
    "i = 0\n",
    "num_grad_updates = 0\n",
    "unlogged_steps = 0\n",
    "policy_rollouts = deque([], maxlen=20)\n",
    "R2 = jnp.ones(10)\n",
    "with tqdm.tqdm(total=max_steps) as pbar:\n",
    "    \n",
    "    while i < max_steps:\n",
    "\n",
    "\n",
    "        #with CodeTimer('rollout'):\n",
    "            \n",
    "            replay_buffer,actor_buffer,policy_rollout,policy_return,undisc_policy_return,num_steps = rollout_policy(agent,env,exploration_rng,\n",
    "                    replay_buffer,actor_buffer,\n",
    "                    warmup=(i < start_steps))\n",
    "            policy_rollouts.append(policy_rollout)\n",
    "            unlogged_steps += num_steps\n",
    "            i+=num_steps\n",
    "            pbar.update(num_steps)\n",
    "            \n",
    "            \n",
    "        \n",
    "            if replay_buffer.size > start_steps:\n",
    "        \n",
    "        \n",
    "            #with CodeTimer('update_critic'):\n",
    "                key = jax.random.PRNGKey(0)\n",
    "                transitions = replay_buffer.get_all()\n",
    "                tmp = partial(jax.random.choice,a=replay_buffer.size, shape=(10000,256), replace=True)\n",
    "                idxs = jax.vmap(tmp)(jax.random.split(key, 10))\n",
    "                agent, critic_update_info = agent.update_many_critics(transitions,idxs,10000,R2)\n",
    "            \n",
    "    #        with CodeTimer('evaluate_critic'):        \n",
    "                critic_params = agent.critic.params\n",
    "                critic_stats = agent.critic.extra_variables\n",
    "                anc_return = policy_rollouts[-1].policy_return\n",
    "                flattened_rollouts = flatten_rollouts(policy_rollouts)\n",
    "                R2,bias = evaluate_critics(agent,critic_params,critic_stats,\n",
    "                                           anc_return,flattened_rollouts)\n",
    "\n",
    "            #with CodeTimer('update_actor'):\n",
    "                actor_batch = actor_buffer.get_all() \n",
    "                sample_weights = policy_rollouts[-1].disc_masks\n",
    "                agent, actor_update_info = agent.update_actor(actor_batch,R2.reshape(-1,1),sample_weights)   \n",
    "                num_grad_updates += 1 \n",
    "                \n",
    "            \n",
    "                \n",
    "                if unlogged_steps > log_interval:\n",
    "                    \n",
    "                    #with CodeTimer('eval'):\n",
    "                            \n",
    "                        \n",
    "                        update_info = {**critic_update_info, **actor_update_info,\n",
    "                                'R2_validation': jnp.max(R2),'bias': jnp.max(bias),'num_grad_updates': num_grad_updates\n",
    "                                }\n",
    "                        exploration_metrics = {f'exploration/disc_return': policy_return}\n",
    "                        \n",
    "                        train_metrics = {f'training/{k}': v for k, v in update_info.items()}\n",
    "                        \n",
    "                        # eval_info = evaluate(agent.actor, eval_env, num_episodes=eval_episodes)\n",
    "                        # eval_metrics = {f'evaluation/{k}': v for k, v in eval_info.items()}\n",
    "                        \n",
    "                    #with CodeTimer('logging'):\n",
    "                        \n",
    "                        wandb.log(train_metrics, step=int(i),commit=False)\n",
    "                        wandb.log({\"undisc_return\":undisc_policy_return},step=int(i))\n",
    "                        wandb.log(exploration_metrics, step=int(i),commit=True)\n",
    "                        #wandb.log(eval_metrics, step=int(i),commit=True)\n",
    "                        unlogged_steps = 0\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
