{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementations of algorithms for continuous control.\"\"\"\n",
    "import functools\n",
    "from jaxrl_m.typing import *\n",
    "\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from jaxrl_m.common import TrainState, target_update, nonpytree_field\n",
    "from jaxrl_m.networks import DeterministicPolicy,Policy, Critic, ensemblize\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "class SACAgent(flax.struct.PyTreeNode):\n",
    "    rng: PRNGKey\n",
    "    critic: (TrainState)\n",
    "    target_critic: TrainState\n",
    "    actor: TrainState\n",
    "    config: dict = nonpytree_field()\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    # @jax.jit    \n",
    "    # def reset_critic_optimizer(agent):\n",
    "    \n",
    "    #     new_opt_state = agent.critic.tx.init(agent.critic.params)\n",
    "    #     new_critic = agent.critic.replace(opt_state=new_opt_state)\n",
    "        \n",
    "    #     return agent.replace(critic=new_critic)\n",
    "        \n",
    "    @partial(jax.jit,static_argnames=('num_steps',))  \n",
    "    \n",
    "    \n",
    "    def update_many_critics(agent,transitions: Batch,idxs:jnp.array,num_steps:int):\n",
    "    \n",
    "        def update_one_critic(critic_params,critic_opt_params,\n",
    "                            agent,transitions,idxs,num_steps):\n",
    "            \n",
    "            \n",
    "            \n",
    "            def one_update(agent,\n",
    "                        critic_params,critic_opt_params,\n",
    "                        batch: Batch):\n",
    "                    \n",
    "                new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "\n",
    "            \n",
    "                def critic_loss_fn(critic_params):\n",
    "                    next_actions = agent.actor(batch['next_observations'])\n",
    "                    concat_actions = jnp.concatenate([batch[\"actions\"],next_actions])\n",
    "                    concat_observations = jnp.concatenate([batch[\"observations\"],batch[\"next_observations\"]])\n",
    "                    \n",
    "                    concat_q = agent.critic(concat_observations, concat_actions,\n",
    "                                            True,params=critic_params)\n",
    "                    \n",
    "                \n",
    "                    q,next_q = jnp.split(concat_q,2,axis=0) ## axis=1 for ensemble\n",
    "                    \n",
    "                \n",
    "                    target_q = batch['rewards'] + agent.config['discount'] * batch['masks'] * next_q\n",
    "                    target_q = jax.lax.stop_gradient(target_q)\n",
    "                    \n",
    "                    critic_loss = ((target_q-q)**2).mean()\n",
    "                    \n",
    "                    return critic_loss, {}\n",
    "        \n",
    "\n",
    "                grads,_ = jax.grad(critic_loss_fn, has_aux=True)(critic_params)\n",
    "                updates, new_opt_state = agent.critic.tx.update(grads, critic_opt_params, critic_params)\n",
    "                new_params = optax.apply_updates(critic_params, updates)\n",
    "\n",
    "                \n",
    "                return agent,new_params,new_opt_state\n",
    "            \n",
    "            \n",
    "            get_batch = lambda transitions,idx : jax.tree_map(lambda x : x[idx],transitions)\n",
    "                \n",
    "            agent,critic_params,critic_opt_params = jax.lax.fori_loop(0, num_steps, \n",
    "                        lambda i, args: one_update(*args,get_batch(transitions,idxs[i])),\n",
    "                        (agent,critic_params,critic_opt_params))\n",
    "            \n",
    "            #agent,critic_params,critic_opt_params = one_update(agent,critic_params,critic_opt_params,get_batch(transitions,idxs[0]))\n",
    "\n",
    "            return critic_params,critic_opt_params\n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "        opt_state = jax.vmap(agent.critic.tx.init,in_axes=0)(agent.critic.params)\n",
    "        critic = agent.critic.replace(opt_state=opt_state)\n",
    "        \n",
    "        tmp = partial(update_one_critic,agent=agent,transitions=transitions,idxs=idxs,num_steps=num_steps)\n",
    "        new_critic = jax.vmap(tmp,in_axes=(0,0))(critic.params,\n",
    "                                                 critic.opt_state)\n",
    "\n",
    "        agent = agent.replace(critic=new_critic)\n",
    "        \n",
    "    \n",
    "        return agent,{}\n",
    "       \n",
    "    \n",
    "\n",
    "        \n",
    "    @jax.jit\n",
    "    def update_actor(agent, batch: Batch):\n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "\n",
    "        def actor_loss_fn(actor_params):\n",
    "            \n",
    "            actions = agent.actor(batch['observations'], params=actor_params)\n",
    "            q = agent.critic(batch['observations'], actions)\n",
    "            \n",
    "            actor_loss = (-q).mean()\n",
    "            \n",
    "            return actor_loss, {\n",
    "                'actor_loss': actor_loss,\n",
    "              \n",
    "            }\n",
    "\n",
    "        new_actor, actor_info = agent.actor.apply_loss_fn(loss_fn=actor_loss_fn, has_aux=True)\n",
    "\n",
    "        return agent.replace(rng=new_rng,actor=new_actor,), {**actor_info}\n",
    "        \n",
    "\n",
    "\n",
    "    @jax.jit\n",
    "    def sample_actions(agent,observations: np.ndarray) -> jnp.ndarray:\n",
    "        actions = agent.actor(observations)\n",
    "       \n",
    "        return actions\n",
    "    \n",
    " \n",
    "\n",
    "def create_learner(\n",
    "                 seed: int,\n",
    "                 observations: jnp.ndarray,\n",
    "                 actions: jnp.ndarray,\n",
    "                 actor_lr: float = 3e-4,\n",
    "                 critic_lr: float = 3e-4,\n",
    "                 hidden_dims: Sequence[int] = (256, 256),\n",
    "                 discount: float = 0.99,\n",
    "                 tau: float = 0.005,\n",
    "            **kwargs):\n",
    "\n",
    "        print('Extra kwargs:', kwargs)\n",
    "\n",
    "        rng = jax.random.PRNGKey(seed)\n",
    "        rng, actor_key, critic_key = jax.random.split(rng, 3)\n",
    "\n",
    "        action_dim = actions.shape[-1]\n",
    "        actor_def = DeterministicPolicy((64,64), action_dim=action_dim,final_fc_init_scale=1.0)\n",
    "\n",
    "        actor_params = actor_def.init(actor_key, observations)['params']\n",
    "        actor = TrainState.create(actor_def, actor_params, tx=optax.adam(learning_rate=actor_lr))\n",
    "        \n",
    "        \n",
    "        critic_def = Critic(hidden_dims)\n",
    "        critic_params = critic_def.init(critic_key, observations, actions,False)['params']\n",
    "        tmp_critic = TrainState.create(critic_def, critic_params,optax.adam(learning_rate=critic_lr))\n",
    "        \n",
    "        \n",
    "        \n",
    "        critic_keys  = jax.random.split(critic_key, 5)\n",
    "        critic_params = jax.vmap(critic_def.init,in_axes=(0,None,None))(critic_keys, observations, actions)['params']\n",
    "        critic_opt_state = jax.vmap(tmp_critic.tx.init,in_axes=0)(critic_params)\n",
    "        critic = tmp_critic.replace(params=critic_params,opt_state=critic_opt_state)\n",
    "        ##### Extra\n",
    "        opt_state = jax.vmap(critic.tx.init,in_axes=0)(critic.params)\n",
    "        critic = critic.replace(params=critic_params,opt_state=opt_state)\n",
    "\n",
    "        \n",
    "        config = flax.core.FrozenDict(dict(\n",
    "            discount=discount,\n",
    "            target_update_rate=tau,    \n",
    "        ))\n",
    "\n",
    "        return SACAgent(rng, critic=critic, target_critic=tmp_critic, actor=actor, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from jaxrl_m.rollout import PolicyRollout,rollout_policy\n",
    "\n",
    "\n",
    "def f(anc_agent,obs,actor):\n",
    "\n",
    "    actions = anc_agent.actor(obs, params=actor)\n",
    "    q = anc_agent.critic(obs, actions,False,params=anc_agent.critic.params)\n",
    "   \n",
    "    return q\n",
    "    \n",
    "    \n",
    "@jax.jit\n",
    "def estimate_return(anc_agent,anc_return,acq_rollout:PolicyRollout,):\n",
    "    \n",
    "    acq_obs = acq_rollout.observations\n",
    "    acq_masks = acq_rollout.disc_masks\n",
    "  \n",
    "    acq_actor = acq_rollout.policy_params\n",
    "    acq_return = acq_rollout.policy_return\n",
    "    \n",
    "    anc_actor = anc_agent.actor.params\n",
    "    \n",
    "    acq_q = f(anc_agent,acq_obs,acq_actor)\n",
    "    anc_q = f(anc_agent,acq_obs,anc_actor)\n",
    "    \n",
    "    adv = ((acq_q - anc_q)*acq_masks).sum()/5\n",
    "    acq_return_pred = anc_return + adv\n",
    "  \n",
    "    \n",
    "    return acq_return_pred,acq_return\n",
    "\n",
    "\n",
    "def evaluate_critic(anc_agent,anc_return,policy_rollouts):\n",
    "\n",
    "    y_pred,y= [],[]\n",
    "    for policy_rollout in policy_rollouts:\n",
    "        \n",
    "        acq_return_pred,acq_return = estimate_return(anc_agent,anc_return,policy_rollout)\n",
    "        y_pred.append(acq_return_pred),y.append(acq_return)\n",
    "        \n",
    "    y_pred,y = np.array(y_pred),np.array(y)\n",
    "    a2 = jnp.clip(((y-y_pred)**2),a_min=1e-4).sum()\n",
    "    b2=((y-y.mean())**2).sum()\n",
    "    R2 = 1-(a2/b2)  \n",
    "    bias = (y_pred-y).mean()\n",
    "    \n",
    "    return R2,bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 15:15:39.954265: W external/xla/xla/service/gpu/nvptx_compiler.cc:679] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.103). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahdikallel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/tmptf4crcj3/wandb/run-20231220_151540-l88ri1c0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mahdikallel/jaxrl_m/runs/l88ri1c0' target=\"_blank\">scarlet-violet-257</a></strong> to <a href='https://wandb.ai/mahdikallel/jaxrl_m' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mahdikallel/jaxrl_m' target=\"_blank\">https://wandb.ai/mahdikallel/jaxrl_m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mahdikallel/jaxrl_m/runs/l88ri1c0' target=\"_blank\">https://wandb.ai/mahdikallel/jaxrl_m/runs/l88ri1c0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra kwargs: {'max_steps': 1000000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 516/1000000 [00:01<48:50, 341.04it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#agent = agent.reset_critic_optimizer()\u001b[39;00m\n\u001b[1;32m     82\u001b[0m agent, critic_update_info \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mupdate_many_critics(transitions,idxs,\u001b[38;5;241m5000\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m R2,bias \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpolicy_rollouts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_return\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpolicy_rollouts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m actor_batch \u001b[38;5;241m=\u001b[39m actor_buffer\u001b[38;5;241m.\u001b[39mget_all()      \n\u001b[1;32m     87\u001b[0m agent, actor_update_info \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mupdate_actor(actor_batch)   \n",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m, in \u001b[0;36mevaluate_critic\u001b[0;34m(anc_agent, anc_return, policy_rollouts)\u001b[0m\n\u001b[1;32m     35\u001b[0m y_pred,y\u001b[38;5;241m=\u001b[39m [],[]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m policy_rollout \u001b[38;5;129;01min\u001b[39;00m policy_rollouts:\n\u001b[0;32m---> 38\u001b[0m     acq_return_pred,acq_return \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43manc_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43manc_return\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpolicy_rollout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     y_pred\u001b[38;5;241m.\u001b[39mappend(acq_return_pred),y\u001b[38;5;241m.\u001b[39mappend(acq_return)\n\u001b[1;32m     41\u001b[0m y_pred,y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_pred),np\u001b[38;5;241m.\u001b[39marray(y)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m, in \u001b[0;36mestimate_return\u001b[0;34m(anc_agent, anc_return, acq_rollout)\u001b[0m\n\u001b[1;32m     19\u001b[0m acq_return \u001b[38;5;241m=\u001b[39m acq_rollout\u001b[38;5;241m.\u001b[39mpolicy_return\n\u001b[1;32m     21\u001b[0m anc_actor \u001b[38;5;241m=\u001b[39m anc_agent\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mparams\n\u001b[0;32m---> 23\u001b[0m acq_q \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43manc_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43macq_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43macq_actor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m anc_q \u001b[38;5;241m=\u001b[39m f(anc_agent,acq_obs,anc_actor)\n\u001b[1;32m     26\u001b[0m adv \u001b[38;5;241m=\u001b[39m ((acq_q \u001b[38;5;241m-\u001b[39m anc_q)\u001b[38;5;241m*\u001b[39macq_masks)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m, in \u001b[0;36mf\u001b[0;34m(anc_agent, obs, actor)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(anc_agent,obs,actor):\n\u001b[1;32m      6\u001b[0m     actions \u001b[38;5;241m=\u001b[39m anc_agent\u001b[38;5;241m.\u001b[39mactor(obs, params\u001b[38;5;241m=\u001b[39mactor)\n\u001b[0;32m----> 7\u001b[0m     q \u001b[38;5;241m=\u001b[39m anc_agent\u001b[38;5;241m.\u001b[39mcritic(obs, actions,\u001b[38;5;28;01mFalse\u001b[39;00m,params\u001b[38;5;241m=\u001b[39m\u001b[43manc_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'params'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import jax\n",
    "import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "from jaxrl_m.wandb import setup_wandb, default_wandb_config, get_flag_dict\n",
    "import wandb\n",
    "from jaxrl_m.evaluation import supply_rng, evaluate, flatten, EpisodeMonitor\n",
    "from jaxrl_m.dataset import ReplayBuffer\n",
    "from collections import deque\n",
    "from jaxrl_m.rollout import PolicyRollout,rollout_policy\n",
    "\n",
    "env_name='InvertedPendulum-v4'\n",
    "seed=np.random.choice(1000000)\n",
    "eval_episodes=10\n",
    "batch_size = 256\n",
    "max_steps = int(1e6)\n",
    "start_steps = 500                   \n",
    "log_interval = 5000\n",
    "#eval_interval = 10000\n",
    "\n",
    "wandb_config = default_wandb_config()\n",
    "wandb_config.update({\n",
    "    'project': 'd4rl_test',\n",
    "    'group': 'sac_test',\n",
    "    'name': 'sac_{env_name}',\n",
    "})\n",
    "\n",
    "\n",
    "env = EpisodeMonitor(gym.make(env_name))\n",
    "eval_env = EpisodeMonitor(gym.make(env_name))\n",
    "setup_wandb({\"bonjour\":1})\n",
    "\n",
    "example_transition = dict(\n",
    "    observations=env.observation_space.sample(),\n",
    "    actions=env.action_space.sample(),\n",
    "    rewards=0.0,\n",
    "    masks=1.0,\n",
    "    next_observations=env.observation_space.sample(),\n",
    "    discounts=1.0,\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer.create(example_transition, size=int(1e6))\n",
    "actor_buffer = ReplayBuffer.create(example_transition, size=int(5e3))\n",
    "\n",
    "agent = create_learner(seed,\n",
    "                example_transition['observations'][None],\n",
    "                example_transition['actions'][None],\n",
    "                max_steps=max_steps,\n",
    "                #**FLAGS.config\n",
    "                )\n",
    "\n",
    "exploration_metrics = dict()\n",
    "obs,info = env.reset()    \n",
    "exploration_rng = jax.random.PRNGKey(0)\n",
    "i = 0\n",
    "num_grad_updates = 0\n",
    "unlogged_steps = 0\n",
    "policy_rollouts = deque([], maxlen=10)\n",
    "with tqdm.tqdm(total=max_steps) as pbar:\n",
    "    \n",
    "    while i < max_steps:\n",
    "    \n",
    "        replay_buffer,actor_buffer,policy_rollout,policy_return,num_steps = rollout_policy(agent,env,exploration_rng,\n",
    "                replay_buffer,actor_buffer,\n",
    "                warmup=(i < start_steps))\n",
    "        policy_rollouts.append(policy_rollout)\n",
    "        unlogged_steps += num_steps\n",
    "        i+=num_steps\n",
    "        pbar.update(num_steps)\n",
    "        \n",
    "            \n",
    "        if replay_buffer.size > start_steps:\n",
    "        \n",
    "        \n",
    "            transitions = replay_buffer.get_all()\n",
    "            idxs = jax.random.choice(a=replay_buffer.size, shape=(5000,256), replace=True,key=jax.random.PRNGKey(0))\n",
    "            #agent = agent.reset_critic_optimizer()\n",
    "            agent, critic_update_info = agent.update_many_critics(transitions,idxs,5000)\n",
    "            R2,bias = evaluate_critic(agent,policy_rollouts[-1].policy_return,policy_rollouts)\n",
    "\n",
    "            \n",
    "            actor_batch = actor_buffer.get_all()      \n",
    "            agent, actor_update_info = agent.update_actor(actor_batch)   \n",
    "            num_grad_updates += 1 \n",
    "            update_info = {**critic_update_info, **actor_update_info,\n",
    "                           'R2_validation': R2,'bias': bias,'num_grad_updates': num_grad_updates}\n",
    "            \n",
    "            if unlogged_steps > log_interval:\n",
    "                exploration_metrics = {f'exploration/disc_return': policy_return}\n",
    "                wandb.log(exploration_metrics, step=int(i),commit=False)\n",
    "                train_metrics = {f'training/{k}': v for k, v in update_info.items()}\n",
    "                wandb.log(train_metrics, step=int(i),commit=False)\n",
    "                #wandb.log(exploration_metrics, step=i)\n",
    "                policy_fn = agent.actor\n",
    "                eval_info = evaluate(policy_fn, eval_env, num_episodes=eval_episodes)\n",
    "                eval_metrics = {f'evaluation/{k}': v for k, v in eval_info.items()}\n",
    "                print('evaluating')\n",
    "                wandb.log(eval_metrics, step=int(i),commit=True)\n",
    "                unlogged_steps = 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
