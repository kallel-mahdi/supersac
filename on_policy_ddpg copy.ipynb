{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementations of algorithms for continuous control.\"\"\"\n",
    "import functools\n",
    "from jaxrl_m.typing import *\n",
    "\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from jaxrl_m.common import TrainState, target_update, nonpytree_field\n",
    "from jaxrl_m.networks import DeterministicPolicy,Policy, Critic, ensemblize\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from functools import partial\n",
    "\n",
    "NUM_CRITICS = 5\n",
    "\n",
    "class SACAgent(flax.struct.PyTreeNode):\n",
    "    rng: PRNGKey\n",
    "    critic: TrainState\n",
    "    actor : TrainState\n",
    "    config: dict = nonpytree_field()\n",
    "    \n",
    "    @partial(jax.jit,static_argnames=('num_steps',))  \n",
    "    def update_many_critics(agent,transitions: Batch,idxs:jnp.array,num_steps:int,R2):\n",
    "\n",
    "        def update_one_critic(critic,idxs,\n",
    "                            agent,transitions,num_steps):\n",
    "            \n",
    "            def one_update(agent,critic,batch: Batch):\n",
    "                                  \n",
    "                def critic_loss_fn(critic_params):\n",
    "                    next_actions = agent.actor(batch['next_observations'])\n",
    "                    concat_actions = jnp.concatenate([batch[\"actions\"],next_actions])\n",
    "                    concat_observations = jnp.concatenate([batch[\"observations\"],batch[\"next_observations\"]])\n",
    "                    \n",
    "                    concat_q = critic(concat_observations, concat_actions,\n",
    "                                            True,params=critic_params)\n",
    "                    q,next_q = jnp.split(concat_q,2,axis=0) ## axis=1 for ensemble\n",
    "                    \n",
    "                    target_q = batch['rewards'] + agent.config['discount'] * batch['masks'] * next_q\n",
    "                    target_q = jax.lax.stop_gradient(target_q)\n",
    "                    \n",
    "                    critic_loss = ((target_q-q)**2).mean()\n",
    "                    \n",
    "                    return critic_loss, {}\n",
    "        \n",
    "\n",
    "                \n",
    "                new_critic, critic_info = critic.apply_loss_fn(loss_fn=critic_loss_fn, has_aux=True)\n",
    "                \n",
    "                return agent,new_critic\n",
    "            \n",
    "            \n",
    "            get_batch = lambda transitions,idx : jax.tree_map(lambda x : x[idx],transitions)\n",
    "                \n",
    "            agent,new_critic = jax.lax.fori_loop(0, num_steps, \n",
    "                        lambda i, args: one_update(*args,get_batch(transitions,idxs[i])),\n",
    "                        (agent,critic))\n",
    "            \n",
    "            return new_critic\n",
    "        \n",
    "        \n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "        critic = agent.critic\n",
    "        \n",
    "        ###### Reset critic params ######\n",
    "        \n",
    "        reset = lambda rng,params : critic.init(rng,\n",
    "                                                agent.config[\"observations\"], agent.config[\"actions\"],False)[\"params\"]\n",
    "        no_reset = lambda rng,params: params\n",
    "        f = lambda  mask,rng,params :lax.cond(mask,reset,no_reset,rng,params)\n",
    "        mask = jnp.zeros((NUM_CRITICS))\n",
    "        mask.at[jnp.argmin(R2)].set(1)\n",
    "        rngs = jax.random.split(agent.rng, NUM_CRITICS)\n",
    "        critic_params = jax.vmap(f,in_axes=(0,0,0))(mask,rngs,critic.params)\n",
    "        ###################################\n",
    "        critic_def = Critic((256,256))\n",
    "        critics = jax.vmap(TrainState.create,in_axes=(None,0,None))(critic_def,critic_params,optax.adam(learning_rate=3e-4))\n",
    "        tmp = partial(update_one_critic,agent=agent,transitions=transitions,num_steps=num_steps)\n",
    "        new_critics = jax.vmap(tmp,in_axes=(0,0))(critics,idxs)\n",
    "        agent = agent.replace(rng=new_rng,critic=new_critics)\n",
    "        \n",
    "    \n",
    "        return agent,{}\n",
    "    \n",
    "    \n",
    "    @jax.jit\n",
    "    def update_actor(agent,batch: Batch,R2):\n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "        \n",
    "        def actor_loss_fn(actor_params,R2):\n",
    "            \n",
    "            actions = agent.actor(batch['observations'], params=actor_params)\n",
    "            \n",
    "            call_one_critic = lambda observations,actions,params : agent.critic(observations,actions,params=params)\n",
    "            q = jax.vmap(call_one_critic,in_axes=(None,None,0))(batch['observations'], actions,agent.critic.params)##critic\n",
    "            q_weights = jax.nn.softmax(R2,axis=0)\n",
    "            q = jnp.sum(q_weights*q,axis=0)\n",
    "            q = q*batch['masks']  \n",
    "            actor_loss = (-q).mean()\n",
    "            lr_bonus = jnp.exp(jnp.max(R2))/jnp.exp(1)\n",
    "            actor_loss = lr_bonus*(-q).mean()\n",
    "            \n",
    "            return actor_loss, {\n",
    "                'actor_loss': actor_loss,\n",
    "              \n",
    "            }\n",
    "\n",
    "        loss_fn = partial(actor_loss_fn,R2=R2)\n",
    "        new_actor, actor_info = agent.actor.apply_loss_fn(loss_fn=loss_fn, has_aux=True)\n",
    "\n",
    "        return agent.replace(rng=new_rng,actor=new_actor,), {**actor_info}\n",
    "    \n",
    "       \n",
    "\n",
    "    @jax.jit\n",
    "    def sample_actions(agent, observations: np.ndarray, seed, random=False) -> jnp.ndarray:\n",
    "        \n",
    "        actions = agent.actor(observations)\n",
    "        \n",
    "        add_noise =  lambda actions,: actions+0.1*jax.random.normal(seed, actions.shape)\n",
    "        identity = lambda actions : actions\n",
    "        actions = jax.lax.cond(random, add_noise,identity,actions)\n",
    "        \n",
    "        actions = jnp.clip(actions, -1, 1)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    " \n",
    "\n",
    "def create_learner(\n",
    "                 seed: int,\n",
    "                 observations: jnp.ndarray,\n",
    "                 actions: jnp.ndarray,\n",
    "                 actor_lr: float = 3e-4,\n",
    "                 critic_lr: float = 3e-4,\n",
    "                 hidden_dims: Sequence[int] = (256, 256),\n",
    "                 discount: float = 0.99,\n",
    "                 \n",
    "            **kwargs):\n",
    "\n",
    "        print('Extra kwargs:', kwargs)\n",
    "\n",
    "        rng = jax.random.PRNGKey(seed)\n",
    "        rng, actor_key, critic_key = jax.random.split(rng, 3)\n",
    "\n",
    "        action_dim = actions.shape[-1]\n",
    "        actor_def = DeterministicPolicy((64,64), action_dim=action_dim,final_fc_init_scale=1.0)\n",
    "\n",
    "        actor_params = actor_def.init(actor_key, observations)['params']\n",
    "        actor = TrainState.create(actor_def, actor_params, tx=optax.adam(learning_rate=actor_lr))\n",
    "       \n",
    "        critic_def = Critic(hidden_dims)\n",
    "        critic_keys  = jax.random.split(critic_key, NUM_CRITICS)\n",
    "        critic_params = jax.vmap(critic_def.init,in_axes=(0,None,None))(critic_keys, observations, actions)['params']\n",
    "        critics = jax.vmap(TrainState.create,in_axes=(None,0,None))(critic_def,critic_params,optax.adam(learning_rate=critic_lr))\n",
    "\n",
    "        config = flax.core.FrozenDict(dict(\n",
    "            discount=discount,\n",
    "            observations = observations,\n",
    "            actions = actions,\n",
    "            \n",
    "        ))\n",
    "\n",
    "        return SACAgent(rng,config=config,critic=critics,actor=actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 22:34:55.386320: W external/xla/xla/service/gpu/nvptx_compiler.cc:679] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.103). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahdikallel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/tmp5nxjnqfi/wandb/run-20240124_223456-wouloulou_20240124_223455</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mahdikallel/ddpg_on_policy/runs/wouloulou_20240124_223455' target=\"_blank\">wouloulou</a></strong> to <a href='https://wandb.ai/mahdikallel/ddpg_on_policy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mahdikallel/ddpg_on_policy' target=\"_blank\">https://wandb.ai/mahdikallel/ddpg_on_policy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mahdikallel/ddpg_on_policy/runs/wouloulou_20240124_223455' target=\"_blank\">https://wandb.ai/mahdikallel/ddpg_on_policy/runs/wouloulou_20240124_223455</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra kwargs: {'max_steps': 1000000}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m warmup_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m warmup_steps \u001b[38;5;241m<\u001b[39m start_steps:\n\u001b[0;32m---> 73\u001b[0m     replay_buffer,_,_,policy_return,undisc_policy_return,num_steps \u001b[38;5;241m=\u001b[39m \u001b[43mrollout_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexploration_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mactor_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mnum_rollouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_ROLLOUTS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     warmup_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_steps\n\u001b[1;32m     79\u001b[0m random \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/supersac/jaxrl_m/rollout.py:46\u001b[0m, in \u001b[0;36mrollout_policy\u001b[0;34m(agent, env, exploration_rng, replay_buffer, actor_buffer, warmup, num_rollouts, random)\u001b[0m\n\u001b[1;32m     42\u001b[0m transition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(observations\u001b[38;5;241m=\u001b[39mobs,actions\u001b[38;5;241m=\u001b[39maction,\n\u001b[1;32m     43\u001b[0m     rewards\u001b[38;5;241m=\u001b[39mreward,masks\u001b[38;5;241m=\u001b[39mmask,next_observations\u001b[38;5;241m=\u001b[39mnext_obs,discounts\u001b[38;5;241m=\u001b[39mdisc)\n\u001b[1;32m     45\u001b[0m replay_buffer\u001b[38;5;241m.\u001b[39madd_transition(transition)\n\u001b[0;32m---> 46\u001b[0m \u001b[43mactor_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_transition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m observations[\u001b[38;5;241m1000\u001b[39m\u001b[38;5;241m*\u001b[39mn_rollouts\u001b[38;5;241m+\u001b[39mepisode_step] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m     49\u001b[0m disc_masks[\u001b[38;5;241m1000\u001b[39m\u001b[38;5;241m*\u001b[39mn_rollouts\u001b[38;5;241m+\u001b[39mepisode_step] \u001b[38;5;241m=\u001b[39m disc\n",
      "File \u001b[0;32m~/Desktop/supersac/jaxrl_m/dataset.py:101\u001b[0m, in \u001b[0;36mReplayBuffer.add_transition\u001b[0;34m(self, transition)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_idx\u001b[39m(buffer, new_element):\n\u001b[1;32m     99\u001b[0m     buffer[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpointer] \u001b[38;5;241m=\u001b[39m new_element\n\u001b[0;32m--> 101\u001b[0m \u001b[43mtree_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mset_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpointer \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpointer \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_size\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpointer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize)\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/tree_util.py:242\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_map\u001b[39m(f: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any],\n\u001b[1;32m    207\u001b[0m              tree: Any,\n\u001b[1;32m    208\u001b[0m              \u001b[38;5;241m*\u001b[39mrest: Any,\n\u001b[1;32m    209\u001b[0m              is_leaf: Callable[[Any], \u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Maps a multi-input function over pytree args to produce a new pytree.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    [[5, 7, 9], [6, 1, 2]]\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m   leaves, treedef \u001b[38;5;241m=\u001b[39m \u001b[43mtree_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m   all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treedef\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[1;32m    244\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m treedef\u001b[38;5;241m.\u001b[39munflatten(f(\u001b[38;5;241m*\u001b[39mxs) \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mall_leaves))\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/tree_util.py:83\u001b[0m, in \u001b[0;36mtree_flatten\u001b[0;34m(tree, is_leaf)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_flatten\u001b[39m(tree: Any,\n\u001b[1;32m     64\u001b[0m                  is_leaf: Callable[[Any], \u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     65\u001b[0m                  ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Leaf], PyTreeDef]:\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Flattens a pytree.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m  The flattening order (i.e. the order of elements in the output list)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    element is a treedef representing the structure of the flattened tree.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import jax\n",
    "import tqdm\n",
    "from collections import deque\n",
    "import wandb    \n",
    "import gymnasium as gym\n",
    "import copy\n",
    "\n",
    "from jaxrl_m.utils import *\n",
    "from jaxrl_m.common import CodeTimer\n",
    "from jaxrl_m.wandb import setup_wandb, default_wandb_config, get_flag_dict\n",
    "from jaxrl_m.evaluation import supply_rng, evaluate, flatten, EpisodeMonitor\n",
    "from jaxrl_m.common import compute_dormant\n",
    "from jaxrl_m.dataset import ReplayBuffer\n",
    "from jaxrl_m.rollout import PolicyRollout,rollout_policy,rollout_policy2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NUM_ROLLOUTS = 3\n",
    "MAX_SIZE = 25\n",
    "\n",
    "env = 'HalfCheetah'\n",
    "env_name=env+'-v4'\n",
    "seed=np.random.choice(1000000)\n",
    "eval_episodes=10\n",
    "batch_size = 256\n",
    "max_steps = int(1e6)\n",
    "start_steps = 50000                  \n",
    "log_interval = 5000\n",
    "\n",
    "wandb_config = default_wandb_config()   \n",
    "wandb_config.update({\n",
    "    'project': 'ddpg_on_policy',\n",
    "    'name': env+\"_\"+str(seed),\n",
    "    'hyperparam_dict':{\"name\":env},\n",
    "})\n",
    "\n",
    "env = EpisodeMonitor(gym.make(env_name))\n",
    "eval_env = EpisodeMonitor(gym.make(env_name))\n",
    "setup_wandb(**wandb_config)\n",
    "\n",
    "example_transition = dict(\n",
    "    observations=env.observation_space.sample(),\n",
    "    actions=env.action_space.sample(),\n",
    "    rewards=0.0,\n",
    "    masks=1.0,\n",
    "    next_observations=env.observation_space.sample(),\n",
    "    discounts=1.0,\n",
    ")   \n",
    "\n",
    "replay_buffer = ReplayBuffer.create(example_transition, size=int(5e5))\n",
    "actor_buffer = ReplayBuffer.create(example_transition, size=int(5e3))\n",
    "\n",
    "agent = create_learner(seed,\n",
    "                example_transition['observations'][None],\n",
    "                example_transition['actions'][None],\n",
    "                max_steps=max_steps,\n",
    "                )\n",
    "\n",
    "\n",
    "obs,info = env.reset()    \n",
    "exploration_rng = jax.random.PRNGKey(0)\n",
    "unlogged_steps,num_grad_updates,i = 0,0,0\n",
    "policy_rollouts = deque([], maxlen=MAX_SIZE)\n",
    "R2 = jnp.ones(NUM_CRITICS)\n",
    "\n",
    "warmup_steps = 0\n",
    "while warmup_steps < start_steps:\n",
    "    \n",
    "    replay_buffer,_,_,policy_return,undisc_policy_return,num_steps = rollout_policy(agent,env,exploration_rng,\n",
    "                        replay_buffer,actor_buffer,\n",
    "                        warmup=False,num_rollouts=NUM_ROLLOUTS)\n",
    "    warmup_steps += num_steps\n",
    "\n",
    "\n",
    "random = True\n",
    "actor_old = agent.actor.params\n",
    "exploration_key = jax.random.PRNGKey(0)\n",
    "first = True\n",
    "with tqdm.tqdm(total=max_steps) as pbar:\n",
    "    \n",
    "    while i < max_steps:\n",
    "\n",
    "        ### Rollout policy ###\n",
    "        num_rollouts =  2 if random else NUM_ROLLOUTS\n",
    "\n",
    "        actor_buffer = ReplayBuffer.create(example_transition, size=int(NUM_ROLLOUTS*1000))\n",
    "        replay_buffer,actor_buffer,policy_rollout,policy_return,undisc_policy_return,num_steps = rollout_policy(\n",
    "                                                                agent,env,exploration_rng,\n",
    "                                                                replay_buffer,actor_buffer,warmup=False,\n",
    "                                                                num_rollouts=num_rollouts,random=random,\n",
    "                                                                )\n",
    "        i+=num_steps\n",
    "        unlogged_steps += num_steps\n",
    "        pbar.update(num_steps)\n",
    "        \n",
    "        if not random : \n",
    "            \n",
    "            policy_rollouts.append(policy_rollout)\n",
    "            \n",
    "            ### Update critics ###\n",
    "            transitions = replay_buffer.get_all()\n",
    "            tmp = partial(jax.random.choice,a=replay_buffer.size, shape=(10000,256), replace=True)\n",
    "            idxs = jax.vmap(tmp)(jax.random.split(exploration_key, NUM_CRITICS))\n",
    "            agent, critic_update_info = agent.update_many_critics(transitions,idxs,10000,R2)\n",
    "\n",
    "            ### Update critic weights ##                    \n",
    "            flattened_rollouts = flatten_rollouts(policy_rollouts)\n",
    "            R2,bias,_,_ = train_evaluation(agent,policy_rollout.policy_return,flattened_rollouts)\n",
    "         \n",
    "            ### Update actor ###        \n",
    "            actor_batch = actor_buffer.get_all() \n",
    "            agent, actor_update_info = agent.update_actor(actor_batch,R2.reshape(-1,1))   \n",
    "            \n",
    "            ### Divers ###\n",
    "            percent_dormant = compute_dormant(agent,policy_rollout.observations,0.2)\n",
    "            action_distance = measure_action_distance(agent,agent.actor.params,actor_old,policy_rollout.observations)\n",
    "            exploration_rng, key = jax.random.split(exploration_rng)\n",
    "            actor_old = agent.actor.params                    \n",
    "            num_grad_updates += 1 \n",
    "\n",
    "        if unlogged_steps >= log_interval:\n",
    "\n",
    "            ### Log stuff to w&b ### \n",
    "            update_info = {**critic_update_info, **actor_update_info}\n",
    "            R2_train_info = {'max': jnp.max(R2),'bias': bias[jnp.argmax(R2)],\n",
    "                                \"histogram\": wandb.Histogram(jnp.clip(R2,a_min=-1,a_max=1)),\n",
    "                                }\n",
    "            R2_metrics = {f'R2/{k}': v for k, v in R2_train_info.items()}\n",
    "            exploration_metrics = {f'exploration/disc_return': policy_return}\n",
    "            train_metrics = {f'training/{k}': v for k, v in update_info.items()}\n",
    "            \n",
    "            wandb.log(train_metrics, step=int(i),commit=False)\n",
    "            wandb.log({\"undisc_return\":undisc_policy_return,\"dormant\":percent_dormant,\"action_distance\":action_distance},step=int(i),commit=False)\n",
    "            wandb.log(R2_metrics, step=int(i),commit=False)\n",
    "            wandb.log(exploration_metrics, step=int(i),commit=True)\n",
    "            \n",
    "            unlogged_steps = 0\n",
    "\n",
    "        random = not random\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7265108489777838"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "disc = 0.99\n",
    "rewards = [disc**i for i in range(1000)]\n",
    "rewards = np.cumsum(rewards)*(1-disc)\n",
    "#np.argmax(rewards>0.995)\n",
    "rewards[128]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
