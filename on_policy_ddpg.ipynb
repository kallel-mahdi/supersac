{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementations of algorithms for continuous control.\"\"\"\n",
    "import functools\n",
    "from jaxrl_m.typing import *\n",
    "\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from jaxrl_m.common import TrainState, target_update, nonpytree_field\n",
    "from jaxrl_m.networks import DeterministicPolicy,Policy, Critic, ensemblize\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "NUM_CRITICS = 5\n",
    "NUM_ROLLOUTS = 5\n",
    "MAX_SIZE = 40\n",
    "\n",
    "class SACAgent(flax.struct.PyTreeNode):\n",
    "    rng: PRNGKey\n",
    "    critic: TrainState\n",
    "    actor : TrainState\n",
    "    config: dict = nonpytree_field()\n",
    "    \n",
    "    @partial(jax.jit,static_argnames=('num_steps',))  \n",
    "    def update_many_critics(agent,transitions: Batch,idxs:jnp.array,num_steps:int,R2):\n",
    "\n",
    "        def update_one_critic(critic,idxs,\n",
    "                            agent,transitions,num_steps):\n",
    "            \n",
    "            def one_update(agent,critic,batch: Batch):\n",
    "                                  \n",
    "                def critic_loss_fn(critic_params):\n",
    "                    next_actions = agent.actor(batch['next_observations'])\n",
    "                    concat_actions = jnp.concatenate([batch[\"actions\"],next_actions])\n",
    "                    concat_observations = jnp.concatenate([batch[\"observations\"],batch[\"next_observations\"]])\n",
    "                    \n",
    "                    concat_q = critic(concat_observations, concat_actions,\n",
    "                                            True,params=critic_params)\n",
    "                    q,next_q = jnp.split(concat_q,2,axis=0) ## axis=1 for ensemble\n",
    "                    \n",
    "                    target_q = batch['rewards'] + agent.config['discount'] * batch['masks'] * next_q\n",
    "                    target_q = jax.lax.stop_gradient(target_q)\n",
    "                    \n",
    "                    critic_loss = ((target_q-q)**2).mean()\n",
    "                    \n",
    "                    return critic_loss, {}\n",
    "        \n",
    "\n",
    "                \n",
    "                new_critic, critic_info = critic.apply_loss_fn(loss_fn=critic_loss_fn, has_aux=True)\n",
    "                \n",
    "                return agent,new_critic\n",
    "            \n",
    "            \n",
    "            get_batch = lambda transitions,idx : jax.tree_map(lambda x : x[idx],transitions)\n",
    "                \n",
    "            agent,new_critic = jax.lax.fori_loop(0, num_steps, \n",
    "                        lambda i, args: one_update(*args,get_batch(transitions,idxs[i])),\n",
    "                        (agent,critic))\n",
    "            \n",
    "            return new_critic\n",
    "        \n",
    "        \n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "        critic = agent.critic\n",
    "        \n",
    "        ###### Reset critic params ######\n",
    "        \n",
    "        reset = lambda rng,params : critic.init(rng,\n",
    "                                                agent.config[\"observations\"], agent.config[\"actions\"],False)[\"params\"]\n",
    "        no_reset = lambda rng,params: params\n",
    "        f = lambda  mask,rng,params :lax.cond(mask,reset,no_reset,rng,params)\n",
    "        mask = jnp.zeros((NUM_CRITICS))\n",
    "        mask.at[jnp.argmin(R2)].set(1)\n",
    "        rngs = jax.random.split(agent.rng, NUM_CRITICS)\n",
    "        critic_params = jax.vmap(f,in_axes=(0,0,0))(mask,rngs,critic.params)\n",
    "        ###################################\n",
    "        critic_def = Critic((256,256))\n",
    "        critics = jax.vmap(TrainState.create,in_axes=(None,0,None))(critic_def,critic_params,optax.adam(learning_rate=3e-4))\n",
    "        tmp = partial(update_one_critic,agent=agent,transitions=transitions,num_steps=num_steps)\n",
    "        new_critics = jax.vmap(tmp,in_axes=(0,0))(critics,idxs)\n",
    "        agent = agent.replace(rng=new_rng,critic=new_critics)\n",
    "        \n",
    "    \n",
    "        return agent,{}\n",
    "    \n",
    "    @partial(jax.jit,static_argnames=('num_steps',)) \n",
    "    def update_many_o_critics(agent,c_critics_params,\n",
    "                              transitions: Batch,idxs:jnp.array,num_steps:int,R2): \n",
    "        \n",
    "      \n",
    "        def update_o_critic(o_critic,idxs,agent,transitions,num_steps,\n",
    "                            c_critics_params,R2):\n",
    "                \n",
    "                def one_update(agent,o_critic,c_critics_params,R2:float,batch: Batch,):\n",
    "                                    \n",
    "                    def critic_loss_fn(o_critic_params,R2):\n",
    "                        next_actions = agent.actor(batch['next_observations'])\n",
    "                        q = o_critic(batch[\"observations\"], batch[\"actions\"],\n",
    "                                                True,params=o_critic_params)\n",
    "                        next_q1 = o_critic(batch[\"next_observations\"], next_actions,\n",
    "                                                False,params=o_critic_params)\n",
    "                        call_one_critic = lambda observations,actions,params : o_critic(observations,actions,params=params)\n",
    "                        next_q = jax.vmap(call_one_critic,in_axes=(None,None,0))(batch['next_observations'], next_actions,c_critics_params)\n",
    "                        q_weights = jax.nn.softmax(R2,axis=0)\n",
    "                        # q_weights /= jnp.max(q_weights)\n",
    "                        # next_q = jnp.max(q_weights.reshape(-1,1)*next_q,axis=0)\n",
    "                        # next_q = jnp.maximum(next_q,next_q1)\n",
    "                        next_q_mean = jnp.average(next_q, weights=q_weights,axis=0)\n",
    "                        next_q_var = jnp.average((next_q - next_q_mean)**2, weights=q_weights,axis=0)\n",
    "                        next_q_std = jnp.sqrt(next_q_var)\n",
    "                        next_q_o = next_q_mean + next_q_std\n",
    "                        \n",
    "                        target_q = batch['rewards'] + agent.config['discount'] * batch['masks'] * next_q_o\n",
    "                        target_q = jax.lax.stop_gradient(target_q)\n",
    "                        \n",
    "                        critic_loss = ((target_q-q)**2).mean()\n",
    "                        \n",
    "                        return critic_loss, {}\n",
    "\n",
    "                    tmp = partial(critic_loss_fn,R2=R2)\n",
    "                    new_critic, critic_info = o_critic.apply_loss_fn(loss_fn=tmp, has_aux=True)\n",
    "                    \n",
    "                    return agent,new_critic,c_critics_params,R2\n",
    "                \n",
    "                \n",
    "                get_batch = lambda transitions,idx : jax.tree_map(lambda x : x[idx],transitions)\n",
    "                agent,new_critic,_,_ = jax.lax.fori_loop(0, num_steps, \n",
    "                            lambda i, args: one_update(*args,get_batch(transitions,idxs[i])),\n",
    "                            (agent,o_critic,c_critics_params,R2))\n",
    "            \n",
    "                \n",
    "                return new_critic\n",
    "            \n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "        critic_keys  = jax.random.split(curr_key, NUM_CRITICS)\n",
    "        critic_def = Critic((256,256))  \n",
    "        o_critic_params = jax.vmap(critic_def.init,in_axes=(0,None,None))(critic_keys, \n",
    "                                                                          agent.config[\"observations\"], agent.config[\"actions\"])[\"params\"]\n",
    "        critics = jax.vmap(TrainState.create,in_axes=(None,0,None))(critic_def,o_critic_params,optax.adam(learning_rate=3e-4))\n",
    "        tmp = partial(update_o_critic,agent=agent,transitions=transitions,num_steps=num_steps,\n",
    "                      c_critics_params=c_critics_params,R2=R2)\n",
    "        new_critics = jax.vmap(tmp,in_axes=(0,0))(critics,idxs)\n",
    "        agent = agent.replace(rng=new_rng,critic=new_critics)\n",
    "   \n",
    "        return agent,{}\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    @jax.jit\n",
    "    def update_actor(agent,batch: Batch,R2):\n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "        \n",
    "        def actor_loss_fn(actor_params,R2):\n",
    "            \n",
    "            actions = agent.actor(batch['observations'], params=actor_params)\n",
    "            \n",
    "            call_one_critic = lambda observations,actions,params : agent.critic(observations,actions,params=params)\n",
    "            q = jax.vmap(call_one_critic,in_axes=(None,None,0))(batch['observations'], actions,agent.critic.params)##critic\n",
    "            q_weights = jax.nn.softmax(R2,axis=0)\n",
    "            q = jnp.sum(q_weights*q,axis=0)\n",
    "            q = q*batch['discounts']  \n",
    "            lr_bonus = jnp.exp(jnp.max(R2))/jnp.exp(1)\n",
    "            actor_loss = lr_bonus*(-q).mean()\n",
    "            # actor_loss = (-q).mean()\n",
    "            \n",
    "            \n",
    "            return actor_loss, {\n",
    "                'actor_loss': actor_loss,\n",
    "              \n",
    "            }\n",
    "\n",
    "        loss_fn = partial(actor_loss_fn,R2=R2)\n",
    "        new_actor, actor_info = agent.actor.apply_loss_fn(loss_fn=loss_fn, has_aux=True)\n",
    "\n",
    "        return agent.replace(rng=new_rng,actor=new_actor,), {**actor_info}\n",
    "    \n",
    "       \n",
    "\n",
    "    @jax.jit\n",
    "    def sample_actions(agent,observations: np.ndarray,seed,random=False) -> jnp.ndarray:\n",
    "        \n",
    "        actions = agent.actor(observations)\n",
    "        actions = jnp.clip(actions,-1,1)\n",
    "        return actions\n",
    "    \n",
    " \n",
    "\n",
    "def create_learner(\n",
    "                 seed: int,\n",
    "                 observations: jnp.ndarray,\n",
    "                 actions: jnp.ndarray,\n",
    "                 actor_lr: float = 3e-4,\n",
    "                 critic_lr: float = 3e-4,\n",
    "                 hidden_dims: Sequence[int] = (256, 256),\n",
    "                 discount: float = 0.99,\n",
    "                 \n",
    "            **kwargs):\n",
    "\n",
    "        print('Extra kwargs:', kwargs)\n",
    "\n",
    "        rng = jax.random.PRNGKey(seed)\n",
    "        rng, actor_key, critic_key = jax.random.split(rng, 3)\n",
    "\n",
    "        action_dim = actions.shape[-1]\n",
    "        actor_def = DeterministicPolicy((256,256), action_dim=action_dim,final_fc_init_scale=1.0)\n",
    "\n",
    "        actor_params = actor_def.init(actor_key, observations)['params']\n",
    "        actor = TrainState.create(actor_def, actor_params, tx=optax.adam(learning_rate=actor_lr))\n",
    "        \n",
    "        critic_def = Critic(hidden_dims)\n",
    "        critic_keys  = jax.random.split(critic_key, NUM_CRITICS)\n",
    "        critic_params = jax.vmap(critic_def.init,in_axes=(0,None,None))(critic_keys, observations, actions)['params']\n",
    "        critics = jax.vmap(TrainState.create,in_axes=(None,0,None))(critic_def,critic_params,optax.adam(learning_rate=critic_lr))\n",
    "\n",
    "        \n",
    "        config = flax.core.FrozenDict(dict(\n",
    "            discount=discount,\n",
    "            observations = observations,\n",
    "            actions = actions,\n",
    "            \n",
    "        ))\n",
    "\n",
    "        return SACAgent(rng,config=config,critic=critics,actor=actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 15:56:06.801530: W external/xla/xla/service/gpu/nvptx_compiler.cc:679] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.103). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahdikallel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/tmpkhhj06zj/wandb/run-20240122_155607-wouloulou_20240122_155607</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mahdikallel/normalized/runs/wouloulou_20240122_155607' target=\"_blank\">wouloulou</a></strong> to <a href='https://wandb.ai/mahdikallel/normalized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mahdikallel/normalized' target=\"_blank\">https://wandb.ai/mahdikallel/normalized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mahdikallel/normalized/runs/wouloulou_20240122_155607' target=\"_blank\">https://wandb.ai/mahdikallel/normalized/runs/wouloulou_20240122_155607</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra kwargs: {'max_steps': 1000000}\n",
      "Extra kwargs: {'max_steps': 1000000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5000/1000000 [00:05<17:53, 926.86it/s] \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(5,), (5, 256)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/util.py:284\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/util.py:277\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 277\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:152\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 152\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:168\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(5,), (5, 256)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 154\u001b[0m\n\u001b[1;32m    152\u001b[0m tmp \u001b[38;5;241m=\u001b[39m partial(jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice,a\u001b[38;5;241m=\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39msize, shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10000\u001b[39m,\u001b[38;5;241m256\u001b[39m), replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    153\u001b[0m idxs \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(tmp)(jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(exploration_key, NUM_CRITICS))                \n\u001b[0;32m--> 154\u001b[0m o_agent, _ \u001b[38;5;241m=\u001b[39m \u001b[43mo_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_many_o_critics\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43midxs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mR2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m### Update optimistic critic weights ##\u001b[39;00m\n\u001b[1;32m    157\u001b[0m flattened_rollouts \u001b[38;5;241m=\u001b[39m flatten_rollouts(policy_rollouts)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 147\u001b[0m, in \u001b[0;36mSACAgent.update_many_o_critics\u001b[0;34m(agent, c_critics_params, transitions, idxs, num_steps, R2)\u001b[0m\n\u001b[1;32m    144\u001b[0m critics \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(TrainState\u001b[38;5;241m.\u001b[39mcreate,in_axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28;01mNone\u001b[39;00m))(critic_def,o_critic_params,optax\u001b[38;5;241m.\u001b[39madam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m))\n\u001b[1;32m    145\u001b[0m tmp \u001b[38;5;241m=\u001b[39m partial(update_o_critic,agent\u001b[38;5;241m=\u001b[39magent,transitions\u001b[38;5;241m=\u001b[39mtransitions,num_steps\u001b[38;5;241m=\u001b[39mnum_steps,\n\u001b[1;32m    146\u001b[0m               c_critics_params\u001b[38;5;241m=\u001b[39mc_critics_params,R2\u001b[38;5;241m=\u001b[39mR2)\n\u001b[0;32m--> 147\u001b[0m new_critics \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43min_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcritics\u001b[49m\u001b[43m,\u001b[49m\u001b[43midxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m agent \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mreplace(rng\u001b[38;5;241m=\u001b[39mnew_rng,critic\u001b[38;5;241m=\u001b[39mnew_critics)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m agent,{}\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 132\u001b[0m, in \u001b[0;36mSACAgent.update_many_o_critics.<locals>.update_o_critic\u001b[0;34m(o_critic, idxs, agent, transitions, num_steps, c_critics_params, R2)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agent,new_critic,c_critics_params,R2\n\u001b[1;32m    131\u001b[0m get_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m transitions,idx : jax\u001b[38;5;241m.\u001b[39mtree_map(\u001b[38;5;28;01mlambda\u001b[39;00m x : x[idx],transitions)\n\u001b[0;32m--> 132\u001b[0m agent,new_critic,_,_ \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfori_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43midxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43mo_critic\u001b[49m\u001b[43m,\u001b[49m\u001b[43mc_critics_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43mR2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_critic\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 133\u001b[0m, in \u001b[0;36mSACAgent.update_many_o_critics.<locals>.update_o_critic.<locals>.<lambda>\u001b[0;34m(i, args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agent,new_critic,c_critics_params,R2\n\u001b[1;32m    131\u001b[0m get_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m transitions,idx : jax\u001b[38;5;241m.\u001b[39mtree_map(\u001b[38;5;28;01mlambda\u001b[39;00m x : x[idx],transitions)\n\u001b[1;32m    132\u001b[0m agent,new_critic,_,_ \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mfori_loop(\u001b[38;5;241m0\u001b[39m, num_steps, \n\u001b[0;32m--> 133\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m i, args: \u001b[43mone_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43midxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    134\u001b[0m             (agent,o_critic,c_critics_params,R2))\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_critic\n",
      "Cell \u001b[0;32mIn[1], line 126\u001b[0m, in \u001b[0;36mSACAgent.update_many_o_critics.<locals>.update_o_critic.<locals>.one_update\u001b[0;34m(agent, o_critic, c_critics_params, R2, batch)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m critic_loss, {}\n\u001b[1;32m    125\u001b[0m tmp \u001b[38;5;241m=\u001b[39m partial(critic_loss_fn,R2\u001b[38;5;241m=\u001b[39mR2)\n\u001b[0;32m--> 126\u001b[0m new_critic, critic_info \u001b[38;5;241m=\u001b[39m \u001b[43mo_critic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m agent,new_critic,c_critics_params,R2\n",
      "File \u001b[0;32m~/Desktop/supersac/jaxrl_m/common.py:148\u001b[0m, in \u001b[0;36mTrainState.apply_loss_fn\u001b[0;34m(self, loss_fn, pmap_axis, has_aux)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03mTakes a gradient step towards minimizing `loss_fn`. Internally, this calls\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m`jax.grad` followed by `TrainState.apply_gradients`. If pmap_axis is provided,\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03madditionally it averages gradients (and info) across devices before performing update.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[0;32m--> 148\u001b[0m     grads, info \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pmap_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         grads \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mpmean(grads, axis_name\u001b[38;5;241m=\u001b[39mpmap_axis)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 113\u001b[0m, in \u001b[0;36mSACAgent.update_many_o_critics.<locals>.update_o_critic.<locals>.one_update.<locals>.critic_loss_fn\u001b[0;34m(o_critic_params, R2)\u001b[0m\n\u001b[1;32m    109\u001b[0m q_weights \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(R2,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# q_weights /= jnp.max(q_weights)\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# next_q = jnp.max(q_weights.reshape(-1,1)*next_q,axis=0)\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# next_q = jnp.maximum(next_q,next_q1)\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m next_q_mean \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msum(\u001b[43mq_weights\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnext_q\u001b[49m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    114\u001b[0m next_q_var \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39maverage((next_q \u001b[38;5;241m-\u001b[39m next_q_mean)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, weights\u001b[38;5;241m=\u001b[39mq_weights,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    115\u001b[0m next_q_std \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqrt(next_q_var)\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:728\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 728\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:256\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    254\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 256\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/numpy/ufuncs.py:96\u001b[0m, in \u001b[0;36m_maybe_bool_binop.<locals>.fn\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(x1, x2, \u001b[38;5;241m/\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m   x1, x2 \u001b[38;5;241m=\u001b[39m \u001b[43mpromote_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m lax_fn(x1, x2) \u001b[38;5;28;01mif\u001b[39;00m x1\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_ \u001b[38;5;28;01melse\u001b[39;00m bool_lax_fn(x1, x2)\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:363\u001b[0m, in \u001b[0;36mpromote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    361\u001b[0m check_arraylike(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    362\u001b[0m _check_no_float0s(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpromote_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpromote_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:248\u001b[0m, in \u001b[0;36mpromote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mnumpy_rank_promotion\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    247\u001b[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[0;32m--> 248\u001b[0m result_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, (\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m (result_rank \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(shp)) \u001b[38;5;241m+\u001b[39m shp)\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arg, shp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, shapes)]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:168\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    166\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(5,), (5, 256)]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import jax\n",
    "import tqdm\n",
    "from collections import deque\n",
    "import wandb    \n",
    "import gymnasium as gym\n",
    "import copy\n",
    "\n",
    "from jaxrl_m.utils import *\n",
    "from jaxrl_m.common import CodeTimer\n",
    "from jaxrl_m.wandb import setup_wandb, default_wandb_config, get_flag_dict\n",
    "from jaxrl_m.evaluation import supply_rng, evaluate, flatten, EpisodeMonitor\n",
    "from jaxrl_m.common import compute_dormant\n",
    "from jaxrl_m.dataset import ReplayBuffer\n",
    "from jaxrl_m.rollout import PolicyRollout,rollout_policy,rollout_policy2\n",
    "\n",
    "env = 'HalfCheetah'\n",
    "env_name=env+'-v4'\n",
    "seed=np.random.choice(1000000)\n",
    "eval_episodes=10\n",
    "batch_size = 256\n",
    "max_steps = int(1e6)\n",
    "start_steps = 50000                   \n",
    "log_interval = 10000\n",
    "\n",
    "wandb_config = default_wandb_config()   \n",
    "wandb_config.update({\n",
    "    'project': 'normalized',\n",
    "    'name': env+\"_\"+str(seed),\n",
    "    'hyperparam_dict':{\"name\":env},\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "env = EpisodeMonitor(gym.make(env_name))\n",
    "eval_env = EpisodeMonitor(gym.make(env_name))\n",
    "setup_wandb(**wandb_config)\n",
    "\n",
    "example_transition = dict(\n",
    "    observations=env.observation_space.sample(),\n",
    "    actions=env.action_space.sample(),\n",
    "    rewards=0.0,\n",
    "    masks=1.0,\n",
    "    next_observations=env.observation_space.sample(),\n",
    "    discounts=1.0,\n",
    ")   \n",
    "\n",
    "replay_buffer = ReplayBuffer.create(example_transition, size=int(1e6))\n",
    "actor_buffer = ReplayBuffer.create(example_transition, size=int(5e3))\n",
    "\n",
    "agent = create_learner(seed,\n",
    "                example_transition['observations'][None],\n",
    "                example_transition['actions'][None],\n",
    "                max_steps=max_steps,\n",
    "                )\n",
    "\n",
    "o_agent = create_learner(seed,\n",
    "                example_transition['observations'][None],\n",
    "                example_transition['actions'][None],\n",
    "                max_steps=max_steps,\n",
    "                )\n",
    "\n",
    "\n",
    "obs,info = env.reset()    \n",
    "exploration_rng = jax.random.PRNGKey(0)\n",
    "unlogged_steps,num_grad_updates,i = 0,0,0\n",
    "policy_rollouts = deque([], maxlen=MAX_SIZE)\n",
    "R2 = jnp.ones(NUM_CRITICS)\n",
    "\n",
    "warmup_steps = 0\n",
    "while warmup_steps < start_steps:\n",
    "    \n",
    "    replay_buffer,_,_,policy_return,undisc_policy_return,num_steps = rollout_policy(agent,env,exploration_rng,\n",
    "                        replay_buffer,actor_buffer,\n",
    "                        warmup=False,num_rollouts=NUM_ROLLOUTS)\n",
    "    warmup_steps += num_steps\n",
    "\n",
    "\n",
    "optimistic = False\n",
    "actor_old = agent.actor.params\n",
    "exploration_key = jax.random.PRNGKey(0)\n",
    "with tqdm.tqdm(total=max_steps) as pbar:\n",
    "    \n",
    "    while i < max_steps:\n",
    "\n",
    "            ### Rollout policy ###\n",
    "            num_rollouts =  NUM_ROLLOUTS\n",
    "            rollout_agent = o_agent if optimistic else agent\n",
    "            #print(f'rolling out with agent {'optimistic'*optimistic or 'conservative'*(not optimistic)}')\n",
    "            actor_buffer = ReplayBuffer.create(example_transition, size=int(NUM_ROLLOUTS*1000))\n",
    "            replay_buffer,actor_buffer,policy_rollout,policy_return,undisc_policy_return,num_steps = rollout_policy2(\n",
    "                                                                    rollout_agent,env,exploration_rng,\n",
    "                                                                    replay_buffer,actor_buffer,warmup=False,\n",
    "                                                                    num_rollouts=num_rollouts,random=None,\n",
    "                                                                    )\n",
    "            if i == 0 : policy_rollouts.append(policy_rollout) ###HOTFIX: For flattening rollouts\n",
    "            i+=num_steps\n",
    "            unlogged_steps += num_steps\n",
    "            pbar.update(num_steps)\n",
    "            policy_rollouts.append(policy_rollout)\n",
    "            \n",
    "            ### If optimistic rollout is better we update conservative actor###\n",
    "            if optimistic :\n",
    "                optimistic_return = policy_return\n",
    "                if policy_return > anc_rollout.policy_return :\n",
    "                    \n",
    "                    anc_rollout = policy_rollout\n",
    "                    agent = agent.replace(actor=o_agent.actor)\n",
    "                    conservative_buffer = actor_buffer\n",
    "                    \n",
    "                \n",
    "            ### If we rolled out conservative we update optimistic ###\n",
    "            elif not optimistic :\n",
    "                anc_rollout = policy_rollout\n",
    "                conservative_return = policy_return\n",
    "                o_agent = o_agent.replace(actor=agent.actor)\n",
    "                conservative_buffer = actor_buffer\n",
    "                \n",
    "            \n",
    "                \n",
    "            optimistic = not optimistic\n",
    "            \n",
    "            ### Update critics ###\n",
    "            transitions = replay_buffer.get_all()\n",
    "            tmp = partial(jax.random.choice,a=replay_buffer.size, shape=(10000,256), replace=True)\n",
    "            idxs = jax.vmap(tmp)(jax.random.split(exploration_key, NUM_CRITICS))\n",
    "            agent, critic_update_info = agent.update_many_critics(transitions,idxs,10000,R2)\n",
    "\n",
    "            ### Update critic weights ##                    \n",
    "            flattened_rollouts = flatten_rollouts(policy_rollouts)\n",
    "            R2,bias,_,_ = train_evaluation(agent,anc_rollout.policy_return,flattened_rollouts)\n",
    "        \n",
    "            if not optimistic : \n",
    "                \n",
    "                ### Update actor ###        \n",
    "                actor_batch = conservative_buffer.get_all() \n",
    "                agent, actor_update_info = agent.update_actor(actor_batch,R2.reshape(-1,1))   \n",
    "                \n",
    "                ### Divers ###\n",
    "                percent_dormant = compute_dormant(agent,policy_rollouts[-1].observations,0.2)\n",
    "                action_distance = measure_action_distance(agent,agent.actor.params,actor_old,policy_rollouts[-2].observations)\n",
    "                exploration_rng, key = jax.random.split(exploration_rng)\n",
    "                actor_old = agent.actor.params                    \n",
    "                num_grad_updates += 1 \n",
    "                \n",
    "            if optimistic :\n",
    "                \n",
    "                ### Update optimistic critics ###\n",
    "                transitions = replay_buffer.get_all()\n",
    "                tmp = partial(jax.random.choice,a=replay_buffer.size, shape=(10000,256), replace=True)\n",
    "                idxs = jax.vmap(tmp)(jax.random.split(exploration_key, NUM_CRITICS))                \n",
    "                o_agent, _ = o_agent.update_many_o_critics(agent.critic.params,transitions,idxs,10000,R2)\n",
    "                \n",
    "                ### Update optimistic critic weights ##\n",
    "                flattened_rollouts = flatten_rollouts(policy_rollouts)\n",
    "                R2_o,bias_o,_,_ = train_evaluation(o_agent,anc_rollout.policy_return,flattened_rollouts)\n",
    "                \n",
    "                ### Update optimistic actor ###\n",
    "                actor_batch = actor_buffer.get_all() \n",
    "                o_agent, _ = o_agent.update_actor(actor_batch,R2_o.reshape(-1,1))   \n",
    "\n",
    "\n",
    "            if unlogged_steps >= log_interval:\n",
    "\n",
    "                ### Log stuff to w&b ### \n",
    "                update_info = {**critic_update_info, **actor_update_info}\n",
    "                R2_train_info = {'max': jnp.max(R2),'bias': bias[jnp.argmax(R2)],\n",
    "                                    'max_optimistic': jnp.max(R2_o),'bias_optimistic': bias_o[jnp.argmax(R2_o)],\n",
    "                                    \"histogram\": wandb.Histogram(jnp.clip(R2,a_min=-1,a_max=1)),\n",
    "                                    \"histogram_optimistic\": wandb.Histogram(jnp.clip(R2_o,a_min=-1,a_max=1))}\n",
    "                R2_metrics = {f'R2/{k}': v for k, v in R2_train_info.items()}\n",
    "                exploration_metrics = {f'exploration/disc_return': policy_return,'exploration/conser_return': conservative_return,'exploration/optim_return': optimistic_return}\n",
    "                train_metrics = {f'training/{k}': v for k, v in update_info.items()}\n",
    "                \n",
    "                wandb.log(train_metrics, step=int(i),commit=False)\n",
    "                wandb.log({\"undisc_return\":undisc_policy_return,\"dormant\":percent_dormant,\"action_distance\":action_distance},step=int(i),commit=False)\n",
    "                wandb.log(R2_metrics, step=int(i),commit=False)\n",
    "                wandb.log(exploration_metrics, step=int(i),commit=True)\n",
    "                \n",
    "                unlogged_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def weighted_std(values, weights):\n",
    "    average = jnp.average(values, weights=weights,axis=0)\n",
    "    variance = jnp.average((values - average)**2, weights=weights,axis=0)\n",
    "    average2 = jnp.sum(weights*values,axis=0)\n",
    "    assert jnp.allclose(average,average2)\n",
    "    return jnp.sqrt(variance)\n",
    "\n",
    "# Example usage\n",
    "key = jax.random.PRNGKey(0)\n",
    "values = jax.random.normal(key,(5,100))\n",
    "weights = jnp.array([0.1, 0.2, 0.3, 0.2, 0.2])\n",
    "\n",
    "weighted_std_dev = weighted_std(values, weights.reshape(-1,1))\n",
    "print(\"Weighted Standard Deviation:\", weighted_std_dev)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
