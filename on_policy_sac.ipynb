{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdi/Desktop/jaxrl_m/.venv/lib/python3.10/site-packages/jax/_src/api_util.py:229: SyntaxWarning: Jitted function has invalid argnames {'num_batches'} in static_argnames. Function does not take these args.This warning will be replaced by an error after 2022-08-20 at the earliest.\n",
      "  warnings.warn(f\"Jitted function has invalid argnames {invalid_argnames} \"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Implementations of algorithms for continuous control.\"\"\"\n",
    "import functools\n",
    "from jaxrl_m.typing import *\n",
    "\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from jaxrl_m.common import TrainState, target_update, nonpytree_field\n",
    "from jaxrl_m.networks import Policy, Critic, ensemblize\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "class Temperature(nn.Module):\n",
    "    initial_temperature: float = 1.\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self) -> jnp.ndarray:\n",
    "        log_temp = self.param('log_temp',\n",
    "                              init_fn=lambda key: jnp.full(\n",
    "                                  (), jnp.log(self.initial_temperature)))\n",
    "        return jnp.exp(log_temp)\n",
    "\n",
    "class SACAgent(flax.struct.PyTreeNode):\n",
    "    rng: PRNGKey\n",
    "    critic: TrainState\n",
    "    target_critic: TrainState\n",
    "    actor: TrainState\n",
    "    temp: TrainState\n",
    "    config: dict = nonpytree_field()\n",
    "    \n",
    "    \n",
    "    @jax.jit    \n",
    "    def reset_critic_optimizer(agent):\n",
    "    \n",
    "        new_opt_state = agent.critic.tx.init(agent.critic.params)\n",
    "        new_critic = agent.critic.replace(opt_state=new_opt_state)\n",
    "        \n",
    "        return agent.replace(critic=new_critic)\n",
    "\n",
    "    \n",
    "    # @jax.jit\n",
    "    # def update_critic(agent, batch: Batch,num_update=0):\n",
    "    #     new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "\n",
    "     \n",
    "    #     def critic_loss_fn(critic_params):\n",
    "    #         next_dist = agent.actor(batch['next_observations'])\n",
    "    #         next_actions, next_log_probs = next_dist.sample_and_log_prob(seed=next_key)\n",
    "\n",
    "    #         next_q1, next_q2 = agent.target_critic(batch['next_observations'], next_actions,True,\n",
    "    #                                                params=None,rngs={'dropout': next_key})\n",
    "    #         next_q = jnp.minimum(next_q1, next_q2)\n",
    "    #         target_q = batch['rewards'] + agent.config['discount'] * batch['masks'] * next_q\n",
    "\n",
    "    #         if agent.config['backup_entropy']:\n",
    "    #             target_q = target_q - agent.config['discount'] * batch['masks'] * next_log_probs * agent.temp()\n",
    "            \n",
    "            \n",
    "    #         q1, q2 = agent.critic(batch['observations'], batch['actions'],True,\n",
    "    #                                             params=critic_params,rngs={'dropout': curr_key},\n",
    "    #                                             )\n",
    "    #         critic_loss = ((q1 - target_q)**2 + (q2 - target_q)**2).mean()\n",
    "            \n",
    "    #         return critic_loss, {\n",
    "    #             'critic_loss': critic_loss,\n",
    "    #             'q1': q1.mean(),\n",
    "    #         }     \n",
    "    \n",
    "\n",
    "        \n",
    "    #     new_critic, critic_info = agent.critic.apply_loss_fn(loss_fn=critic_loss_fn, has_aux=True)\n",
    "    #     new_target_critic = target_update(agent.critic, agent.target_critic, agent.config['target_update_rate'])\n",
    "    #     return agent.replace(rng=new_rng, critic=new_critic, target_critic=new_target_critic), {**critic_info}\n",
    "    \n",
    "    \n",
    "    #     # update = lambda target : target_update(new_critic,target,agent.config['target_update_rate'])\n",
    "    #     # no_update = lambda target: target\n",
    "    #     # new_target_critic = lax.cond(num_update%2==0,update,no_update,agent.target_critic)\n",
    "        \n",
    "        \n",
    "        \n",
    "    @partial(jax.jit,static_argnames=('num_batches',))  \n",
    "    def update_critic2(agent, transitions: Batch,idxs:jnp.array,num_steps:int):\n",
    "        \n",
    "        def one_update(agent, batch: Batch):\n",
    "                \n",
    "            new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "\n",
    "        \n",
    "            def critic_loss_fn(critic_params):\n",
    "                next_dist = agent.actor(batch['next_observations'])\n",
    "                next_actions, next_log_probs = next_dist.sample_and_log_prob(seed=next_key)\n",
    "\n",
    "                next_q1, next_q2 = agent.target_critic(batch['next_observations'], next_actions,True,\n",
    "                                                    params=None,rngs={'dropout': next_key})\n",
    "                next_q = jnp.minimum(next_q1, next_q2)\n",
    "                target_q = batch['rewards'] + agent.config['discount'] * batch['masks'] * next_q\n",
    "\n",
    "                if agent.config['backup_entropy']:\n",
    "                    target_q = target_q - agent.config['discount'] * batch['masks'] * next_log_probs * agent.temp()\n",
    "                \n",
    "                \n",
    "                q1, q2 = agent.critic(batch['observations'], batch['actions'],True,\n",
    "                                                    params=critic_params,rngs={'dropout': curr_key},\n",
    "                                                    )\n",
    "                critic_loss = ((q1 - target_q)**2 + (q2 - target_q)**2).mean()\n",
    "                \n",
    "                return critic_loss, {\n",
    "                    'critic_loss': critic_loss,\n",
    "                    'q1': q1.mean(),\n",
    "                }  \n",
    "            \n",
    "            new_critic, critic_info = agent.critic.apply_loss_fn(loss_fn=critic_loss_fn, has_aux=True)\n",
    "            new_target_critic = target_update(agent.critic, agent.target_critic, agent.config['target_update_rate'])\n",
    "            return agent.replace(rng=new_rng, critic=new_critic, target_critic=new_target_critic)\n",
    "        \n",
    "        \n",
    "        get_batch = lambda transitions,idx : jax.tree_map(lambda x : x[idx],transitions)\n",
    "        \n",
    "        agent = jax.lax.fori_loop(0, num_steps, \n",
    "                        lambda i, agent: one_update(agent,get_batch(transitions,idxs[i])),\n",
    "                        agent)\n",
    "        \n",
    "        return agent,{}\n",
    "    \n",
    "\n",
    "        \n",
    "    @jax.jit\n",
    "    def update_actor(agent, batch: Batch):\n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "\n",
    "        def actor_loss_fn(actor_params):\n",
    "            # dist = agent.actor(batch['observations'], params=actor_params)\n",
    "            # actions, log_probs = dist.sample_and_log_prob(seed=curr_key)\n",
    "            # q1, q2 = agent.critic(batch['observations'], actions)\n",
    "            \n",
    "            observations = jnp.repeat(batch['observations'],5,axis=0)\n",
    "            discounts = jnp.repeat(batch['discounts'],5,axis=0)\n",
    "            \n",
    "            dist = agent.actor(observations, params=actor_params)\n",
    "            actions, log_probs = dist.sample_and_log_prob(seed=curr_key)\n",
    "            q1, q2 = agent.critic(observations, actions)#params=agent.target_critic.params)\n",
    "            \n",
    "            #q = jnp.minimum(q1, q2)\n",
    "            q = (q1 + q2)/2\n",
    "\n",
    "            actor_loss = ((log_probs * agent.temp() - q)).mean()\n",
    "            #actor_loss = ((log_probs * agent.temp() - q)*discounts).mean()\n",
    "            return actor_loss, {\n",
    "                'actor_loss': actor_loss,\n",
    "                'entropy': -1 * log_probs.mean(),\n",
    "            }\n",
    "        \n",
    "        def temp_loss_fn(temp_params, entropy, target_entropy):\n",
    "            temperature = agent.temp(params=temp_params)\n",
    "            temp_loss = (temperature * (entropy - target_entropy)).mean()\n",
    "            return temp_loss, {\n",
    "                'temp_loss': temp_loss,\n",
    "                'temperature': temperature,\n",
    "            }\n",
    "\n",
    "        \n",
    "        new_actor, actor_info = agent.actor.apply_loss_fn(loss_fn=actor_loss_fn, has_aux=True)\n",
    "        temp_loss_fn = functools.partial(temp_loss_fn, entropy=actor_info['entropy'], target_entropy=agent.config['target_entropy'])\n",
    "        new_temp, temp_info = agent.temp.apply_loss_fn(loss_fn=temp_loss_fn, has_aux=True)\n",
    "\n",
    "\n",
    "        return agent.replace(rng=new_rng,actor=new_actor, temp=new_temp), {**actor_info, **temp_info}\n",
    "        \n",
    "\n",
    "\n",
    "    @jax.jit\n",
    "    def sample_actions(agent,   \n",
    "                       observations: np.ndarray,\n",
    "                       *,\n",
    "                       seed: PRNGKey,\n",
    "                       temperature: float = 1.0,\n",
    "                       ) -> jnp.ndarray:\n",
    "        actions = agent.actor(observations, temperature=temperature).sample(seed=seed)\n",
    "        #actions = jnp.clip(actions, -1, 1)\n",
    "        return actions\n",
    "\n",
    "\n",
    "\n",
    "def create_learner(\n",
    "                 seed: int,\n",
    "                 observations: jnp.ndarray,\n",
    "                 actions: jnp.ndarray,\n",
    "                 actor_lr: float = 3e-4,\n",
    "                 critic_lr: float = 3e-4,\n",
    "                 temp_lr: float = 3e-2,## Test\n",
    "                 hidden_dims: Sequence[int] = (256, 256),\n",
    "                 discount: float = 0.99,\n",
    "                 tau: float = 0.005,\n",
    "                 target_entropy: float = None,\n",
    "                 backup_entropy: bool = True,\n",
    "            **kwargs):\n",
    "\n",
    "        print('Extra kwargs:', kwargs)\n",
    "\n",
    "        rng = jax.random.PRNGKey(seed)\n",
    "        rng, actor_key, critic_key = jax.random.split(rng, 3)\n",
    "\n",
    "        action_dim = actions.shape[-1]\n",
    "        actor_def = Policy((64,64), action_dim=action_dim, \n",
    "            log_std_min=-10.0, state_dependent_std=True, tanh_squash_distribution=True, final_fc_init_scale=1.0)\n",
    "\n",
    "        actor_params = actor_def.init(actor_key, observations)['params']\n",
    "        actor = TrainState.create(actor_def, actor_params, tx=optax.adam(learning_rate=actor_lr))\n",
    "\n",
    "        critic_def = ensemblize(Critic, num_qs=2,split_rngs={\"dropout\":True})(hidden_dims)\n",
    "        \n",
    "        critic_params = critic_def.init(critic_key, observations, actions)['params']\n",
    "        critic = TrainState.create(critic_def, critic_params, tx=optax.adam(learning_rate=critic_lr))\n",
    "        target_critic = TrainState.create(critic_def, critic_params)\n",
    "\n",
    "        temp_def = Temperature()\n",
    "        temp_params = temp_def.init(rng)['params']\n",
    "        temp = TrainState.create(temp_def, temp_params, tx=optax.adam(learning_rate=temp_lr))\n",
    "\n",
    "        if target_entropy is None:\n",
    "            target_entropy = -0.5 * action_dim\n",
    "\n",
    "        config = flax.core.FrozenDict(dict(\n",
    "            discount=discount,\n",
    "            target_update_rate=tau,\n",
    "            target_entropy=target_entropy,\n",
    "            backup_entropy=backup_entropy,            \n",
    "        ))\n",
    "\n",
    "        return SACAgent(rng, critic=critic, target_critic=target_critic, actor=actor, temp=temp, config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxrl_m.dataset import ReplayBuffer\n",
    "from flax import struct\n",
    "import chex\n",
    "\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class PolicyRollout:\n",
    "    \n",
    "    policy_params : chex.Array\n",
    "    num_rollouts : chex.Array \n",
    "    policy_return : chex.Array\n",
    "    observations : chex.Array\n",
    "    disc_masks : chex.Array\n",
    "    \n",
    "\n",
    "def rollout_policy(agent,env,exploration_rng,\n",
    "                   replay_buffer,actor_buffer,\n",
    "                   warmup=False,num_rollouts=10,):\n",
    "    \n",
    "    \n",
    "    actor_buffer.reset()\n",
    "    obs,_ = env.reset()  \n",
    "    n_steps,n_rollouts,episode_step,disc,mask = 0,0,0,1.,1.\n",
    "    max_steps = num_rollouts*1000\n",
    "    observations,disc_masks,rewards = np.zeros((max_steps,obs.shape[0])),np.zeros((max_steps,)),np.zeros((max_steps,))\n",
    "    \n",
    "    \n",
    "    while n_rollouts < num_rollouts:\n",
    "        \n",
    "        if warmup:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            exploration_rng, key = jax.random.split(exploration_rng)\n",
    "            action = agent.sample_actions(obs, seed=key)\n",
    "        \n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "        #reward = reward/400\n",
    "        \n",
    "        mask = float(not done)\n",
    "    \n",
    "        transition = dict(observations=obs,actions=action,\n",
    "            rewards=reward,masks=mask,next_observations=next_obs,discounts=disc)\n",
    "        \n",
    "        \n",
    "        replay_buffer.add_transition(transition)\n",
    "        actor_buffer.add_transition(transition)\n",
    "    \n",
    "        observations[1000*n_rollouts+episode_step] = obs\n",
    "        disc_masks[1000*n_rollouts+episode_step] = disc\n",
    "        rewards[1000*n_rollouts+episode_step] = reward\n",
    "        \n",
    "        obs = next_obs\n",
    "        disc *= (0.99*mask)\n",
    "        episode_step += 1\n",
    "        n_steps += 1\n",
    "        \n",
    "        if (done or truncated) :\n",
    "            \n",
    "            #exploration_metrics = {f'exploration/{k}': v for k, v in flatten(info).items()}\n",
    "            obs,_= env.reset()\n",
    "            n_rollouts += 1\n",
    "            episode_step = 0\n",
    "            disc,mask = 1.,1.\n",
    "\n",
    "    policy_return = (disc_masks*rewards).sum()/num_rollouts\n",
    "    policy_rollout = PolicyRollout(policy_params=agent.actor.params,\n",
    "                                   policy_return=policy_return,\n",
    "                                   observations=observations,\n",
    "                                   disc_masks=disc_masks,\n",
    "                                    num_rollouts=num_rollouts)\n",
    "    \n",
    "    return replay_buffer,actor_buffer,policy_rollout,policy_return,n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def f(anc_agent,obs,actor):\n",
    "\n",
    "    dist = anc_agent.actor(obs, params=actor)\n",
    "    actions, _ = dist.sample_and_log_prob(seed=jax.random.PRNGKey(0))\n",
    "    q1, q2 = anc_agent.critic(obs, actions,params=anc_agent.target_critic.params)\n",
    "    #q = jnp.minimum(q1, q2)\n",
    "    q = (q1+q2)/2\n",
    "    \n",
    "    return q\n",
    "    \n",
    "\n",
    "@jax.jit\n",
    "def estimate_return(anc_agent,anc_return,acq_rollout:PolicyRollout,):\n",
    "    \n",
    "    # acq_obs = acq_rollout.observations\n",
    "    # acq_masks = acq_rollout.disc_masks\n",
    "    acq_obs = jnp.repeat(acq_rollout.observations,10,axis=0)\n",
    "    acq_masks = jnp.repeat(acq_rollout.disc_masks,10,axis=0)\n",
    "    \n",
    "    acq_actor = acq_rollout.policy_params\n",
    "    acq_return = acq_rollout.policy_return\n",
    "    \n",
    "    anc_actor = anc_agent.actor.params\n",
    "    \n",
    "    acq_q = f(anc_agent,acq_obs,acq_actor)\n",
    "    anc_q = f(anc_agent,acq_obs,anc_actor)\n",
    "    \n",
    "    adv = ((acq_q - anc_q)*acq_masks).sum()/10\n",
    "    acq_return_pred = anc_return + adv\n",
    "  \n",
    "    \n",
    "    return acq_return_pred,acq_return\n",
    "\n",
    "\n",
    "def evaluate_critic(anc_agent,anc_return,policy_rollouts):\n",
    "\n",
    "    y_pred,y= [],[]\n",
    "    for policy_rollout in policy_rollouts:\n",
    "        \n",
    "        acq_return_pred,acq_return = estimate_return(anc_agent,anc_return,policy_rollout)\n",
    "        y_pred.append(acq_return_pred),y.append(acq_return)\n",
    "        \n",
    "    y_pred,y = np.array(y_pred),np.array(y)\n",
    "    a2 = jnp.clip(((y-y_pred)**2),a_min=1e-4).sum()\n",
    "    b2=((y-y.mean())**2).sum()\n",
    "    R2 = 1-(a2/b2)  \n",
    "    \n",
    "    return R2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 00:17:52.257123: W external/xla/xla/service/gpu/nvptx_compiler.cc:679] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.103). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahdikallel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/tmpqaae71sn/wandb/run-20231215_001753-pae0obw2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mahdikallel/jaxrl_m/runs/pae0obw2' target=\"_blank\">earnest-night-72</a></strong> to <a href='https://wandb.ai/mahdikallel/jaxrl_m' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mahdikallel/jaxrl_m' target=\"_blank\">https://wandb.ai/mahdikallel/jaxrl_m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mahdikallel/jaxrl_m/runs/pae0obw2' target=\"_blank\">https://wandb.ai/mahdikallel/jaxrl_m/runs/pae0obw2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra kwargs: {'max_steps': 1000000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5100/1000000 [00:04<17:52, 928.05it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10251/1000000 [01:29<3:10:33, 86.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 15599/1000000 [02:13<1:57:19, 139.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 21052/1000000 [02:43<1:24:32, 192.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 26620/1000000 [03:06<1:06:27, 244.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 31792/1000000 [03:23<50:37, 318.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 36960/1000000 [03:36<39:31, 406.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 42455/1000000 [03:46<28:00, 569.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 47903/1000000 [03:54<24:00, 660.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 54379/1000000 [04:04<24:03, 655.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 61189/1000000 [04:14<22:55, 682.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 67757/1000000 [04:23<19:53, 780.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 73212/1000000 [04:29<18:41, 826.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 79319/1000000 [04:36<16:59, 902.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 86569/1000000 [04:45<17:51, 852.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 93081/1000000 [04:52<16:20, 924.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100416/1000000 [04:59<15:19, 978.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 108477/1000000 [05:07<14:24, 1031.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 115327/1000000 [05:14<14:54, 988.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 121625/1000000 [05:21<14:57, 978.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 131308/1000000 [05:29<13:05, 1105.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 140467/1000000 [05:38<13:13, 1083.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 145682/1000000 [05:44<13:43, 1037.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 151202/1000000 [05:50<14:00, 1010.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 158054/1000000 [05:56<13:50, 1013.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 164027/1000000 [06:03<14:07, 986.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 173708/1000000 [06:11<12:49, 1073.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 180098/1000000 [06:18<13:21, 1023.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 188796/1000000 [06:25<12:35, 1074.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 195676/1000000 [06:32<12:49, 1045.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 203305/1000000 [06:40<12:37, 1052.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 212844/1000000 [06:47<11:43, 1119.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 222566/1000000 [06:55<11:03, 1171.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 232330/1000000 [07:02<10:35, 1208.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 241318/1000000 [07:10<10:28, 1206.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 249262/1000000 [07:17<10:38, 1176.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 258255/1000000 [07:24<10:21, 1194.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 268130/1000000 [07:32<10:00, 1219.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 277866/1000000 [07:40<09:46, 1232.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 287608/1000000 [07:47<09:33, 1242.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 297351/1000000 [07:55<09:21, 1251.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 307104/1000000 [08:03<09:14, 1250.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 317104/1000000 [08:11<09:02, 1259.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 327104/1000000 [08:18<08:45, 1280.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 337104/1000000 [08:26<08:39, 1276.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 347104/1000000 [08:34<08:30, 1279.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 357104/1000000 [08:41<08:13, 1302.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 357104/1000000 [08:47<15:50, 676.52it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mahdi/Desktop/jaxrl_m/on_policy_droq.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/on_policy_droq.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mwith\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(total\u001b[39m=\u001b[39mmax_steps) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/on_policy_droq.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m i \u001b[39m<\u001b[39m max_steps:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/on_policy_droq.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m         replay_buffer,actor_buffer,policy_rollout,policy_return,num_steps \u001b[39m=\u001b[39m rollout_policy(agent,env,exploration_rng,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/on_policy_droq.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m                    replay_buffer,actor_buffer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/on_policy_droq.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m                    warmup\u001b[39m=\u001b[39;49m(i \u001b[39m<\u001b[39;49m start_steps))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/on_policy_droq.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m         policy_rollouts\u001b[39m.\u001b[39mappend(policy_rollout)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/on_policy_droq.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m         unlogged_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m num_steps\n",
      "\u001b[1;32m/home/mahdi/Desktop/jaxrl_m/on_policy_droq.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/on_policy_droq.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/on_policy_droq.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     exploration_rng, key \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(exploration_rng)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/on_policy_droq.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49msample_actions(obs, seed\u001b[39m=\u001b[39;49mkey)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/on_policy_droq.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m next_obs, reward, done, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btesla.la1.uni-wuerzburg.de/home/mahdi/Desktop/jaxrl_m/on_policy_droq.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m#reward = reward/400\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/jaxrl_m/.venv/lib/python3.10/site-packages/jax/_src/tree_util.py:699\u001b[0m, in \u001b[0;36mregister_pytree_with_keys.<locals>.flatten_func_impl\u001b[0;34m(tree)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Extends the set of types that are considered internal nodes in pytrees.\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \n\u001b[1;32m    679\u001b[0m \u001b[39mThis is a more powerful alternative to ``register_pytree_node`` that allows\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39m    calling functions without keys like ``tree_map`` and ``tree_flatten``.\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m flatten_func:\n\u001b[0;32m--> 699\u001b[0m   \u001b[39mdef\u001b[39;00m \u001b[39mflatten_func_impl\u001b[39m(tree):\n\u001b[1;32m    700\u001b[0m     key_children, treedef \u001b[39m=\u001b[39m flatten_with_keys(tree)\n\u001b[1;32m    701\u001b[0m     \u001b[39mreturn\u001b[39;00m [c \u001b[39mfor\u001b[39;00m _, c \u001b[39min\u001b[39;00m key_children], treedef\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import jax\n",
    "import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "from jaxrl_m.wandb import setup_wandb, default_wandb_config, get_flag_dict\n",
    "import wandb\n",
    "from jaxrl_m.evaluation import supply_rng, evaluate, flatten, EpisodeMonitor\n",
    "from jaxrl_m.dataset import ReplayBuffer\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "env_name='InvertedPendulum-v4'\n",
    "seed=np.random.choice(1000000)\n",
    "eval_episodes=10\n",
    "batch_size = 256\n",
    "max_steps = int(1e6)\n",
    "start_steps = 5000                   \n",
    "log_interval = 5000\n",
    "#eval_interval = 10000\n",
    "\n",
    "wandb_config = default_wandb_config()\n",
    "wandb_config.update({\n",
    "    'project': 'd4rl_test',\n",
    "    'group': 'sac_test',\n",
    "    'name': 'sac_{env_name}',\n",
    "})\n",
    "\n",
    "\n",
    "env = EpisodeMonitor(gym.make(env_name))\n",
    "eval_env = EpisodeMonitor(gym.make(env_name))\n",
    "setup_wandb({\"bonjour\":1})\n",
    "\n",
    "example_transition = dict(\n",
    "    observations=env.observation_space.sample(),\n",
    "    actions=env.action_space.sample(),\n",
    "    rewards=0.0,\n",
    "    masks=1.0,\n",
    "    next_observations=env.observation_space.sample(),\n",
    "    discounts=1.0,\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer.create(example_transition, size=int(1e6))\n",
    "actor_buffer = ReplayBuffer.create(example_transition, size=int(5e3))\n",
    "\n",
    "agent = create_learner(seed,\n",
    "                example_transition['observations'][None],\n",
    "                example_transition['actions'][None],\n",
    "                max_steps=max_steps,\n",
    "                #**FLAGS.config\n",
    "                )\n",
    "\n",
    "exploration_metrics = dict()\n",
    "obs,info = env.reset()    \n",
    "exploration_rng = jax.random.PRNGKey(0)\n",
    "i = 0\n",
    "unlogged_steps = 0\n",
    "policy_rollouts = deque([], maxlen=10)\n",
    "with tqdm.tqdm(total=max_steps) as pbar:\n",
    "    \n",
    "    while i < max_steps:\n",
    "    \n",
    "        replay_buffer,actor_buffer,policy_rollout,policy_return,num_steps = rollout_policy(agent,env,exploration_rng,\n",
    "                   replay_buffer,actor_buffer,\n",
    "                   warmup=(i < start_steps))\n",
    "        policy_rollouts.append(policy_rollout)\n",
    "        unlogged_steps += num_steps\n",
    "        i+=num_steps\n",
    "        pbar.update(num_steps)\n",
    "        \n",
    "            \n",
    "        if replay_buffer.size > start_steps:\n",
    "        \n",
    "         \n",
    "            transitions = replay_buffer.get_all()\n",
    "            idxs = jax.random.choice(a=replay_buffer.size, shape=(10000,256), replace=True,key=jax.random.PRNGKey(0))\n",
    "            agent.reset_critic_optimizer()\n",
    "            agent, critic_update_info = agent.update_critic2(transitions,idxs,10000)\n",
    "            R2 = evaluate_critic(agent,policy_rollouts[-1].policy_return,policy_rollouts)\n",
    "\n",
    "            \n",
    "            actor_batch = actor_buffer.get_all()      \n",
    "            agent, actor_update_info = agent.update_actor(actor_batch)    \n",
    "            \n",
    "            \n",
    "            update_info = {**critic_update_info, **actor_update_info, 'R2_validation': R2}\n",
    "            \n",
    "            \n",
    "            if unlogged_steps > log_interval:\n",
    "                exploration_metrics = {f'exploration/disc_return': policy_return}\n",
    "                wandb.log(exploration_metrics, step=int(i),commit=False)\n",
    "                train_metrics = {f'training/{k}': v for k, v in update_info.items()}\n",
    "                wandb.log(train_metrics, step=int(i),commit=False)\n",
    "                #wandb.log(exploration_metrics, step=i)\n",
    "                policy_fn = partial(supply_rng(agent.sample_actions), temperature=0.0)\n",
    "                eval_info = evaluate(policy_fn, eval_env, num_episodes=eval_episodes)\n",
    "                eval_metrics = {f'evaluation/{k}': v for k, v in eval_info.items()}\n",
    "                print('evaluating')\n",
    "                wandb.log(eval_metrics, step=int(i),commit=True)\n",
    "                unlogged_steps = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt_state = tx.init(params)\n",
    "\n",
    "#dir(agent.critic)\n",
    "new_opt_state = agent.critic.tx.init(agent.critic.params)\n",
    "new_critic = agent.critic.replace(opt_state=new_opt_state)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
