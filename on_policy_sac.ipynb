{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdi/Desktop/supersac/.venv/lib/python3.10/site-packages/jax/_src/api_util.py:229: SyntaxWarning: Jitted function has invalid argnames {'num_batches'} in static_argnames. Function does not take these args.This warning will be replaced by an error after 2022-08-20 at the earliest.\n",
      "  warnings.warn(f\"Jitted function has invalid argnames {invalid_argnames} \"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Implementations of algorithms for continuous control.\"\"\"\n",
    "import functools\n",
    "from jaxrl_m.typing import *\n",
    "\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from jaxrl_m.common import TrainState, target_update, nonpytree_field\n",
    "from jaxrl_m.networks import Policy, Critic, ensemblize\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from functools import partial\n",
    "\n",
    "NUM_ROLLOUTS = 5\n",
    "\n",
    "class Temperature(nn.Module):\n",
    "    initial_temperature: float = 0.01\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self) -> jnp.ndarray:\n",
    "        log_temp = self.param('log_temp',\n",
    "                              init_fn=lambda key: jnp.full(\n",
    "                                  (), jnp.log(self.initial_temperature)))\n",
    "        return jnp.exp(log_temp)\n",
    "\n",
    "class SACAgent(flax.struct.PyTreeNode):\n",
    "    rng: PRNGKey\n",
    "    critic: TrainState\n",
    "    target_critic: TrainState\n",
    "    actor: TrainState\n",
    "    temp: TrainState\n",
    "    config: dict = nonpytree_field()\n",
    "    \n",
    "    \n",
    "    @jax.jit    \n",
    "    def reset_critic_optimizer(agent):\n",
    "    \n",
    "        new_opt_state = agent.critic.tx.init(agent.critic.params)\n",
    "        new_critic = agent.critic.replace(opt_state=new_opt_state)\n",
    "        \n",
    "        return agent.replace(critic=new_critic)\n",
    "\n",
    "        \n",
    "        \n",
    "    @partial(jax.jit,static_argnames=('num_batches',))  \n",
    "    def update_critic2(agent, transitions: Batch,idxs:jnp.array,num_steps:int):\n",
    "        \n",
    "        def one_update(agent, batch: Batch):\n",
    "                \n",
    "            new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "\n",
    "        \n",
    "            # def critic_loss_fn(critic_params):\n",
    "            #     next_dist = agent.actor(batch['next_observations'])\n",
    "            #     next_actions, next_log_probs = next_dist.sample_and_log_prob(seed=next_key)\n",
    "\n",
    "            #     next_q1, next_q2 = agent.target_critic(batch['next_observations'], next_actions,True,\n",
    "            #                                         params=None,rngs={'dropout': next_key})\n",
    "            #     next_q = jnp.minimum(next_q1, next_q2)\n",
    "            #     target_q = batch['rewards'] + agent.config['discount'] * batch['masks'] * next_q\n",
    "\n",
    "            #     if agent.config['backup_entropy']:\n",
    "            #         target_q = target_q - agent.config['discount'] * batch['masks'] * next_log_probs * agent.temp()\n",
    "                \n",
    "                \n",
    "            #     q1, q2 = agent.critic(batch['observations'], batch['actions'],True,\n",
    "            #                                         params=critic_params,rngs={'dropout': curr_key},\n",
    "            #                                         )\n",
    "            #     critic_loss = ((q1 - target_q)**2 + (q2 - target_q)**2).mean()\n",
    "                \n",
    "            #     return critic_loss, {\n",
    "            #         'critic_loss': critic_loss,\n",
    "            #         'q1': q1.mean(),\n",
    "            #     }  \n",
    "            \n",
    "            def critic_loss_fn(critic_params):\n",
    "                    \n",
    "                    \n",
    "                    next_dist = agent.actor(batch['next_observations'])\n",
    "                    next_actions, next_log_probs = next_dist.sample_and_log_prob(seed=next_key)\n",
    "\n",
    "                    concat_actions = jnp.concatenate([batch[\"actions\"],next_actions])\n",
    "                    concat_observations = jnp.concatenate([batch[\"observations\"],batch[\"next_observations\"]])\n",
    "                    \n",
    "                    concat_q = agent.critic(concat_observations, concat_actions,\n",
    "                                            True,params=critic_params)\n",
    "                    q,next_q = jnp.split(concat_q,2,axis=0) ## axis=1 for ensemble\n",
    "                    target_q = batch['rewards'] + agent.config['discount'] * batch['masks'] * next_q\n",
    "                    \n",
    "                    # if agent.config['backup_entropy']:\n",
    "                    #     target_q = target_q - agent.config['discount'] * batch['masks'] * next_log_probs * agent.temp()\n",
    "                    target_q = jax.lax.stop_gradient(target_q)\n",
    "                    critic_loss = ((target_q-q)**2).mean()\n",
    "                    \n",
    "                    return critic_loss, {\n",
    "                    'critic_loss': critic_loss,\n",
    "                    'q1': q.mean(),\n",
    "                }  \n",
    "            \n",
    "            new_critic, critic_info = agent.critic.apply_loss_fn(loss_fn=critic_loss_fn, has_aux=True)\n",
    "            new_target_critic = target_update(agent.critic, agent.target_critic, agent.config['target_update_rate'])\n",
    "            return agent.replace(rng=new_rng, critic=new_critic, target_critic=new_target_critic)\n",
    "        \n",
    "        \n",
    "        get_batch = lambda transitions,idx : jax.tree_map(lambda x : x[idx],transitions)\n",
    "        \n",
    "        agent = jax.lax.fori_loop(0, num_steps, \n",
    "                        lambda i, agent: one_update(agent,get_batch(transitions,idxs[i])),\n",
    "                        agent)\n",
    "        \n",
    "        return agent,{}\n",
    "    \n",
    "\n",
    "        \n",
    "    @jax.jit\n",
    "    def update_actor(agent, batch: Batch):\n",
    "        new_rng, curr_key, next_key = jax.random.split(agent.rng, 3)\n",
    "\n",
    "        def actor_loss_fn(actor_params):\n",
    "            observations = jnp.repeat(batch['observations'], 5, axis=0)\n",
    "            discounts = jnp.repeat(batch['discounts'], 5, axis=0)\n",
    "            masks = jnp.int32(jnp.repeat(batch['masks'], 5, axis=0))\n",
    "\n",
    "            dist = agent.actor(observations, params=actor_params)\n",
    "            actions, log_probs = dist.sample_and_log_prob(seed=curr_key)\n",
    "            q = agent.critic(observations, actions)\n",
    "\n",
    "            actor_loss = ((log_probs * agent.temp() - q)).mean()\n",
    "            #actor_loss = (discounts*(log_probs * 0.01 - q)).mean()\n",
    "            return actor_loss, {\n",
    "                'actor_loss': actor_loss,\n",
    "                'entropy': -1 * log_probs.mean(),\n",
    "                #'entropy2': -1*dist.entropy().mean(),\n",
    "            }\n",
    "        \n",
    "        def temp_loss_fn(temp_params, entropy, target_entropy):\n",
    "            temperature = agent.temp(params=temp_params)\n",
    "            temp_loss = (temperature * (entropy - target_entropy)).mean()\n",
    "            return temp_loss, {\n",
    "                'temp_loss': temp_loss,\n",
    "                'temperature': temperature,\n",
    "            }\n",
    "\n",
    "        \n",
    "        new_actor, actor_info = agent.actor.apply_loss_fn(loss_fn=actor_loss_fn, has_aux=True)\n",
    "        temp_loss_fn = functools.partial(temp_loss_fn, entropy=actor_info['entropy'], target_entropy=agent.config['target_entropy'])\n",
    "        new_temp, temp_info = agent.temp.apply_loss_fn(loss_fn=temp_loss_fn, has_aux=True)\n",
    "        new_temp.params[\"log_temp\"] = jnp.clip(new_temp.params[\"log_temp\"],jnp.log(0.001),jnp.log(1000.0))\n",
    "\n",
    "        return agent.replace(rng=new_rng, actor=new_actor, temp=new_temp), {**actor_info, **temp_info}\n",
    "        \n",
    "\n",
    "    @jax.jit\n",
    "    def sample_actions(agent,   \n",
    "                       observations: np.ndarray,\n",
    "                       *,\n",
    "                       seed: PRNGKey,\n",
    "                       temperature: float = 1.0,\n",
    "                       ) -> jnp.ndarray:\n",
    "        actions = agent.actor(observations, temperature=temperature).sample(seed=seed)\n",
    "        #actions = jnp.clip(actions, -1, 1)\n",
    "        return actions\n",
    "\n",
    "\n",
    "\n",
    "def create_learner(\n",
    "                 seed: int,\n",
    "                 observations: jnp.ndarray,\n",
    "                 actions: jnp.ndarray,\n",
    "                 actor_lr: float = 3e-4,\n",
    "                 critic_lr: float = 3e-4,\n",
    "                 temp_lr: float =3e-1,## Test\n",
    "                 hidden_dims: Sequence[int] = (256, 256),\n",
    "                 discount: float = 0.99,\n",
    "                 tau: float = 0.005,\n",
    "                 target_entropy: float = None,\n",
    "                 backup_entropy: bool = True,\n",
    "            **kwargs):\n",
    "\n",
    "        print('Extra kwargs:', kwargs)\n",
    "\n",
    "        rng = jax.random.PRNGKey(seed)\n",
    "        rng, actor_key, critic_key = jax.random.split(rng, 3)\n",
    "\n",
    "        action_dim = actions.shape[-1]\n",
    "        actor_def = Policy((256,256), action_dim=action_dim, \n",
    "            log_std_min=-10.0, state_dependent_std=True, tanh_squash_distribution=True, final_fc_init_scale=1.0)\n",
    "\n",
    "        actor_params = actor_def.init(actor_key, observations)['params']\n",
    "        actor = TrainState.create(actor_def, actor_params, tx=optax.adam(learning_rate=actor_lr))\n",
    "\n",
    "        #critic_def = ensemblize(Critic, num_qs=2,split_rngs={\"dropout\":True})(hidden_dims)\n",
    "        critic_def = Critic(hidden_dims)\n",
    "        critic_params = critic_def.init(critic_key, observations, actions)['params']\n",
    "        critic = TrainState.create(critic_def, critic_params, tx=optax.adam(learning_rate=critic_lr))\n",
    "        target_critic = TrainState.create(critic_def, critic_params)\n",
    "\n",
    "        temp_def = Temperature()\n",
    "        temp_params = temp_def.init(rng)['params']\n",
    "        temp = TrainState.create(temp_def, temp_params, tx=optax.sgd(learning_rate=temp_lr))\n",
    "\n",
    "        if target_entropy is None:\n",
    "            target_entropy = -0.5 * action_dim\n",
    "\n",
    "        config = flax.core.FrozenDict(dict(\n",
    "            discount=discount,\n",
    "            target_update_rate=tau,\n",
    "            target_entropy=target_entropy,\n",
    "            backup_entropy=backup_entropy,            \n",
    "        ))\n",
    "\n",
    "        return SACAgent(rng, critic=critic, target_critic=target_critic, actor=actor, temp=temp, config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxrl_m.dataset import ReplayBuffer\n",
    "from flax import struct\n",
    "import chex\n",
    "\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class PolicyRollout:\n",
    "    \n",
    "    policy_params : chex.Array\n",
    "    num_rollouts : chex.Array \n",
    "    policy_return : chex.Array\n",
    "    observations : chex.Array\n",
    "    disc_masks : chex.Array\n",
    "    \n",
    "\n",
    "def rollout_policy(agent,env,exploration_rng,\n",
    "                   replay_buffer,actor_buffer,\n",
    "                   warmup=False,num_rollouts=NUM_ROLLOUTS,):\n",
    "    \n",
    "    \n",
    "    actor_buffer.reset()\n",
    "    obs,_ = env.reset()  \n",
    "    n_steps,n_rollouts,episode_step,disc,mask = 0,0,0,1.,1.\n",
    "    max_steps = num_rollouts*1000\n",
    "    observations,disc_masks,rewards = np.zeros((max_steps,obs.shape[0])),np.zeros((max_steps,)),np.zeros((max_steps,))\n",
    "    \n",
    "    \n",
    "    while n_rollouts < num_rollouts:\n",
    "        \n",
    "        if warmup:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            exploration_rng, key = jax.random.split(exploration_rng)\n",
    "            action = agent.sample_actions(obs, seed=key)\n",
    "        \n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "        #reward = reward/400\n",
    "        \n",
    "        mask = float(not done)\n",
    "    \n",
    "        transition = dict(observations=obs,actions=action,\n",
    "            rewards=reward,masks=mask,next_observations=next_obs,discounts=disc)\n",
    "        \n",
    "        \n",
    "        replay_buffer.add_transition(transition)\n",
    "        actor_buffer.add_transition(transition)\n",
    "    \n",
    "        observations[1000*n_rollouts+episode_step] = obs\n",
    "        disc_masks[1000*n_rollouts+episode_step] = disc\n",
    "        rewards[1000*n_rollouts+episode_step] = reward\n",
    "        \n",
    "        obs = next_obs\n",
    "        disc *= (0.99*mask)\n",
    "        episode_step += 1\n",
    "        n_steps += 1\n",
    "        \n",
    "        if (done or truncated) :\n",
    "            \n",
    "            #exploration_metrics = {f'exploration/{k}': v for k, v in flatten(info).items()}\n",
    "            obs,_= env.reset()\n",
    "            n_rollouts += 1\n",
    "            episode_step = 0\n",
    "            disc,mask = 1.,1.\n",
    "\n",
    "    policy_return = (disc_masks*rewards).sum()/num_rollouts\n",
    "    policy_rollout = PolicyRollout(policy_params=agent.actor.params,\n",
    "                                   policy_return=policy_return,\n",
    "                                   observations=observations,\n",
    "                                   disc_masks=disc_masks,\n",
    "                                    num_rollouts=num_rollouts)\n",
    "    \n",
    "    return replay_buffer,actor_buffer,policy_rollout,policy_return,n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def f(anc_agent,obs,actor):\n",
    "\n",
    "    dist = anc_agent.actor(obs, params=actor)\n",
    "    actions, _ = dist.sample_and_log_prob(seed=jax.random.PRNGKey(0))\n",
    "    q = anc_agent.critic(obs, actions,params=anc_agent.target_critic.params)\n",
    "    \n",
    "    return q\n",
    "    \n",
    "\n",
    "@jax.jit\n",
    "def estimate_return(anc_agent,anc_return,acq_rollout:PolicyRollout,):\n",
    "    \n",
    "    # acq_obs = acq_rollout.observations\n",
    "    # acq_masks = acq_rollout.disc_masks\n",
    "    acq_obs = jnp.repeat(acq_rollout.observations,10,axis=0)\n",
    "    acq_masks = jnp.repeat(acq_rollout.disc_masks,10,axis=0)\n",
    "    \n",
    "    acq_actor = acq_rollout.policy_params\n",
    "    acq_return = acq_rollout.policy_return\n",
    "    \n",
    "    anc_actor = anc_agent.actor.params\n",
    "    \n",
    "    acq_q = f(anc_agent,acq_obs,acq_actor)\n",
    "    anc_q = f(anc_agent,acq_obs,anc_actor)\n",
    "    \n",
    "    adv = ((acq_q - anc_q)*acq_masks).sum()/(NUM_ROLLOUTS *10)\n",
    "    acq_return_pred = anc_return + adv\n",
    "  \n",
    "    \n",
    "    return acq_return_pred,acq_return\n",
    "\n",
    "\n",
    "def evaluate_critic(anc_agent,anc_return,policy_rollouts):\n",
    "\n",
    "    y_pred,y= [],[]\n",
    "    for policy_rollout in policy_rollouts:\n",
    "        \n",
    "        acq_return_pred,acq_return = estimate_return(anc_agent,anc_return,policy_rollout)\n",
    "        y_pred.append(acq_return_pred),y.append(acq_return)\n",
    "        \n",
    "    y_pred,y = np.array(y_pred),np.array(y)\n",
    "    a2 = jnp.clip(((y-y_pred)**2),a_min=1e-4).sum()\n",
    "    b2=((y-y.mean())**2).sum()\n",
    "    R2 = 1-(a2/b2)  \n",
    "    \n",
    "    return R2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 23:14:30.982513: W external/xla/xla/service/gpu/nvptx_compiler.cc:679] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.103). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahdikallel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/tmpo6gh59ln/wandb/run-20240121_231431-of9hr78h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mahdikallel/jaxrl_m/runs/of9hr78h' target=\"_blank\">cerulean-deluge-188</a></strong> to <a href='https://wandb.ai/mahdikallel/jaxrl_m' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mahdikallel/jaxrl_m' target=\"_blank\">https://wandb.ai/mahdikallel/jaxrl_m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mahdikallel/jaxrl_m/runs/of9hr78h' target=\"_blank\">https://wandb.ai/mahdikallel/jaxrl_m/runs/of9hr78h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra kwargs: {'max_steps': 1000000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5052/1000000 [00:03<11:47, 1406.02it/s] \n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "`entropy` is not implemented for this transformed distribution, because its bijector's Jacobian determinant is not known to be constant.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 89\u001b[0m\n\u001b[1;32m     85\u001b[0m R2 \u001b[38;5;241m=\u001b[39m evaluate_critic(agent,policy_rollouts[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpolicy_return,policy_rollouts)\n\u001b[1;32m     88\u001b[0m actor_batch \u001b[38;5;241m=\u001b[39m actor_buffer\u001b[38;5;241m.\u001b[39mget_all()      \n\u001b[0;32m---> 89\u001b[0m agent, actor_update_info \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactor_batch\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m#print(f'actor_update_info {actor_update_info}')\u001b[39;00m\n\u001b[1;32m     92\u001b[0m update_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcritic_update_info, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mactor_update_info, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2_validation\u001b[39m\u001b[38;5;124m'\u001b[39m: R2}\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 148\u001b[0m, in \u001b[0;36mSACAgent.update_actor\u001b[0;34m(agent, batch)\u001b[0m\n\u001b[1;32m    141\u001b[0m     temp_loss \u001b[38;5;241m=\u001b[39m (temperature \u001b[38;5;241m*\u001b[39m (entropy \u001b[38;5;241m-\u001b[39m target_entropy))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m temp_loss, {\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: temp_loss,\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m: temperature,\n\u001b[1;32m    145\u001b[0m     }\n\u001b[0;32m--> 148\u001b[0m new_actor, actor_info \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactor_loss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m temp_loss_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(temp_loss_fn, entropy\u001b[38;5;241m=\u001b[39mactor_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m], target_entropy\u001b[38;5;241m=\u001b[39magent\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_entropy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    150\u001b[0m new_temp, temp_info \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtemp\u001b[38;5;241m.\u001b[39mapply_loss_fn(loss_fn\u001b[38;5;241m=\u001b[39mtemp_loss_fn, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/supersac/jaxrl_m/common.py:148\u001b[0m, in \u001b[0;36mTrainState.apply_loss_fn\u001b[0;34m(self, loss_fn, pmap_axis, has_aux)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03mTakes a gradient step towards minimizing `loss_fn`. Internally, this calls\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m`jax.grad` followed by `TrainState.apply_gradients`. If pmap_axis is provided,\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03madditionally it averages gradients (and info) across devices before performing update.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[0;32m--> 148\u001b[0m     grads, info \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pmap_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         grads \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mpmean(grads, axis_name\u001b[38;5;241m=\u001b[39mpmap_axis)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 136\u001b[0m, in \u001b[0;36mSACAgent.update_actor.<locals>.actor_loss_fn\u001b[0;34m(actor_params)\u001b[0m\n\u001b[1;32m    131\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m ((log_probs \u001b[38;5;241m*\u001b[39m agent\u001b[38;5;241m.\u001b[39mtemp() \u001b[38;5;241m-\u001b[39m q))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#actor_loss = (discounts*(log_probs * 0.01 - q)).mean()\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m actor_loss, {\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: actor_loss,\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m log_probs\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[1;32m    137\u001b[0m }\n",
      "File \u001b[0;32m~/Desktop/supersac/.venv/lib/python3.10/site-packages/distrax/_src/distributions/transformed.py:240\u001b[0m, in \u001b[0;36mTransformed.entropy\u001b[0;34m(self, input_hint)\u001b[0m\n\u001b[1;32m    238\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m entropy \u001b[38;5;241m+\u001b[39m fldj\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    241\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`entropy` is not implemented for this transformed distribution, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbecause its bijector\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Jacobian determinant is not known to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: `entropy` is not implemented for this transformed distribution, because its bijector's Jacobian determinant is not known to be constant."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import jax\n",
    "import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "from jaxrl_m.wandb import setup_wandb, default_wandb_config, get_flag_dict\n",
    "import wandb\n",
    "from jaxrl_m.evaluation import supply_rng, evaluate, flatten, EpisodeMonitor\n",
    "from jaxrl_m.dataset import ReplayBuffer\n",
    "from collections import deque\n",
    "from jax import config\n",
    "# config.update(\"jax_debug_nans\", True)\n",
    "# config.update(\"jax_enable_x64\", True)\n",
    "        \n",
    "\n",
    "env_name='Hopper-v4'\n",
    "seed=np.random.choice(1000000)\n",
    "eval_episodes=10\n",
    "batch_size = 256\n",
    "max_steps = int(1e6)\n",
    "start_steps = 5000                   \n",
    "log_interval = 5000\n",
    "#eval_interval = 10000\n",
    "\n",
    "wandb_config = default_wandb_config()\n",
    "wandb_config.update({\n",
    "    'project': 'on_policy_sac',\n",
    "    'group': 'sac_test',\n",
    "    'name': 'sac_{env_name}',\n",
    "})\n",
    "\n",
    "\n",
    "env = EpisodeMonitor(gym.make(env_name))\n",
    "eval_env = EpisodeMonitor(gym.make(env_name))\n",
    "setup_wandb({\"bonjour\":1})\n",
    "\n",
    "example_transition = dict(\n",
    "    observations=env.observation_space.sample(),\n",
    "    actions=env.action_space.sample(),\n",
    "    rewards=0.0,\n",
    "    masks=1.0,\n",
    "    next_observations=env.observation_space.sample(),\n",
    "    discounts=1.0,\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer.create(example_transition, size=int(1e6))\n",
    "actor_buffer = ReplayBuffer.create(example_transition, size=int(10e3))\n",
    "\n",
    "agent = create_learner(seed,\n",
    "                example_transition['observations'][None],\n",
    "                example_transition['actions'][None],\n",
    "                max_steps=max_steps,\n",
    "                #**FLAGS.config\n",
    "                )\n",
    "\n",
    "exploration_metrics = dict()\n",
    "obs,info = env.reset()    \n",
    "exploration_rng = jax.random.PRNGKey(0)\n",
    "i = 0\n",
    "unlogged_steps = 0\n",
    "policy_rollouts = deque([], maxlen=10)\n",
    "with tqdm.tqdm(total=max_steps) as pbar:\n",
    "    \n",
    "    while i < max_steps:\n",
    "    \n",
    "        replay_buffer,actor_buffer,policy_rollout,policy_return,num_steps = rollout_policy(agent,env,exploration_rng,\n",
    "                   replay_buffer,actor_buffer,\n",
    "                   warmup=(i < start_steps))\n",
    "        policy_rollouts.append(policy_rollout)\n",
    "        unlogged_steps += num_steps\n",
    "        i+=num_steps\n",
    "        pbar.update(num_steps)\n",
    "        \n",
    "            \n",
    "        if replay_buffer.size > start_steps:\n",
    "        \n",
    "         \n",
    "            transitions = replay_buffer.get_all()\n",
    "            idxs = jax.random.choice(a=replay_buffer.size, shape=(10000,256), replace=True,key=jax.random.PRNGKey(0))\n",
    "            agent.reset_critic_optimizer()\n",
    "            agent, critic_update_info = agent.update_critic2(transitions,idxs,10000)\n",
    "            R2 = evaluate_critic(agent,policy_rollouts[-1].policy_return,policy_rollouts)\n",
    "\n",
    "            \n",
    "            actor_batch = actor_buffer.get_all()      \n",
    "            agent, actor_update_info = agent.update_actor(actor_batch)    \n",
    "            #print(f'actor_update_info {actor_update_info}')\n",
    "            \n",
    "            update_info = {**critic_update_info, **actor_update_info, 'R2_validation': R2}\n",
    "            \n",
    "            \n",
    "            if unlogged_steps > log_interval:\n",
    "                exploration_metrics = {f'exploration/disc_return': policy_return}\n",
    "                wandb.log(exploration_metrics, step=int(i),commit=False)\n",
    "                train_metrics = {f'training/{k}': v for k, v in update_info.items()}\n",
    "                wandb.log(train_metrics, step=int(i),commit=False)\n",
    "                #wandb.log(exploration_metrics, step=i)\n",
    "                policy_fn = partial(supply_rng(agent.sample_actions), temperature=0.0)\n",
    "                eval_info = evaluate(policy_fn, eval_env, num_episodes=eval_episodes)\n",
    "                eval_metrics = {f'evaluation/{k}': v for k, v in eval_info.items()}\n",
    "                wandb.log(eval_metrics, step=int(i),commit=True)\n",
    "                unlogged_steps = 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
